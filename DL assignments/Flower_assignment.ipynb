{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flower_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5o6a1KD34s7"
      },
      "source": [
        "As per the Mr. Umair Shahzad this assignment is to learn that how the images can be converted to tensors and the accuracy is not that important. If Convolution, Image Augumentation and Transfer learning is used, then accuracy can be achived above 80%. But without all this stuff, accuracy is not good. So please don't reject this assignment. It took me 3 weeks to complete this assignment since initially I lost so much time to attain 85% accuracy by using convolution and Transfer learning. Thanks "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE7WobAkruEw"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "import tensorflow as tf\n",
        "from google.colab import files\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFWiOQXyr3cJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f732fb71-52ce-420e-aaae-a3709d989c1e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2Of6YLbuem9"
      },
      "source": [
        "local_zip = \"/content/drive/MyDrive/flowers_new.zip\"\n",
        "zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
        "zip_ref.extractall(\"/flower_new\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdHL6gFs3VPx"
      },
      "source": [
        "**Converting the images to gray scale tensor and resize**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hLh4vC-0iSU"
      },
      "source": [
        "pic_size = 50\n",
        "def convert_image(source, label):\n",
        "  fname = os.listdir(source)\n",
        "  images= None\n",
        "  \n",
        "  for i,value in enumerate(fname):\n",
        "    source_path = os.path.join(source, fname[i])\n",
        "    if os.path.getsize(source_path)==0:\n",
        "      print(\"file has zero size\")\n",
        "    else:\n",
        "      img=image.load_img(source_path, target_size=(pic_size, pic_size)) # Load image\n",
        "      x=image.img_to_array(img)                               # Image to array RGB\n",
        "      x=tf.image.rgb_to_grayscale(x)                          # gray scale conversion\n",
        "      x=np.expand_dims(x, axis=0)\n",
        "      if images is None:\n",
        "        images = x\n",
        "      else:\n",
        "        images = np.vstack((images, x))\n",
        "  print(images.shape)\n",
        "  images=images.reshape(images.shape[0], pic_size*pic_size*1)\n",
        "  print(images.shape)\n",
        "  label_in = np.ones((len(fname), 1))\n",
        "  label_in.fill(label)\n",
        "  print(label_in.shape)\n",
        "  images_new=np.hstack((images,label_in))\n",
        "  print(images_new.shape)\n",
        "  return images_new\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea28Aa7106bF"
      },
      "source": [
        "daisy_source_dir = \"/flower_new/flowers/daisy\"\n",
        "\n",
        "dandelion_source_dir = \"/flower_new/flowers/dandelion\"\n",
        "\n",
        "rose_source_dir = \"/flower_new/flowers/rose\"\n",
        "\n",
        "sunflower_source_dir = \"/flower_new/flowers/sunflower\"\n",
        "\n",
        "tulip_source_dir = \"/flower_new/flowers/tulip\""
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2KZtw7A3N7l"
      },
      "source": [
        "**Converting the Images into tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI_0ht--10CQ",
        "outputId": "0a6887a6-9815-453a-f825-2b286664e387"
      },
      "source": [
        "daisy = convert_image(daisy_source_dir, 0)\n",
        "dandelion = convert_image(dandelion_source_dir, 1)\n",
        "rose = convert_image(rose_source_dir, 2)\n",
        "sunflower = convert_image(sunflower_source_dir, 3)\n",
        "tulip = convert_image(tulip_source_dir, 4)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(769, 50, 50, 1)\n",
            "(769, 2500)\n",
            "(769, 1)\n",
            "(769, 2501)\n",
            "(1052, 50, 50, 1)\n",
            "(1052, 2500)\n",
            "(1052, 1)\n",
            "(1052, 2501)\n",
            "(784, 50, 50, 1)\n",
            "(784, 2500)\n",
            "(784, 1)\n",
            "(784, 2501)\n",
            "(734, 50, 50, 1)\n",
            "(734, 2500)\n",
            "(734, 1)\n",
            "(734, 2501)\n",
            "(984, 50, 50, 1)\n",
            "(984, 2500)\n",
            "(984, 1)\n",
            "(984, 2501)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR4y7Pgy7LkA",
        "outputId": "12a8f07a-3547-41a3-be76-8d321464dad7"
      },
      "source": [
        "data_array = np.vstack((daisy, dandelion, rose, sunflower, tulip))\n",
        "data_array.shape\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4323, 2501)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uswIk-O9hMz-"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(data_array)"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "chqXV_2-hRa9",
        "outputId": "de35535a-05c7-4d7c-a62c-0f3005a8b2ad"
      },
      "source": [
        "df.head(3)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "      <th>2500</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>88.601402</td>\n",
              "      <td>76.363800</td>\n",
              "      <td>26.770901</td>\n",
              "      <td>34.884102</td>\n",
              "      <td>90.681198</td>\n",
              "      <td>152.131104</td>\n",
              "      <td>160.130295</td>\n",
              "      <td>166.912415</td>\n",
              "      <td>179.754013</td>\n",
              "      <td>184.981506</td>\n",
              "      <td>182.981705</td>\n",
              "      <td>184.981506</td>\n",
              "      <td>184.981506</td>\n",
              "      <td>184.269699</td>\n",
              "      <td>178.911209</td>\n",
              "      <td>188.138306</td>\n",
              "      <td>176.655609</td>\n",
              "      <td>152.658005</td>\n",
              "      <td>55.811104</td>\n",
              "      <td>16.896700</td>\n",
              "      <td>14.896900</td>\n",
              "      <td>15.1249</td>\n",
              "      <td>18.885702</td>\n",
              "      <td>39.248901</td>\n",
              "      <td>46.248203</td>\n",
              "      <td>40.117802</td>\n",
              "      <td>22.113401</td>\n",
              "      <td>14.108001</td>\n",
              "      <td>9.706300</td>\n",
              "      <td>14.064900</td>\n",
              "      <td>15.608700</td>\n",
              "      <td>15.608700</td>\n",
              "      <td>12.749100</td>\n",
              "      <td>15.976801</td>\n",
              "      <td>18.261801</td>\n",
              "      <td>17.962900</td>\n",
              "      <td>24.962200</td>\n",
              "      <td>145.022308</td>\n",
              "      <td>53.877102</td>\n",
              "      <td>10.353400</td>\n",
              "      <td>...</td>\n",
              "      <td>15.772000</td>\n",
              "      <td>14.772100</td>\n",
              "      <td>24.141001</td>\n",
              "      <td>27.847900</td>\n",
              "      <td>33.109398</td>\n",
              "      <td>22.848400</td>\n",
              "      <td>14.7721</td>\n",
              "      <td>10.011400</td>\n",
              "      <td>15.131101</td>\n",
              "      <td>31.368301</td>\n",
              "      <td>149.213104</td>\n",
              "      <td>146.914398</td>\n",
              "      <td>12.125200</td>\n",
              "      <td>12.125200</td>\n",
              "      <td>12.125200</td>\n",
              "      <td>11.837100</td>\n",
              "      <td>18.141600</td>\n",
              "      <td>23.913002</td>\n",
              "      <td>18.831800</td>\n",
              "      <td>19.130701</td>\n",
              "      <td>26.136202</td>\n",
              "      <td>23.158100</td>\n",
              "      <td>27.869499</td>\n",
              "      <td>38.917698</td>\n",
              "      <td>33.978302</td>\n",
              "      <td>30.847601</td>\n",
              "      <td>41.749500</td>\n",
              "      <td>44.686203</td>\n",
              "      <td>53.240101</td>\n",
              "      <td>67.825798</td>\n",
              "      <td>110.111504</td>\n",
              "      <td>127.923103</td>\n",
              "      <td>130.069199</td>\n",
              "      <td>105.772697</td>\n",
              "      <td>38.369301</td>\n",
              "      <td>23.902201</td>\n",
              "      <td>18.771702</td>\n",
              "      <td>12.125200</td>\n",
              "      <td>12.837001</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>236.806702</td>\n",
              "      <td>202.319107</td>\n",
              "      <td>206.720795</td>\n",
              "      <td>212.236404</td>\n",
              "      <td>198.280899</td>\n",
              "      <td>252.974701</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.746506</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>117.947098</td>\n",
              "      <td>204.041595</td>\n",
              "      <td>197.031494</td>\n",
              "      <td>170.262207</td>\n",
              "      <td>189.109406</td>\n",
              "      <td>134.389297</td>\n",
              "      <td>72.574196</td>\n",
              "      <td>46.856998</td>\n",
              "      <td>38.660400</td>\n",
              "      <td>39.660301</td>\n",
              "      <td>43.659901</td>\n",
              "      <td>51.0443</td>\n",
              "      <td>56.787903</td>\n",
              "      <td>61.944504</td>\n",
              "      <td>62.303501</td>\n",
              "      <td>85.267799</td>\n",
              "      <td>104.323204</td>\n",
              "      <td>104.867104</td>\n",
              "      <td>103.807106</td>\n",
              "      <td>99.067902</td>\n",
              "      <td>87.910202</td>\n",
              "      <td>87.079903</td>\n",
              "      <td>73.156708</td>\n",
              "      <td>42.992901</td>\n",
              "      <td>39.133400</td>\n",
              "      <td>49.033699</td>\n",
              "      <td>85.730995</td>\n",
              "      <td>211.695099</td>\n",
              "      <td>236.024002</td>\n",
              "      <td>200.115417</td>\n",
              "      <td>...</td>\n",
              "      <td>93.766197</td>\n",
              "      <td>81.984596</td>\n",
              "      <td>90.239601</td>\n",
              "      <td>99.722496</td>\n",
              "      <td>99.804207</td>\n",
              "      <td>101.804001</td>\n",
              "      <td>100.1632</td>\n",
              "      <td>97.081802</td>\n",
              "      <td>101.081398</td>\n",
              "      <td>97.961502</td>\n",
              "      <td>99.961304</td>\n",
              "      <td>96.722801</td>\n",
              "      <td>98.722603</td>\n",
              "      <td>95.722900</td>\n",
              "      <td>91.723297</td>\n",
              "      <td>101.950302</td>\n",
              "      <td>91.951302</td>\n",
              "      <td>78.066704</td>\n",
              "      <td>71.708298</td>\n",
              "      <td>65.964699</td>\n",
              "      <td>57.105602</td>\n",
              "      <td>3.287900</td>\n",
              "      <td>3.629900</td>\n",
              "      <td>23.059601</td>\n",
              "      <td>38.434200</td>\n",
              "      <td>52.808903</td>\n",
              "      <td>53.959702</td>\n",
              "      <td>67.936699</td>\n",
              "      <td>62.089905</td>\n",
              "      <td>78.262405</td>\n",
              "      <td>80.197601</td>\n",
              "      <td>78.088303</td>\n",
              "      <td>81.197502</td>\n",
              "      <td>75.697205</td>\n",
              "      <td>82.006302</td>\n",
              "      <td>72.050400</td>\n",
              "      <td>50.749001</td>\n",
              "      <td>47.938698</td>\n",
              "      <td>54.628300</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.999800</td>\n",
              "      <td>1.771800</td>\n",
              "      <td>1.771800</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.368000</td>\n",
              "      <td>4.191900</td>\n",
              "      <td>4.104000</td>\n",
              "      <td>5.542900</td>\n",
              "      <td>6.498000</td>\n",
              "      <td>9.121700</td>\n",
              "      <td>11.121500</td>\n",
              "      <td>10.762500</td>\n",
              "      <td>8.779700</td>\n",
              "      <td>8.067900</td>\n",
              "      <td>7.068000</td>\n",
              "      <td>7.296000</td>\n",
              "      <td>6.498000</td>\n",
              "      <td>6.384000</td>\n",
              "      <td>5.884900</td>\n",
              "      <td>5.086900</td>\n",
              "      <td>4.104000</td>\n",
              "      <td>...</td>\n",
              "      <td>65.018799</td>\n",
              "      <td>64.937202</td>\n",
              "      <td>52.197201</td>\n",
              "      <td>36.908901</td>\n",
              "      <td>42.811302</td>\n",
              "      <td>49.300701</td>\n",
              "      <td>1.1740</td>\n",
              "      <td>45.136101</td>\n",
              "      <td>47.066700</td>\n",
              "      <td>23.766802</td>\n",
              "      <td>43.697201</td>\n",
              "      <td>41.039501</td>\n",
              "      <td>40.680500</td>\n",
              "      <td>44.049999</td>\n",
              "      <td>44.295002</td>\n",
              "      <td>45.811001</td>\n",
              "      <td>48.425602</td>\n",
              "      <td>40.164402</td>\n",
              "      <td>58.697403</td>\n",
              "      <td>69.377502</td>\n",
              "      <td>63.991203</td>\n",
              "      <td>58.860699</td>\n",
              "      <td>63.002098</td>\n",
              "      <td>88.210602</td>\n",
              "      <td>86.930504</td>\n",
              "      <td>72.046005</td>\n",
              "      <td>62.002201</td>\n",
              "      <td>54.822502</td>\n",
              "      <td>66.360802</td>\n",
              "      <td>37.186199</td>\n",
              "      <td>57.974800</td>\n",
              "      <td>67.018700</td>\n",
              "      <td>71.089203</td>\n",
              "      <td>60.148701</td>\n",
              "      <td>66.990898</td>\n",
              "      <td>62.219402</td>\n",
              "      <td>61.230301</td>\n",
              "      <td>56.985703</td>\n",
              "      <td>60.942200</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 2501 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "         0           1           2     ...       2498       2499  2500\n",
              "0   88.601402   76.363800   26.770901  ...  12.125200  12.837001   0.0\n",
              "1  236.806702  202.319107  206.720795  ...  47.938698  54.628300   0.0\n",
              "2    0.000000    0.000000    0.000000  ...  56.985703  60.942200   0.0\n",
              "\n",
              "[3 rows x 2501 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "4ULRcxfPkq4N",
        "outputId": "9173ea8f-aec3-4018-e24d-03377a53815e"
      },
      "source": [
        "df.tail(3)"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "      <th>2500</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4320</th>\n",
              "      <td>47.940205</td>\n",
              "      <td>57.792900</td>\n",
              "      <td>71.830101</td>\n",
              "      <td>84.269600</td>\n",
              "      <td>90.225906</td>\n",
              "      <td>95.079102</td>\n",
              "      <td>99.035606</td>\n",
              "      <td>101.818100</td>\n",
              "      <td>107.845306</td>\n",
              "      <td>109.214996</td>\n",
              "      <td>109.986900</td>\n",
              "      <td>111.698608</td>\n",
              "      <td>116.953903</td>\n",
              "      <td>118.953697</td>\n",
              "      <td>124.084206</td>\n",
              "      <td>130.083603</td>\n",
              "      <td>133.154205</td>\n",
              "      <td>135.855011</td>\n",
              "      <td>134.855103</td>\n",
              "      <td>134.926010</td>\n",
              "      <td>136.170898</td>\n",
              "      <td>135.171005</td>\n",
              "      <td>131.715302</td>\n",
              "      <td>130.003601</td>\n",
              "      <td>129.715500</td>\n",
              "      <td>131.732300</td>\n",
              "      <td>128.732605</td>\n",
              "      <td>128.188705</td>\n",
              "      <td>124.993294</td>\n",
              "      <td>121.634598</td>\n",
              "      <td>119.617805</td>\n",
              "      <td>114.633606</td>\n",
              "      <td>108.193504</td>\n",
              "      <td>107.421600</td>\n",
              "      <td>104.176903</td>\n",
              "      <td>100.035500</td>\n",
              "      <td>99.274406</td>\n",
              "      <td>99.818306</td>\n",
              "      <td>103.177002</td>\n",
              "      <td>102.818008</td>\n",
              "      <td>...</td>\n",
              "      <td>109.177200</td>\n",
              "      <td>88.901901</td>\n",
              "      <td>66.811600</td>\n",
              "      <td>109.168106</td>\n",
              "      <td>74.763199</td>\n",
              "      <td>107.729202</td>\n",
              "      <td>113.055405</td>\n",
              "      <td>104.069901</td>\n",
              "      <td>81.328003</td>\n",
              "      <td>70.101105</td>\n",
              "      <td>69.346199</td>\n",
              "      <td>63.058704</td>\n",
              "      <td>56.939201</td>\n",
              "      <td>106.076004</td>\n",
              "      <td>126.306503</td>\n",
              "      <td>132.886703</td>\n",
              "      <td>103.147102</td>\n",
              "      <td>92.148201</td>\n",
              "      <td>83.149101</td>\n",
              "      <td>84.789902</td>\n",
              "      <td>94.016998</td>\n",
              "      <td>107.015701</td>\n",
              "      <td>112.032204</td>\n",
              "      <td>101.098000</td>\n",
              "      <td>40.974701</td>\n",
              "      <td>80.089302</td>\n",
              "      <td>48.107903</td>\n",
              "      <td>60.123703</td>\n",
              "      <td>76.806099</td>\n",
              "      <td>65.024498</td>\n",
              "      <td>63.035503</td>\n",
              "      <td>57.201099</td>\n",
              "      <td>66.052200</td>\n",
              "      <td>79.100204</td>\n",
              "      <td>72.748001</td>\n",
              "      <td>66.873398</td>\n",
              "      <td>64.172600</td>\n",
              "      <td>69.329201</td>\n",
              "      <td>68.970207</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4321</th>\n",
              "      <td>139.048904</td>\n",
              "      <td>138.820908</td>\n",
              "      <td>138.891800</td>\n",
              "      <td>139.891693</td>\n",
              "      <td>141.891510</td>\n",
              "      <td>143.663315</td>\n",
              "      <td>147.951004</td>\n",
              "      <td>153.178513</td>\n",
              "      <td>161.807800</td>\n",
              "      <td>173.023895</td>\n",
              "      <td>186.766708</td>\n",
              "      <td>200.298508</td>\n",
              "      <td>207.988007</td>\n",
              "      <td>217.225800</td>\n",
              "      <td>222.947800</td>\n",
              "      <td>228.940903</td>\n",
              "      <td>233.043701</td>\n",
              "      <td>236.189713</td>\n",
              "      <td>240.395706</td>\n",
              "      <td>232.228699</td>\n",
              "      <td>145.885513</td>\n",
              "      <td>96.293800</td>\n",
              "      <td>179.749512</td>\n",
              "      <td>166.114594</td>\n",
              "      <td>80.723900</td>\n",
              "      <td>82.957901</td>\n",
              "      <td>80.951904</td>\n",
              "      <td>82.820702</td>\n",
              "      <td>83.287506</td>\n",
              "      <td>130.070511</td>\n",
              "      <td>126.121506</td>\n",
              "      <td>114.763603</td>\n",
              "      <td>95.750000</td>\n",
              "      <td>83.743507</td>\n",
              "      <td>82.190598</td>\n",
              "      <td>80.286407</td>\n",
              "      <td>199.253906</td>\n",
              "      <td>158.600510</td>\n",
              "      <td>154.093704</td>\n",
              "      <td>218.202515</td>\n",
              "      <td>...</td>\n",
              "      <td>57.163601</td>\n",
              "      <td>49.054703</td>\n",
              "      <td>41.169304</td>\n",
              "      <td>38.778103</td>\n",
              "      <td>36.838402</td>\n",
              "      <td>35.827698</td>\n",
              "      <td>37.055603</td>\n",
              "      <td>36.425499</td>\n",
              "      <td>36.838402</td>\n",
              "      <td>37.849098</td>\n",
              "      <td>40.158604</td>\n",
              "      <td>40.468300</td>\n",
              "      <td>39.386703</td>\n",
              "      <td>37.436203</td>\n",
              "      <td>34.746101</td>\n",
              "      <td>32.724701</td>\n",
              "      <td>34.322399</td>\n",
              "      <td>37.365303</td>\n",
              "      <td>40.468300</td>\n",
              "      <td>41.593002</td>\n",
              "      <td>41.180103</td>\n",
              "      <td>38.778103</td>\n",
              "      <td>35.599701</td>\n",
              "      <td>183.225601</td>\n",
              "      <td>233.896606</td>\n",
              "      <td>204.050903</td>\n",
              "      <td>149.555099</td>\n",
              "      <td>132.923203</td>\n",
              "      <td>130.353409</td>\n",
              "      <td>113.059097</td>\n",
              "      <td>98.009598</td>\n",
              "      <td>89.944099</td>\n",
              "      <td>98.205299</td>\n",
              "      <td>119.157204</td>\n",
              "      <td>136.320496</td>\n",
              "      <td>105.862503</td>\n",
              "      <td>63.051201</td>\n",
              "      <td>83.840004</td>\n",
              "      <td>113.786499</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4322</th>\n",
              "      <td>150.393402</td>\n",
              "      <td>187.150909</td>\n",
              "      <td>189.597809</td>\n",
              "      <td>130.193604</td>\n",
              "      <td>157.726593</td>\n",
              "      <td>143.387207</td>\n",
              "      <td>199.120102</td>\n",
              "      <td>161.739899</td>\n",
              "      <td>120.747299</td>\n",
              "      <td>147.791000</td>\n",
              "      <td>123.177010</td>\n",
              "      <td>77.668800</td>\n",
              "      <td>98.951103</td>\n",
              "      <td>116.845604</td>\n",
              "      <td>64.200699</td>\n",
              "      <td>70.744003</td>\n",
              "      <td>105.767403</td>\n",
              "      <td>78.032104</td>\n",
              "      <td>59.854000</td>\n",
              "      <td>53.790401</td>\n",
              "      <td>56.919003</td>\n",
              "      <td>58.134903</td>\n",
              "      <td>71.023804</td>\n",
              "      <td>52.939602</td>\n",
              "      <td>57.910603</td>\n",
              "      <td>60.263004</td>\n",
              "      <td>52.705204</td>\n",
              "      <td>70.842003</td>\n",
              "      <td>129.938705</td>\n",
              "      <td>46.678902</td>\n",
              "      <td>75.204201</td>\n",
              "      <td>82.803902</td>\n",
              "      <td>163.158997</td>\n",
              "      <td>79.942604</td>\n",
              "      <td>106.718201</td>\n",
              "      <td>115.906708</td>\n",
              "      <td>131.074799</td>\n",
              "      <td>48.915100</td>\n",
              "      <td>42.872501</td>\n",
              "      <td>28.151300</td>\n",
              "      <td>...</td>\n",
              "      <td>35.796101</td>\n",
              "      <td>15.174201</td>\n",
              "      <td>22.079502</td>\n",
              "      <td>13.705900</td>\n",
              "      <td>59.943604</td>\n",
              "      <td>17.092400</td>\n",
              "      <td>31.240601</td>\n",
              "      <td>103.106499</td>\n",
              "      <td>107.945702</td>\n",
              "      <td>50.679501</td>\n",
              "      <td>42.820301</td>\n",
              "      <td>55.244503</td>\n",
              "      <td>196.960403</td>\n",
              "      <td>228.084213</td>\n",
              "      <td>114.935806</td>\n",
              "      <td>226.641510</td>\n",
              "      <td>55.994801</td>\n",
              "      <td>23.055000</td>\n",
              "      <td>22.272200</td>\n",
              "      <td>27.880301</td>\n",
              "      <td>17.098600</td>\n",
              "      <td>30.884501</td>\n",
              "      <td>26.751001</td>\n",
              "      <td>16.896700</td>\n",
              "      <td>15.896800</td>\n",
              "      <td>30.835199</td>\n",
              "      <td>46.185200</td>\n",
              "      <td>55.716000</td>\n",
              "      <td>112.959404</td>\n",
              "      <td>64.314606</td>\n",
              "      <td>160.073105</td>\n",
              "      <td>191.834900</td>\n",
              "      <td>77.941902</td>\n",
              "      <td>21.891600</td>\n",
              "      <td>0.228000</td>\n",
              "      <td>7.999200</td>\n",
              "      <td>12.749100</td>\n",
              "      <td>18.754700</td>\n",
              "      <td>42.809597</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 2501 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0           1           2     ...       2498        2499  2500\n",
              "4320   47.940205   57.792900   71.830101  ...  69.329201   68.970207   4.0\n",
              "4321  139.048904  138.820908  138.891800  ...  83.840004  113.786499   4.0\n",
              "4322  150.393402  187.150909  189.597809  ...  18.754700   42.809597   4.0\n",
              "\n",
              "[3 rows x 2501 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 157
        },
        "id": "yuvbbiLnGQk7",
        "outputId": "69599f19-7102-4424-fb2e-fb9fe19c1cfe"
      },
      "source": [
        "df[df.duplicated()]"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "      <th>2500</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3097</th>\n",
              "      <td>108.260307</td>\n",
              "      <td>109.146202</td>\n",
              "      <td>78.861099</td>\n",
              "      <td>95.663704</td>\n",
              "      <td>109.858002</td>\n",
              "      <td>108.684006</td>\n",
              "      <td>111.971794</td>\n",
              "      <td>116.8573</td>\n",
              "      <td>108.972107</td>\n",
              "      <td>63.976601</td>\n",
              "      <td>113.857605</td>\n",
              "      <td>115.949799</td>\n",
              "      <td>113.661903</td>\n",
              "      <td>118.683006</td>\n",
              "      <td>80.860901</td>\n",
              "      <td>125.258606</td>\n",
              "      <td>124.970497</td>\n",
              "      <td>118.807701</td>\n",
              "      <td>125.258606</td>\n",
              "      <td>73.893906</td>\n",
              "      <td>131.888107</td>\n",
              "      <td>130.045410</td>\n",
              "      <td>133.887909</td>\n",
              "      <td>92.120102</td>\n",
              "      <td>128.839096</td>\n",
              "      <td>115.356598</td>\n",
              "      <td>84.050003</td>\n",
              "      <td>112.759003</td>\n",
              "      <td>162.764694</td>\n",
              "      <td>159.758804</td>\n",
              "      <td>165.003311</td>\n",
              "      <td>172.985504</td>\n",
              "      <td>131.058899</td>\n",
              "      <td>125.869995</td>\n",
              "      <td>171.013489</td>\n",
              "      <td>175.681702</td>\n",
              "      <td>166.351501</td>\n",
              "      <td>179.094406</td>\n",
              "      <td>183.745605</td>\n",
              "      <td>187.229095</td>\n",
              "      <td>...</td>\n",
              "      <td>10.377800</td>\n",
              "      <td>45.146801</td>\n",
              "      <td>63.033802</td>\n",
              "      <td>20.171999</td>\n",
              "      <td>37.213398</td>\n",
              "      <td>16.090700</td>\n",
              "      <td>17.090599</td>\n",
              "      <td>16.0907</td>\n",
              "      <td>18.313999</td>\n",
              "      <td>61.750801</td>\n",
              "      <td>92.117607</td>\n",
              "      <td>140.064896</td>\n",
              "      <td>4.0920</td>\n",
              "      <td>4.092000</td>\n",
              "      <td>100.137108</td>\n",
              "      <td>36.896301</td>\n",
              "      <td>22.749701</td>\n",
              "      <td>29.7983</td>\n",
              "      <td>14.194201</td>\n",
              "      <td>121.371902</td>\n",
              "      <td>22.737299</td>\n",
              "      <td>53.936203</td>\n",
              "      <td>85.139503</td>\n",
              "      <td>14.755100</td>\n",
              "      <td>48.340599</td>\n",
              "      <td>63.947601</td>\n",
              "      <td>200.216415</td>\n",
              "      <td>154.152893</td>\n",
              "      <td>149.002502</td>\n",
              "      <td>98.194199</td>\n",
              "      <td>25.814100</td>\n",
              "      <td>57.056004</td>\n",
              "      <td>49.072102</td>\n",
              "      <td>100.813301</td>\n",
              "      <td>85.947105</td>\n",
              "      <td>58.270199</td>\n",
              "      <td>37.831604</td>\n",
              "      <td>41.831200</td>\n",
              "      <td>40.872700</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3164</th>\n",
              "      <td>109.135406</td>\n",
              "      <td>111.146004</td>\n",
              "      <td>107.890503</td>\n",
              "      <td>107.972206</td>\n",
              "      <td>69.894402</td>\n",
              "      <td>108.167900</td>\n",
              "      <td>111.199905</td>\n",
              "      <td>114.1996</td>\n",
              "      <td>115.259605</td>\n",
              "      <td>119.156006</td>\n",
              "      <td>76.171104</td>\n",
              "      <td>117.971207</td>\n",
              "      <td>113.128799</td>\n",
              "      <td>112.118103</td>\n",
              "      <td>121.145004</td>\n",
              "      <td>83.974609</td>\n",
              "      <td>122.661003</td>\n",
              "      <td>119.818405</td>\n",
              "      <td>121.818199</td>\n",
              "      <td>84.903603</td>\n",
              "      <td>130.871201</td>\n",
              "      <td>128.256607</td>\n",
              "      <td>133.354797</td>\n",
              "      <td>98.201202</td>\n",
              "      <td>137.756500</td>\n",
              "      <td>131.772400</td>\n",
              "      <td>119.257500</td>\n",
              "      <td>140.820908</td>\n",
              "      <td>120.181999</td>\n",
              "      <td>99.070107</td>\n",
              "      <td>168.654694</td>\n",
              "      <td>170.942612</td>\n",
              "      <td>182.924393</td>\n",
              "      <td>109.800697</td>\n",
              "      <td>180.941605</td>\n",
              "      <td>117.148506</td>\n",
              "      <td>160.886490</td>\n",
              "      <td>189.228912</td>\n",
              "      <td>193.652206</td>\n",
              "      <td>153.781006</td>\n",
              "      <td>...</td>\n",
              "      <td>125.072502</td>\n",
              "      <td>8.960500</td>\n",
              "      <td>6.851300</td>\n",
              "      <td>49.901398</td>\n",
              "      <td>4.058000</td>\n",
              "      <td>64.813499</td>\n",
              "      <td>91.849396</td>\n",
              "      <td>17.7486</td>\n",
              "      <td>89.825096</td>\n",
              "      <td>24.812500</td>\n",
              "      <td>16.411201</td>\n",
              "      <td>18.090500</td>\n",
              "      <td>14.8952</td>\n",
              "      <td>185.233505</td>\n",
              "      <td>49.865700</td>\n",
              "      <td>11.966400</td>\n",
              "      <td>8.057600</td>\n",
              "      <td>1.9890</td>\n",
              "      <td>9.955800</td>\n",
              "      <td>45.971001</td>\n",
              "      <td>90.841606</td>\n",
              "      <td>36.227699</td>\n",
              "      <td>151.164001</td>\n",
              "      <td>53.155102</td>\n",
              "      <td>163.816208</td>\n",
              "      <td>25.732401</td>\n",
              "      <td>115.760796</td>\n",
              "      <td>177.038300</td>\n",
              "      <td>70.095001</td>\n",
              "      <td>34.139999</td>\n",
              "      <td>29.901602</td>\n",
              "      <td>43.747601</td>\n",
              "      <td>37.161301</td>\n",
              "      <td>16.303501</td>\n",
              "      <td>26.897501</td>\n",
              "      <td>25.020601</td>\n",
              "      <td>58.604401</td>\n",
              "      <td>107.194603</td>\n",
              "      <td>78.354599</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>2 rows × 2501 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0           1           2     ...        2498       2499  2500\n",
              "3097  108.260307  109.146202   78.861099  ...   41.831200  40.872700   3.0\n",
              "3164  109.135406  111.146004  107.890503  ...  107.194603  78.354599   3.0\n",
              "\n",
              "[2 rows x 2501 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bG0o5PNZHKmg"
      },
      "source": [
        "df.drop_duplicates(inplace=True)"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BMdEMkuZHg2b",
        "outputId": "14b81f4d-b22c-46b1-b1cb-aa9639af8b47"
      },
      "source": [
        "df.isnull().values.any()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "False"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pkMqJDNUIyK_",
        "outputId": "09360262-24d9-4e3a-fcfe-b91c352bcb88"
      },
      "source": [
        "len(df)"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "4321"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 23
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3ux-iza3hNm"
      },
      "source": [
        "**Splittig the Training and Testing Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq7uYQe2kwLv"
      },
      "source": [
        "x_data = df.sample(frac=1) # Shuffling the data\n",
        "\n",
        "train_data_full= x_data.iloc[:2593,:]\n",
        "test_data_full = x_data.iloc[2593:, :]\n",
        "train_data = train_data_full.iloc[:, :-1]\n",
        "train_labels = train_data_full.iloc[:, -1]\n",
        "test_data = test_data_full.iloc[:, :-1]\n",
        "test_labels = test_data_full.iloc[:, -1]\n"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAwHh-PITTzI",
        "outputId": "524694cd-639f-42a5-837f-b00f5fff9bd1"
      },
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2593, 2500)\n",
            "(1728, 2500)\n",
            "(2593,)\n",
            "(1728,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "qUKvBM9mlB7K",
        "outputId": "801c3769-978e-4cac-b5a5-040b7d94f80f"
      },
      "source": [
        "train_data.head(3)"
      ],
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2460</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3598</th>\n",
              "      <td>87.251709</td>\n",
              "      <td>83.996300</td>\n",
              "      <td>65.053696</td>\n",
              "      <td>77.217407</td>\n",
              "      <td>107.052704</td>\n",
              "      <td>118.952797</td>\n",
              "      <td>184.824203</td>\n",
              "      <td>165.130707</td>\n",
              "      <td>181.774597</td>\n",
              "      <td>178.285400</td>\n",
              "      <td>182.240295</td>\n",
              "      <td>158.801895</td>\n",
              "      <td>162.005402</td>\n",
              "      <td>181.190002</td>\n",
              "      <td>180.220703</td>\n",
              "      <td>213.869598</td>\n",
              "      <td>227.837112</td>\n",
              "      <td>246.069504</td>\n",
              "      <td>238.976105</td>\n",
              "      <td>132.222900</td>\n",
              "      <td>182.251617</td>\n",
              "      <td>248.035309</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.387512</td>\n",
              "      <td>254.387512</td>\n",
              "      <td>249.925705</td>\n",
              "      <td>237.122604</td>\n",
              "      <td>145.843903</td>\n",
              "      <td>137.062103</td>\n",
              "      <td>144.181610</td>\n",
              "      <td>146.970306</td>\n",
              "      <td>140.242096</td>\n",
              "      <td>133.900696</td>\n",
              "      <td>124.750610</td>\n",
              "      <td>245.186493</td>\n",
              "      <td>250.534210</td>\n",
              "      <td>253.279800</td>\n",
              "      <td>241.722107</td>\n",
              "      <td>233.983704</td>\n",
              "      <td>224.918198</td>\n",
              "      <td>...</td>\n",
              "      <td>79.063095</td>\n",
              "      <td>43.038902</td>\n",
              "      <td>63.095703</td>\n",
              "      <td>103.110397</td>\n",
              "      <td>245.080505</td>\n",
              "      <td>62.907600</td>\n",
              "      <td>115.238503</td>\n",
              "      <td>97.141609</td>\n",
              "      <td>32.073902</td>\n",
              "      <td>101.167206</td>\n",
              "      <td>95.928902</td>\n",
              "      <td>82.158302</td>\n",
              "      <td>64.074005</td>\n",
              "      <td>77.171402</td>\n",
              "      <td>98.861198</td>\n",
              "      <td>25.840300</td>\n",
              "      <td>24.324301</td>\n",
              "      <td>35.127499</td>\n",
              "      <td>89.817497</td>\n",
              "      <td>95.020096</td>\n",
              "      <td>77.145004</td>\n",
              "      <td>40.404400</td>\n",
              "      <td>39.838902</td>\n",
              "      <td>53.224400</td>\n",
              "      <td>117.241104</td>\n",
              "      <td>57.778500</td>\n",
              "      <td>71.222595</td>\n",
              "      <td>86.244202</td>\n",
              "      <td>98.128998</td>\n",
              "      <td>53.768200</td>\n",
              "      <td>86.852707</td>\n",
              "      <td>68.549400</td>\n",
              "      <td>69.370598</td>\n",
              "      <td>74.370102</td>\n",
              "      <td>89.809303</td>\n",
              "      <td>82.001205</td>\n",
              "      <td>84.289101</td>\n",
              "      <td>79.773399</td>\n",
              "      <td>74.098999</td>\n",
              "      <td>55.946804</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3521</th>\n",
              "      <td>86.218407</td>\n",
              "      <td>74.989700</td>\n",
              "      <td>86.250702</td>\n",
              "      <td>69.757607</td>\n",
              "      <td>71.822105</td>\n",
              "      <td>79.088005</td>\n",
              "      <td>76.066704</td>\n",
              "      <td>91.770798</td>\n",
              "      <td>97.084595</td>\n",
              "      <td>121.142403</td>\n",
              "      <td>145.252411</td>\n",
              "      <td>153.717316</td>\n",
              "      <td>177.092102</td>\n",
              "      <td>166.831207</td>\n",
              "      <td>159.255600</td>\n",
              "      <td>182.764908</td>\n",
              "      <td>165.367416</td>\n",
              "      <td>175.853500</td>\n",
              "      <td>182.872620</td>\n",
              "      <td>192.009308</td>\n",
              "      <td>124.909500</td>\n",
              "      <td>164.748611</td>\n",
              "      <td>171.755203</td>\n",
              "      <td>190.965805</td>\n",
              "      <td>160.788696</td>\n",
              "      <td>142.220306</td>\n",
              "      <td>120.198097</td>\n",
              "      <td>216.930695</td>\n",
              "      <td>198.997314</td>\n",
              "      <td>128.762604</td>\n",
              "      <td>114.668602</td>\n",
              "      <td>91.881905</td>\n",
              "      <td>107.905006</td>\n",
              "      <td>140.790695</td>\n",
              "      <td>186.616394</td>\n",
              "      <td>157.767303</td>\n",
              "      <td>135.774200</td>\n",
              "      <td>100.183006</td>\n",
              "      <td>86.021004</td>\n",
              "      <td>78.843002</td>\n",
              "      <td>...</td>\n",
              "      <td>113.975197</td>\n",
              "      <td>120.708008</td>\n",
              "      <td>184.200607</td>\n",
              "      <td>194.199600</td>\n",
              "      <td>188.281906</td>\n",
              "      <td>177.953400</td>\n",
              "      <td>174.822693</td>\n",
              "      <td>185.163712</td>\n",
              "      <td>191.010513</td>\n",
              "      <td>183.099197</td>\n",
              "      <td>116.740807</td>\n",
              "      <td>85.916504</td>\n",
              "      <td>72.769806</td>\n",
              "      <td>153.758698</td>\n",
              "      <td>126.994102</td>\n",
              "      <td>148.105896</td>\n",
              "      <td>143.765991</td>\n",
              "      <td>213.116013</td>\n",
              "      <td>135.837601</td>\n",
              "      <td>112.752098</td>\n",
              "      <td>170.764709</td>\n",
              "      <td>60.824802</td>\n",
              "      <td>58.801800</td>\n",
              "      <td>160.181396</td>\n",
              "      <td>171.208099</td>\n",
              "      <td>70.150703</td>\n",
              "      <td>126.734909</td>\n",
              "      <td>170.001709</td>\n",
              "      <td>73.048599</td>\n",
              "      <td>117.018105</td>\n",
              "      <td>85.899506</td>\n",
              "      <td>109.174606</td>\n",
              "      <td>110.034401</td>\n",
              "      <td>105.207306</td>\n",
              "      <td>95.098900</td>\n",
              "      <td>96.060204</td>\n",
              "      <td>124.308502</td>\n",
              "      <td>104.902405</td>\n",
              "      <td>125.622902</td>\n",
              "      <td>104.011902</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>891</th>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>253.974594</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>248.106201</td>\n",
              "      <td>201.611710</td>\n",
              "      <td>228.994110</td>\n",
              "      <td>254.518509</td>\n",
              "      <td>252.974701</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>245.975418</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>250.974899</td>\n",
              "      <td>169.754990</td>\n",
              "      <td>250.746902</td>\n",
              "      <td>252.746704</td>\n",
              "      <td>252.746704</td>\n",
              "      <td>251.991806</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>250.974899</td>\n",
              "      <td>253.746597</td>\n",
              "      <td>238.765106</td>\n",
              "      <td>254.105606</td>\n",
              "      <td>250.746902</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.746506</td>\n",
              "      <td>254.404510</td>\n",
              "      <td>245.747406</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.746506</td>\n",
              "      <td>242.975708</td>\n",
              "      <td>254.105606</td>\n",
              "      <td>251.062790</td>\n",
              "      <td>247.747208</td>\n",
              "      <td>...</td>\n",
              "      <td>223.917511</td>\n",
              "      <td>237.993195</td>\n",
              "      <td>239.036209</td>\n",
              "      <td>196.812408</td>\n",
              "      <td>212.109818</td>\n",
              "      <td>247.747208</td>\n",
              "      <td>233.107712</td>\n",
              "      <td>189.112091</td>\n",
              "      <td>107.761200</td>\n",
              "      <td>234.047501</td>\n",
              "      <td>250.045914</td>\n",
              "      <td>197.111298</td>\n",
              "      <td>210.039108</td>\n",
              "      <td>210.039108</td>\n",
              "      <td>213.109711</td>\n",
              "      <td>238.047119</td>\n",
              "      <td>241.057602</td>\n",
              "      <td>232.819611</td>\n",
              "      <td>167.853912</td>\n",
              "      <td>230.108017</td>\n",
              "      <td>215.995407</td>\n",
              "      <td>254.518509</td>\n",
              "      <td>237.819107</td>\n",
              "      <td>246.704208</td>\n",
              "      <td>248.992096</td>\n",
              "      <td>254.746506</td>\n",
              "      <td>254.746506</td>\n",
              "      <td>249.975006</td>\n",
              "      <td>254.746506</td>\n",
              "      <td>244.747498</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>239.748001</td>\n",
              "      <td>253.974594</td>\n",
              "      <td>252.974701</td>\n",
              "      <td>252.974701</td>\n",
              "      <td>252.974701</td>\n",
              "      <td>252.974701</td>\n",
              "      <td>247.975204</td>\n",
              "      <td>253.974594</td>\n",
              "      <td>253.974594</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 2500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0           1           2     ...        2497        2498        2499\n",
              "3598   87.251709   83.996300   65.053696  ...   79.773399   74.098999   55.946804\n",
              "3521   86.218407   74.989700   86.250702  ...  104.902405  125.622902  104.011902\n",
              "891   254.974518  254.974518  254.974518  ...  247.975204  253.974594  253.974594\n",
              "\n",
              "[3 rows x 2500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "t0nW6mFqleBM",
        "outputId": "98f5531a-a3f8-4cd8-a6cb-bc863816356b"
      },
      "source": [
        "test_data.tail(3)"
      ],
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2460</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1789</th>\n",
              "      <td>27.012800</td>\n",
              "      <td>17.908901</td>\n",
              "      <td>14.027900</td>\n",
              "      <td>18.197001</td>\n",
              "      <td>27.311701</td>\n",
              "      <td>38.252102</td>\n",
              "      <td>26.240900</td>\n",
              "      <td>23.131802</td>\n",
              "      <td>16.436001</td>\n",
              "      <td>23.131802</td>\n",
              "      <td>27.240801</td>\n",
              "      <td>26.415001</td>\n",
              "      <td>30.235901</td>\n",
              "      <td>37.219803</td>\n",
              "      <td>45.057301</td>\n",
              "      <td>59.373402</td>\n",
              "      <td>94.618301</td>\n",
              "      <td>95.139008</td>\n",
              "      <td>71.911598</td>\n",
              "      <td>40.812702</td>\n",
              "      <td>62.992500</td>\n",
              "      <td>67.058403</td>\n",
              "      <td>41.100800</td>\n",
              "      <td>26.240900</td>\n",
              "      <td>28.001902</td>\n",
              "      <td>27.002001</td>\n",
              "      <td>27.887901</td>\n",
              "      <td>39.301300</td>\n",
              "      <td>74.900597</td>\n",
              "      <td>90.949997</td>\n",
              "      <td>92.173302</td>\n",
              "      <td>96.005005</td>\n",
              "      <td>79.699799</td>\n",
              "      <td>83.058601</td>\n",
              "      <td>55.894501</td>\n",
              "      <td>88.113503</td>\n",
              "      <td>90.874397</td>\n",
              "      <td>76.227005</td>\n",
              "      <td>68.324799</td>\n",
              "      <td>67.873398</td>\n",
              "      <td>...</td>\n",
              "      <td>85.204605</td>\n",
              "      <td>68.808601</td>\n",
              "      <td>70.036507</td>\n",
              "      <td>66.335701</td>\n",
              "      <td>67.132202</td>\n",
              "      <td>50.170902</td>\n",
              "      <td>94.048004</td>\n",
              "      <td>3.522000</td>\n",
              "      <td>63.980103</td>\n",
              "      <td>27.589001</td>\n",
              "      <td>114.102005</td>\n",
              "      <td>123.075104</td>\n",
              "      <td>46.807499</td>\n",
              "      <td>128.042206</td>\n",
              "      <td>89.950104</td>\n",
              "      <td>84.260201</td>\n",
              "      <td>110.336502</td>\n",
              "      <td>66.183304</td>\n",
              "      <td>55.905300</td>\n",
              "      <td>65.585503</td>\n",
              "      <td>18.898001</td>\n",
              "      <td>4.109000</td>\n",
              "      <td>7.272000</td>\n",
              "      <td>11.793900</td>\n",
              "      <td>16.375900</td>\n",
              "      <td>24.952900</td>\n",
              "      <td>23.419901</td>\n",
              "      <td>15.262000</td>\n",
              "      <td>16.147900</td>\n",
              "      <td>31.952202</td>\n",
              "      <td>16.147900</td>\n",
              "      <td>6.4570</td>\n",
              "      <td>14.316000</td>\n",
              "      <td>25.952801</td>\n",
              "      <td>34.996601</td>\n",
              "      <td>38.301399</td>\n",
              "      <td>48.067802</td>\n",
              "      <td>78.846199</td>\n",
              "      <td>77.943398</td>\n",
              "      <td>59.274899</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1266</th>\n",
              "      <td>10.000600</td>\n",
              "      <td>1.174000</td>\n",
              "      <td>1.999800</td>\n",
              "      <td>1.472900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>4.130600</td>\n",
              "      <td>0.298900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.174000</td>\n",
              "      <td>36.058102</td>\n",
              "      <td>26.123800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>18.096802</td>\n",
              "      <td>1.780900</td>\n",
              "      <td>6.168900</td>\n",
              "      <td>5.304600</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.227900</td>\n",
              "      <td>1.288000</td>\n",
              "      <td>0.298900</td>\n",
              "      <td>0.587000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.227900</td>\n",
              "      <td>2.348000</td>\n",
              "      <td>0.228000</td>\n",
              "      <td>1.174000</td>\n",
              "      <td>7.076400</td>\n",
              "      <td>7.065600</td>\n",
              "      <td>0.587000</td>\n",
              "      <td>0.885900</td>\n",
              "      <td>0.228000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>1.227900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>86.868607</td>\n",
              "      <td>7.054800</td>\n",
              "      <td>15.027901</td>\n",
              "      <td>10.011400</td>\n",
              "      <td>180.774994</td>\n",
              "      <td>86.048904</td>\n",
              "      <td>2.928800</td>\n",
              "      <td>4.820800</td>\n",
              "      <td>11.897201</td>\n",
              "      <td>16.191101</td>\n",
              "      <td>10.310300</td>\n",
              "      <td>13.256101</td>\n",
              "      <td>22.158199</td>\n",
              "      <td>10.848000</td>\n",
              "      <td>7.940700</td>\n",
              "      <td>4.696000</td>\n",
              "      <td>6.467800</td>\n",
              "      <td>3.750000</td>\n",
              "      <td>7.108700</td>\n",
              "      <td>41.295200</td>\n",
              "      <td>38.906898</td>\n",
              "      <td>38.038101</td>\n",
              "      <td>167.743500</td>\n",
              "      <td>101.178398</td>\n",
              "      <td>43.271698</td>\n",
              "      <td>63.827702</td>\n",
              "      <td>0.587000</td>\n",
              "      <td>2.999700</td>\n",
              "      <td>4.994900</td>\n",
              "      <td>7.065600</td>\n",
              "      <td>1.761000</td>\n",
              "      <td>7.9946</td>\n",
              "      <td>29.718300</td>\n",
              "      <td>1.761000</td>\n",
              "      <td>11.484301</td>\n",
              "      <td>0.526900</td>\n",
              "      <td>1.174000</td>\n",
              "      <td>2.945800</td>\n",
              "      <td>2.059900</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>110.901497</td>\n",
              "      <td>107.673798</td>\n",
              "      <td>114.887505</td>\n",
              "      <td>65.003204</td>\n",
              "      <td>55.219700</td>\n",
              "      <td>55.208900</td>\n",
              "      <td>61.894001</td>\n",
              "      <td>68.280205</td>\n",
              "      <td>75.651001</td>\n",
              "      <td>112.772003</td>\n",
              "      <td>199.081009</td>\n",
              "      <td>156.857193</td>\n",
              "      <td>175.806015</td>\n",
              "      <td>170.116302</td>\n",
              "      <td>161.863007</td>\n",
              "      <td>178.876602</td>\n",
              "      <td>186.945007</td>\n",
              "      <td>167.147202</td>\n",
              "      <td>143.889191</td>\n",
              "      <td>110.214600</td>\n",
              "      <td>181.548096</td>\n",
              "      <td>115.800201</td>\n",
              "      <td>83.966202</td>\n",
              "      <td>74.788406</td>\n",
              "      <td>97.103806</td>\n",
              "      <td>172.015808</td>\n",
              "      <td>170.707809</td>\n",
              "      <td>177.952103</td>\n",
              "      <td>151.144196</td>\n",
              "      <td>162.093811</td>\n",
              "      <td>157.225311</td>\n",
              "      <td>153.898911</td>\n",
              "      <td>136.210403</td>\n",
              "      <td>115.076897</td>\n",
              "      <td>149.751297</td>\n",
              "      <td>179.905411</td>\n",
              "      <td>173.895203</td>\n",
              "      <td>170.667511</td>\n",
              "      <td>182.666306</td>\n",
              "      <td>169.938705</td>\n",
              "      <td>...</td>\n",
              "      <td>89.358498</td>\n",
              "      <td>76.738602</td>\n",
              "      <td>89.098106</td>\n",
              "      <td>84.896599</td>\n",
              "      <td>181.835999</td>\n",
              "      <td>189.636795</td>\n",
              "      <td>100.607101</td>\n",
              "      <td>164.977798</td>\n",
              "      <td>174.248016</td>\n",
              "      <td>179.762512</td>\n",
              "      <td>157.838394</td>\n",
              "      <td>196.874817</td>\n",
              "      <td>147.989212</td>\n",
              "      <td>95.783401</td>\n",
              "      <td>80.966400</td>\n",
              "      <td>76.156303</td>\n",
              "      <td>77.123901</td>\n",
              "      <td>86.064606</td>\n",
              "      <td>96.723206</td>\n",
              "      <td>175.893906</td>\n",
              "      <td>151.920715</td>\n",
              "      <td>157.160599</td>\n",
              "      <td>139.983597</td>\n",
              "      <td>131.023102</td>\n",
              "      <td>159.627213</td>\n",
              "      <td>165.985596</td>\n",
              "      <td>157.855408</td>\n",
              "      <td>140.358612</td>\n",
              "      <td>172.935104</td>\n",
              "      <td>119.898499</td>\n",
              "      <td>95.994499</td>\n",
              "      <td>90.6791</td>\n",
              "      <td>117.159103</td>\n",
              "      <td>160.043610</td>\n",
              "      <td>211.918106</td>\n",
              "      <td>207.198715</td>\n",
              "      <td>213.720505</td>\n",
              "      <td>168.191895</td>\n",
              "      <td>85.765701</td>\n",
              "      <td>84.221901</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 2500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0           1           2     ...        2497       2498       2499\n",
              "1789   27.012800   17.908901   14.027900  ...   78.846199  77.943398  59.274899\n",
              "1266   10.000600    1.174000    1.999800  ...    2.945800   2.059900   0.000000\n",
              "464   110.901497  107.673798  114.887505  ...  168.191895  85.765701  84.221901\n",
              "\n",
              "[3 rows x 2500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 27
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv2OuJJXlelv",
        "outputId": "a8242e4d-c234-4a65-b8eb-7813e845e634"
      },
      "source": [
        "train_labels.head(3)"
      ],
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3598    4.0\n",
              "3521    4.0\n",
              "891     1.0\n",
              "Name: 2500, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyBhp4tfle8-",
        "outputId": "cb0ba916-3106-4173-f2d7-bba6d840beb8"
      },
      "source": [
        "test_labels.tail(3)"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "1789    1.0\n",
              "1266    1.0\n",
              "464     0.0\n",
              "Name: 2500, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 29
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2agMcZm66QWP"
      },
      "source": [
        "**One Hot encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guIhg7cdJ1XV"
      },
      "source": [
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzPx3re4l5Fu",
        "outputId": "8e7c8ee6-3581-4270-903d-acecb9cec4c6"
      },
      "source": [
        "train_labels[:3]"
      ],
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 0., 1.],\n",
              "       [0., 0., 0., 0., 1.],\n",
              "       [0., 1., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 31
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyLY2V7mmG5w",
        "outputId": "b06141d9-fdb6-4c1e-fb2c-e015f000b8a7"
      },
      "source": [
        "test_labels[-3:]"
      ],
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 1., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.],\n",
              "       [1., 0., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr5rYzqP6ck0"
      },
      "source": [
        "**Normalizing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQyRPKppQRs7"
      },
      "source": [
        "train_data=train_data.astype(\"float64\")/255\n",
        "test_data = test_data.astype(\"float64\")/255"
      ],
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "ohzFR13FmZ1O",
        "outputId": "20171e26-ff45-417f-8913-bfc09f2d442a"
      },
      "source": [
        "train_data.head(3)"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2460</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3598</th>\n",
              "      <td>0.342164</td>\n",
              "      <td>0.329397</td>\n",
              "      <td>0.255113</td>\n",
              "      <td>0.302813</td>\n",
              "      <td>0.419815</td>\n",
              "      <td>0.466482</td>\n",
              "      <td>0.724801</td>\n",
              "      <td>0.647571</td>\n",
              "      <td>0.712842</td>\n",
              "      <td>0.699158</td>\n",
              "      <td>0.714668</td>\n",
              "      <td>0.622753</td>\n",
              "      <td>0.635315</td>\n",
              "      <td>0.710549</td>\n",
              "      <td>0.706748</td>\n",
              "      <td>0.838704</td>\n",
              "      <td>0.893479</td>\n",
              "      <td>0.964978</td>\n",
              "      <td>0.937161</td>\n",
              "      <td>0.518521</td>\n",
              "      <td>0.714712</td>\n",
              "      <td>0.972687</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.997598</td>\n",
              "      <td>0.997598</td>\n",
              "      <td>0.980101</td>\n",
              "      <td>0.929893</td>\n",
              "      <td>0.571937</td>\n",
              "      <td>0.537498</td>\n",
              "      <td>0.565418</td>\n",
              "      <td>0.576354</td>\n",
              "      <td>0.549969</td>\n",
              "      <td>0.525101</td>\n",
              "      <td>0.489218</td>\n",
              "      <td>0.961516</td>\n",
              "      <td>0.982487</td>\n",
              "      <td>0.993254</td>\n",
              "      <td>0.947930</td>\n",
              "      <td>0.917583</td>\n",
              "      <td>0.882032</td>\n",
              "      <td>...</td>\n",
              "      <td>0.310051</td>\n",
              "      <td>0.168780</td>\n",
              "      <td>0.247434</td>\n",
              "      <td>0.404354</td>\n",
              "      <td>0.961100</td>\n",
              "      <td>0.246696</td>\n",
              "      <td>0.451916</td>\n",
              "      <td>0.380947</td>\n",
              "      <td>0.125780</td>\n",
              "      <td>0.396734</td>\n",
              "      <td>0.376192</td>\n",
              "      <td>0.322189</td>\n",
              "      <td>0.251271</td>\n",
              "      <td>0.302633</td>\n",
              "      <td>0.387691</td>\n",
              "      <td>0.101335</td>\n",
              "      <td>0.095389</td>\n",
              "      <td>0.137755</td>\n",
              "      <td>0.352225</td>\n",
              "      <td>0.372628</td>\n",
              "      <td>0.302529</td>\n",
              "      <td>0.158449</td>\n",
              "      <td>0.156231</td>\n",
              "      <td>0.208723</td>\n",
              "      <td>0.459769</td>\n",
              "      <td>0.226582</td>\n",
              "      <td>0.279304</td>\n",
              "      <td>0.338213</td>\n",
              "      <td>0.384820</td>\n",
              "      <td>0.210856</td>\n",
              "      <td>0.340599</td>\n",
              "      <td>0.268821</td>\n",
              "      <td>0.272042</td>\n",
              "      <td>0.291647</td>\n",
              "      <td>0.352193</td>\n",
              "      <td>0.321573</td>\n",
              "      <td>0.330545</td>\n",
              "      <td>0.312837</td>\n",
              "      <td>0.290584</td>\n",
              "      <td>0.219399</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3521</th>\n",
              "      <td>0.338111</td>\n",
              "      <td>0.294077</td>\n",
              "      <td>0.338238</td>\n",
              "      <td>0.273559</td>\n",
              "      <td>0.281655</td>\n",
              "      <td>0.310149</td>\n",
              "      <td>0.298301</td>\n",
              "      <td>0.359885</td>\n",
              "      <td>0.380724</td>\n",
              "      <td>0.475068</td>\n",
              "      <td>0.569617</td>\n",
              "      <td>0.602813</td>\n",
              "      <td>0.694479</td>\n",
              "      <td>0.654240</td>\n",
              "      <td>0.624532</td>\n",
              "      <td>0.716725</td>\n",
              "      <td>0.648500</td>\n",
              "      <td>0.689622</td>\n",
              "      <td>0.717148</td>\n",
              "      <td>0.752978</td>\n",
              "      <td>0.489841</td>\n",
              "      <td>0.646073</td>\n",
              "      <td>0.673550</td>\n",
              "      <td>0.748886</td>\n",
              "      <td>0.630544</td>\n",
              "      <td>0.557727</td>\n",
              "      <td>0.471365</td>\n",
              "      <td>0.850709</td>\n",
              "      <td>0.780382</td>\n",
              "      <td>0.504951</td>\n",
              "      <td>0.449681</td>\n",
              "      <td>0.360321</td>\n",
              "      <td>0.423157</td>\n",
              "      <td>0.552120</td>\n",
              "      <td>0.731829</td>\n",
              "      <td>0.618695</td>\n",
              "      <td>0.532448</td>\n",
              "      <td>0.392875</td>\n",
              "      <td>0.337337</td>\n",
              "      <td>0.309188</td>\n",
              "      <td>...</td>\n",
              "      <td>0.446962</td>\n",
              "      <td>0.473365</td>\n",
              "      <td>0.722355</td>\n",
              "      <td>0.761567</td>\n",
              "      <td>0.738360</td>\n",
              "      <td>0.697856</td>\n",
              "      <td>0.685579</td>\n",
              "      <td>0.726132</td>\n",
              "      <td>0.749061</td>\n",
              "      <td>0.718036</td>\n",
              "      <td>0.457807</td>\n",
              "      <td>0.336927</td>\n",
              "      <td>0.285372</td>\n",
              "      <td>0.602975</td>\n",
              "      <td>0.498016</td>\n",
              "      <td>0.580807</td>\n",
              "      <td>0.563788</td>\n",
              "      <td>0.835749</td>\n",
              "      <td>0.532696</td>\n",
              "      <td>0.442165</td>\n",
              "      <td>0.669666</td>\n",
              "      <td>0.238529</td>\n",
              "      <td>0.230595</td>\n",
              "      <td>0.628162</td>\n",
              "      <td>0.671404</td>\n",
              "      <td>0.275101</td>\n",
              "      <td>0.497000</td>\n",
              "      <td>0.666673</td>\n",
              "      <td>0.286465</td>\n",
              "      <td>0.458895</td>\n",
              "      <td>0.336861</td>\n",
              "      <td>0.428136</td>\n",
              "      <td>0.431507</td>\n",
              "      <td>0.412578</td>\n",
              "      <td>0.372937</td>\n",
              "      <td>0.376707</td>\n",
              "      <td>0.487484</td>\n",
              "      <td>0.411382</td>\n",
              "      <td>0.492639</td>\n",
              "      <td>0.407890</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>891</th>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.995979</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.972965</td>\n",
              "      <td>0.790634</td>\n",
              "      <td>0.898016</td>\n",
              "      <td>0.998112</td>\n",
              "      <td>0.992058</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.964609</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.984215</td>\n",
              "      <td>0.665706</td>\n",
              "      <td>0.983321</td>\n",
              "      <td>0.991164</td>\n",
              "      <td>0.991164</td>\n",
              "      <td>0.988203</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.984215</td>\n",
              "      <td>0.995085</td>\n",
              "      <td>0.936334</td>\n",
              "      <td>0.996493</td>\n",
              "      <td>0.983321</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999006</td>\n",
              "      <td>0.997665</td>\n",
              "      <td>0.963715</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999006</td>\n",
              "      <td>0.952846</td>\n",
              "      <td>0.996493</td>\n",
              "      <td>0.984560</td>\n",
              "      <td>0.971558</td>\n",
              "      <td>...</td>\n",
              "      <td>0.878108</td>\n",
              "      <td>0.933307</td>\n",
              "      <td>0.937397</td>\n",
              "      <td>0.771813</td>\n",
              "      <td>0.831803</td>\n",
              "      <td>0.971558</td>\n",
              "      <td>0.914148</td>\n",
              "      <td>0.741616</td>\n",
              "      <td>0.422593</td>\n",
              "      <td>0.917833</td>\n",
              "      <td>0.980572</td>\n",
              "      <td>0.772985</td>\n",
              "      <td>0.823683</td>\n",
              "      <td>0.823683</td>\n",
              "      <td>0.835724</td>\n",
              "      <td>0.933518</td>\n",
              "      <td>0.945324</td>\n",
              "      <td>0.913018</td>\n",
              "      <td>0.658251</td>\n",
              "      <td>0.902384</td>\n",
              "      <td>0.847041</td>\n",
              "      <td>0.998112</td>\n",
              "      <td>0.932624</td>\n",
              "      <td>0.967467</td>\n",
              "      <td>0.976440</td>\n",
              "      <td>0.999006</td>\n",
              "      <td>0.999006</td>\n",
              "      <td>0.980294</td>\n",
              "      <td>0.999006</td>\n",
              "      <td>0.959794</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.940188</td>\n",
              "      <td>0.995979</td>\n",
              "      <td>0.992058</td>\n",
              "      <td>0.992058</td>\n",
              "      <td>0.992058</td>\n",
              "      <td>0.992058</td>\n",
              "      <td>0.972452</td>\n",
              "      <td>0.995979</td>\n",
              "      <td>0.995979</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 2500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2     ...      2497      2498      2499\n",
              "3598  0.342164  0.329397  0.255113  ...  0.312837  0.290584  0.219399\n",
              "3521  0.338111  0.294077  0.338238  ...  0.411382  0.492639  0.407890\n",
              "891   0.999900  0.999900  0.999900  ...  0.972452  0.995979  0.995979\n",
              "\n",
              "[3 rows x 2500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "WFREk3P1miXa",
        "outputId": "896157aa-8c45-4b10-88df-9fdeb03c4ac3"
      },
      "source": [
        "test_data.tail(3)"
      ],
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2460</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>1789</th>\n",
              "      <td>0.105933</td>\n",
              "      <td>0.070231</td>\n",
              "      <td>0.055011</td>\n",
              "      <td>0.071361</td>\n",
              "      <td>0.107105</td>\n",
              "      <td>0.150008</td>\n",
              "      <td>0.102905</td>\n",
              "      <td>0.090713</td>\n",
              "      <td>0.064455</td>\n",
              "      <td>0.090713</td>\n",
              "      <td>0.106827</td>\n",
              "      <td>0.103588</td>\n",
              "      <td>0.118572</td>\n",
              "      <td>0.145960</td>\n",
              "      <td>0.176695</td>\n",
              "      <td>0.232837</td>\n",
              "      <td>0.371052</td>\n",
              "      <td>0.373094</td>\n",
              "      <td>0.282006</td>\n",
              "      <td>0.160050</td>\n",
              "      <td>0.247029</td>\n",
              "      <td>0.262974</td>\n",
              "      <td>0.161180</td>\n",
              "      <td>0.102905</td>\n",
              "      <td>0.109811</td>\n",
              "      <td>0.105890</td>\n",
              "      <td>0.109364</td>\n",
              "      <td>0.154123</td>\n",
              "      <td>0.293728</td>\n",
              "      <td>0.356667</td>\n",
              "      <td>0.361464</td>\n",
              "      <td>0.376490</td>\n",
              "      <td>0.312548</td>\n",
              "      <td>0.325720</td>\n",
              "      <td>0.219194</td>\n",
              "      <td>0.345543</td>\n",
              "      <td>0.356370</td>\n",
              "      <td>0.298929</td>\n",
              "      <td>0.267940</td>\n",
              "      <td>0.266170</td>\n",
              "      <td>...</td>\n",
              "      <td>0.334136</td>\n",
              "      <td>0.269838</td>\n",
              "      <td>0.274653</td>\n",
              "      <td>0.260140</td>\n",
              "      <td>0.263264</td>\n",
              "      <td>0.196749</td>\n",
              "      <td>0.368816</td>\n",
              "      <td>0.013812</td>\n",
              "      <td>0.250902</td>\n",
              "      <td>0.108192</td>\n",
              "      <td>0.447459</td>\n",
              "      <td>0.482647</td>\n",
              "      <td>0.183559</td>\n",
              "      <td>0.502126</td>\n",
              "      <td>0.352746</td>\n",
              "      <td>0.330432</td>\n",
              "      <td>0.432692</td>\n",
              "      <td>0.259542</td>\n",
              "      <td>0.219236</td>\n",
              "      <td>0.257198</td>\n",
              "      <td>0.074110</td>\n",
              "      <td>0.016114</td>\n",
              "      <td>0.028518</td>\n",
              "      <td>0.046251</td>\n",
              "      <td>0.064219</td>\n",
              "      <td>0.097855</td>\n",
              "      <td>0.091843</td>\n",
              "      <td>0.059851</td>\n",
              "      <td>0.063325</td>\n",
              "      <td>0.125303</td>\n",
              "      <td>0.063325</td>\n",
              "      <td>0.025322</td>\n",
              "      <td>0.056141</td>\n",
              "      <td>0.101776</td>\n",
              "      <td>0.137242</td>\n",
              "      <td>0.150202</td>\n",
              "      <td>0.188501</td>\n",
              "      <td>0.309201</td>\n",
              "      <td>0.305660</td>\n",
              "      <td>0.232451</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1266</th>\n",
              "      <td>0.039218</td>\n",
              "      <td>0.004604</td>\n",
              "      <td>0.007842</td>\n",
              "      <td>0.005776</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.016198</td>\n",
              "      <td>0.001172</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004604</td>\n",
              "      <td>0.141404</td>\n",
              "      <td>0.102446</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.070968</td>\n",
              "      <td>0.006984</td>\n",
              "      <td>0.024192</td>\n",
              "      <td>0.020802</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>0.005051</td>\n",
              "      <td>0.001172</td>\n",
              "      <td>0.002302</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>0.009208</td>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.004604</td>\n",
              "      <td>0.027751</td>\n",
              "      <td>0.027708</td>\n",
              "      <td>0.002302</td>\n",
              "      <td>0.003474</td>\n",
              "      <td>0.000894</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.004815</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>...</td>\n",
              "      <td>0.340661</td>\n",
              "      <td>0.027666</td>\n",
              "      <td>0.058933</td>\n",
              "      <td>0.039260</td>\n",
              "      <td>0.708922</td>\n",
              "      <td>0.337447</td>\n",
              "      <td>0.011485</td>\n",
              "      <td>0.018905</td>\n",
              "      <td>0.046656</td>\n",
              "      <td>0.063495</td>\n",
              "      <td>0.040433</td>\n",
              "      <td>0.051985</td>\n",
              "      <td>0.086895</td>\n",
              "      <td>0.042541</td>\n",
              "      <td>0.031140</td>\n",
              "      <td>0.018416</td>\n",
              "      <td>0.025364</td>\n",
              "      <td>0.014706</td>\n",
              "      <td>0.027877</td>\n",
              "      <td>0.161942</td>\n",
              "      <td>0.152576</td>\n",
              "      <td>0.149169</td>\n",
              "      <td>0.657818</td>\n",
              "      <td>0.396778</td>\n",
              "      <td>0.169693</td>\n",
              "      <td>0.250305</td>\n",
              "      <td>0.002302</td>\n",
              "      <td>0.011764</td>\n",
              "      <td>0.019588</td>\n",
              "      <td>0.027708</td>\n",
              "      <td>0.006906</td>\n",
              "      <td>0.031351</td>\n",
              "      <td>0.116542</td>\n",
              "      <td>0.006906</td>\n",
              "      <td>0.045036</td>\n",
              "      <td>0.002066</td>\n",
              "      <td>0.004604</td>\n",
              "      <td>0.011552</td>\n",
              "      <td>0.008078</td>\n",
              "      <td>0.000000</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>464</th>\n",
              "      <td>0.434908</td>\n",
              "      <td>0.422250</td>\n",
              "      <td>0.450539</td>\n",
              "      <td>0.254915</td>\n",
              "      <td>0.216548</td>\n",
              "      <td>0.216505</td>\n",
              "      <td>0.242722</td>\n",
              "      <td>0.267766</td>\n",
              "      <td>0.296671</td>\n",
              "      <td>0.442243</td>\n",
              "      <td>0.780710</td>\n",
              "      <td>0.615126</td>\n",
              "      <td>0.689435</td>\n",
              "      <td>0.667123</td>\n",
              "      <td>0.634757</td>\n",
              "      <td>0.701477</td>\n",
              "      <td>0.733118</td>\n",
              "      <td>0.655479</td>\n",
              "      <td>0.564271</td>\n",
              "      <td>0.432214</td>\n",
              "      <td>0.711953</td>\n",
              "      <td>0.454118</td>\n",
              "      <td>0.329279</td>\n",
              "      <td>0.293288</td>\n",
              "      <td>0.380799</td>\n",
              "      <td>0.674572</td>\n",
              "      <td>0.669442</td>\n",
              "      <td>0.697851</td>\n",
              "      <td>0.592722</td>\n",
              "      <td>0.635662</td>\n",
              "      <td>0.616570</td>\n",
              "      <td>0.603525</td>\n",
              "      <td>0.534158</td>\n",
              "      <td>0.451282</td>\n",
              "      <td>0.587260</td>\n",
              "      <td>0.705511</td>\n",
              "      <td>0.681942</td>\n",
              "      <td>0.669284</td>\n",
              "      <td>0.716338</td>\n",
              "      <td>0.666426</td>\n",
              "      <td>...</td>\n",
              "      <td>0.350425</td>\n",
              "      <td>0.300936</td>\n",
              "      <td>0.349404</td>\n",
              "      <td>0.332928</td>\n",
              "      <td>0.713082</td>\n",
              "      <td>0.743674</td>\n",
              "      <td>0.394538</td>\n",
              "      <td>0.646972</td>\n",
              "      <td>0.683326</td>\n",
              "      <td>0.704951</td>\n",
              "      <td>0.618974</td>\n",
              "      <td>0.772058</td>\n",
              "      <td>0.580350</td>\n",
              "      <td>0.375621</td>\n",
              "      <td>0.317515</td>\n",
              "      <td>0.298652</td>\n",
              "      <td>0.302447</td>\n",
              "      <td>0.337508</td>\n",
              "      <td>0.379307</td>\n",
              "      <td>0.689780</td>\n",
              "      <td>0.595768</td>\n",
              "      <td>0.616316</td>\n",
              "      <td>0.548955</td>\n",
              "      <td>0.513816</td>\n",
              "      <td>0.625989</td>\n",
              "      <td>0.650924</td>\n",
              "      <td>0.619041</td>\n",
              "      <td>0.550426</td>\n",
              "      <td>0.678177</td>\n",
              "      <td>0.470190</td>\n",
              "      <td>0.376449</td>\n",
              "      <td>0.355604</td>\n",
              "      <td>0.459447</td>\n",
              "      <td>0.627622</td>\n",
              "      <td>0.831051</td>\n",
              "      <td>0.812544</td>\n",
              "      <td>0.838120</td>\n",
              "      <td>0.659576</td>\n",
              "      <td>0.336336</td>\n",
              "      <td>0.330282</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 2500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2     ...      2497      2498      2499\n",
              "1789  0.105933  0.070231  0.055011  ...  0.309201  0.305660  0.232451\n",
              "1266  0.039218  0.004604  0.007842  ...  0.011552  0.008078  0.000000\n",
              "464   0.434908  0.422250  0.450539  ...  0.659576  0.336336  0.330282\n",
              "\n",
              "[3 rows x 2500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN7t-Yp03Aq6"
      },
      "source": [
        "**Building the Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA1kj1SirGOl"
      },
      "source": [
        "def build_model():\n",
        "  model=models.Sequential()\n",
        "  \n",
        "  model.add(layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(l2=.002), input_shape=(pic_size*pic_size,)))\n",
        "  model.add(layers.Dropout(.2))\n",
        "\n",
        "  model.add(layers.Dense(5, activation=\"softmax\"))\n",
        "  model.compile(optimizer=\"rmsprop\", loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJIIzSwg27Co"
      },
      "source": [
        "**K fold Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh-zIkt0RJZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a599ee05-6634-45c0-d51c-f196a7630565"
      },
      "source": [
        "k= 5\n",
        "num_val_sample = len(train_data) // k\n",
        "num_epochs =100\n",
        "all_scores = []\n",
        "all_val_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_loss_histories =[]\n",
        "all_acc_histories = []\n",
        "\n",
        "for i in range(k):\n",
        "  print(\"processing fold #\",i)\n",
        "  val_data = train_data[i*num_val_sample:(i+1)*num_val_sample]\n",
        "  val_labels = train_labels[i*num_val_sample: (i+1)*num_val_sample]\n",
        "\n",
        "  partial_train_data=np.concatenate([train_data[:i*num_val_sample], train_data[(i+1)*num_val_sample:]], axis=0)\n",
        "  partial_train_labels=np.concatenate([train_labels[:i*num_val_sample], train_labels[(i+1)*num_val_sample:]], axis=0)\n",
        "\n",
        "  model=build_model()\n",
        "  history = model.fit(partial_train_data, partial_train_labels, validation_data = (val_data, val_labels), epochs= num_epochs,batch_size=8, verbose=1)\n",
        "  val_loss, val_acc = model.evaluate(test_data, test_labels, verbose=0)\n",
        "  val_loss_history = history.history[\"val_loss\"]\n",
        "  val_acc_history = history.history[\"val_accuracy\"]\n",
        "  loss_history = history.history[\"loss\"]\n",
        "  acc_history = history.history[\"accuracy\"]\n",
        "  all_loss_histories.append(loss_history)\n",
        "  all_acc_histories.append(acc_history)\n",
        "  all_val_loss_histories.append(val_loss_history)\n",
        "  all_val_acc_histories.append(val_acc_history)\n",
        "  all_scores.append(val_acc)\n",
        "ave_val_loss_hist = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n",
        "ave_loss_hist = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "ave_val_acc_hist = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "ave_acc_hist = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "all_scores\n"
      ],
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "Epoch 1/100\n",
            "260/260 [==============================] - 2s 5ms/step - loss: 1.9776 - accuracy: 0.2352 - val_loss: 1.5723 - val_accuracy: 0.2181\n",
            "Epoch 2/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.6324 - accuracy: 0.2618 - val_loss: 1.5466 - val_accuracy: 0.2838\n",
            "Epoch 3/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.6274 - accuracy: 0.2748 - val_loss: 1.5610 - val_accuracy: 0.3031\n",
            "Epoch 4/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5965 - accuracy: 0.2926 - val_loss: 1.6394 - val_accuracy: 0.2741\n",
            "Epoch 5/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5810 - accuracy: 0.2776 - val_loss: 1.5747 - val_accuracy: 0.2876\n",
            "Epoch 6/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5701 - accuracy: 0.3007 - val_loss: 1.5306 - val_accuracy: 0.3340\n",
            "Epoch 7/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5776 - accuracy: 0.2914 - val_loss: 1.5378 - val_accuracy: 0.3456\n",
            "Epoch 8/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5950 - accuracy: 0.2762 - val_loss: 1.6329 - val_accuracy: 0.2896\n",
            "Epoch 9/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5723 - accuracy: 0.2943 - val_loss: 1.5477 - val_accuracy: 0.2510\n",
            "Epoch 10/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5472 - accuracy: 0.3052 - val_loss: 1.5767 - val_accuracy: 0.2954\n",
            "Epoch 11/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5342 - accuracy: 0.3075 - val_loss: 1.6003 - val_accuracy: 0.3108\n",
            "Epoch 12/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5425 - accuracy: 0.3074 - val_loss: 1.5433 - val_accuracy: 0.2741\n",
            "Epoch 13/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5276 - accuracy: 0.3230 - val_loss: 1.5606 - val_accuracy: 0.2664\n",
            "Epoch 14/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5182 - accuracy: 0.3014 - val_loss: 1.6449 - val_accuracy: 0.2838\n",
            "Epoch 15/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5277 - accuracy: 0.2878 - val_loss: 1.5378 - val_accuracy: 0.3359\n",
            "Epoch 16/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5213 - accuracy: 0.3108 - val_loss: 1.5501 - val_accuracy: 0.3359\n",
            "Epoch 17/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5243 - accuracy: 0.3198 - val_loss: 1.5487 - val_accuracy: 0.3205\n",
            "Epoch 18/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5179 - accuracy: 0.3257 - val_loss: 1.5990 - val_accuracy: 0.2954\n",
            "Epoch 19/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4715 - accuracy: 0.3406 - val_loss: 1.5797 - val_accuracy: 0.3127\n",
            "Epoch 20/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5101 - accuracy: 0.3259 - val_loss: 1.6159 - val_accuracy: 0.2548\n",
            "Epoch 21/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5240 - accuracy: 0.3333 - val_loss: 1.5592 - val_accuracy: 0.3263\n",
            "Epoch 22/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4923 - accuracy: 0.3214 - val_loss: 1.5596 - val_accuracy: 0.3263\n",
            "Epoch 23/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4975 - accuracy: 0.3162 - val_loss: 1.5490 - val_accuracy: 0.3166\n",
            "Epoch 24/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4825 - accuracy: 0.3401 - val_loss: 1.5783 - val_accuracy: 0.3359\n",
            "Epoch 25/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4801 - accuracy: 0.3511 - val_loss: 1.5555 - val_accuracy: 0.3127\n",
            "Epoch 26/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4896 - accuracy: 0.3089 - val_loss: 1.6407 - val_accuracy: 0.3031\n",
            "Epoch 27/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5031 - accuracy: 0.3330 - val_loss: 1.5701 - val_accuracy: 0.2934\n",
            "Epoch 28/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5153 - accuracy: 0.3244 - val_loss: 1.6549 - val_accuracy: 0.2741\n",
            "Epoch 29/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4810 - accuracy: 0.3355 - val_loss: 1.5625 - val_accuracy: 0.3243\n",
            "Epoch 30/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4748 - accuracy: 0.3315 - val_loss: 1.6382 - val_accuracy: 0.3301\n",
            "Epoch 31/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4866 - accuracy: 0.3519 - val_loss: 1.6596 - val_accuracy: 0.2992\n",
            "Epoch 32/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4626 - accuracy: 0.3391 - val_loss: 1.5988 - val_accuracy: 0.2973\n",
            "Epoch 33/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4559 - accuracy: 0.3760 - val_loss: 1.5813 - val_accuracy: 0.3224\n",
            "Epoch 34/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4702 - accuracy: 0.3470 - val_loss: 1.6080 - val_accuracy: 0.3282\n",
            "Epoch 35/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4792 - accuracy: 0.3505 - val_loss: 1.6137 - val_accuracy: 0.3012\n",
            "Epoch 36/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4781 - accuracy: 0.3509 - val_loss: 1.6071 - val_accuracy: 0.3340\n",
            "Epoch 37/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4655 - accuracy: 0.3455 - val_loss: 1.7924 - val_accuracy: 0.2954\n",
            "Epoch 38/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4616 - accuracy: 0.3532 - val_loss: 1.6275 - val_accuracy: 0.3205\n",
            "Epoch 39/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4676 - accuracy: 0.3569 - val_loss: 1.7648 - val_accuracy: 0.2761\n",
            "Epoch 40/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4803 - accuracy: 0.3369 - val_loss: 1.7578 - val_accuracy: 0.2587\n",
            "Epoch 41/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4631 - accuracy: 0.3449 - val_loss: 1.6011 - val_accuracy: 0.2722\n",
            "Epoch 42/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4610 - accuracy: 0.3510 - val_loss: 1.6900 - val_accuracy: 0.2973\n",
            "Epoch 43/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4520 - accuracy: 0.3571 - val_loss: 1.7441 - val_accuracy: 0.3069\n",
            "Epoch 44/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4445 - accuracy: 0.3671 - val_loss: 2.0708 - val_accuracy: 0.2432\n",
            "Epoch 45/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4466 - accuracy: 0.3820 - val_loss: 1.6209 - val_accuracy: 0.3089\n",
            "Epoch 46/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4456 - accuracy: 0.3620 - val_loss: 1.6154 - val_accuracy: 0.2954\n",
            "Epoch 47/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4546 - accuracy: 0.3423 - val_loss: 1.6136 - val_accuracy: 0.2645\n",
            "Epoch 48/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4507 - accuracy: 0.3695 - val_loss: 1.6345 - val_accuracy: 0.3359\n",
            "Epoch 49/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4385 - accuracy: 0.3638 - val_loss: 1.6897 - val_accuracy: 0.3301\n",
            "Epoch 50/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4476 - accuracy: 0.3557 - val_loss: 1.6353 - val_accuracy: 0.3205\n",
            "Epoch 51/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4141 - accuracy: 0.3932 - val_loss: 1.8520 - val_accuracy: 0.2432\n",
            "Epoch 52/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4358 - accuracy: 0.3817 - val_loss: 1.7232 - val_accuracy: 0.3147\n",
            "Epoch 53/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4325 - accuracy: 0.3889 - val_loss: 1.9619 - val_accuracy: 0.2741\n",
            "Epoch 54/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4632 - accuracy: 0.3649 - val_loss: 1.7267 - val_accuracy: 0.3089\n",
            "Epoch 55/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4333 - accuracy: 0.3561 - val_loss: 1.6270 - val_accuracy: 0.3012\n",
            "Epoch 56/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4267 - accuracy: 0.3813 - val_loss: 1.8281 - val_accuracy: 0.2934\n",
            "Epoch 57/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4210 - accuracy: 0.3933 - val_loss: 1.7209 - val_accuracy: 0.2857\n",
            "Epoch 58/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4179 - accuracy: 0.3873 - val_loss: 1.7824 - val_accuracy: 0.3069\n",
            "Epoch 59/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4184 - accuracy: 0.3691 - val_loss: 1.6277 - val_accuracy: 0.2625\n",
            "Epoch 60/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4104 - accuracy: 0.3762 - val_loss: 1.8083 - val_accuracy: 0.3205\n",
            "Epoch 61/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4316 - accuracy: 0.3768 - val_loss: 2.0597 - val_accuracy: 0.2297\n",
            "Epoch 62/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4212 - accuracy: 0.4053 - val_loss: 1.8506 - val_accuracy: 0.3050\n",
            "Epoch 63/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4411 - accuracy: 0.3894 - val_loss: 1.7282 - val_accuracy: 0.3263\n",
            "Epoch 64/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4207 - accuracy: 0.3698 - val_loss: 1.6778 - val_accuracy: 0.3340\n",
            "Epoch 65/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3890 - accuracy: 0.4115 - val_loss: 2.2462 - val_accuracy: 0.2510\n",
            "Epoch 66/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4329 - accuracy: 0.3786 - val_loss: 3.8653 - val_accuracy: 0.1680\n",
            "Epoch 67/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4186 - accuracy: 0.4110 - val_loss: 1.8473 - val_accuracy: 0.3050\n",
            "Epoch 68/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4398 - accuracy: 0.3857 - val_loss: 1.9561 - val_accuracy: 0.3031\n",
            "Epoch 69/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4406 - accuracy: 0.3608 - val_loss: 1.6747 - val_accuracy: 0.2587\n",
            "Epoch 70/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4562 - accuracy: 0.3695 - val_loss: 1.6787 - val_accuracy: 0.2934\n",
            "Epoch 71/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3961 - accuracy: 0.4132 - val_loss: 2.4131 - val_accuracy: 0.2085\n",
            "Epoch 72/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4163 - accuracy: 0.3962 - val_loss: 2.6224 - val_accuracy: 0.2027\n",
            "Epoch 73/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3869 - accuracy: 0.4362 - val_loss: 1.7010 - val_accuracy: 0.3359\n",
            "Epoch 74/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4294 - accuracy: 0.3916 - val_loss: 1.8616 - val_accuracy: 0.3147\n",
            "Epoch 75/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4096 - accuracy: 0.4036 - val_loss: 1.7684 - val_accuracy: 0.2741\n",
            "Epoch 76/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4183 - accuracy: 0.4011 - val_loss: 1.7602 - val_accuracy: 0.3050\n",
            "Epoch 77/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4420 - accuracy: 0.3893 - val_loss: 1.8786 - val_accuracy: 0.3031\n",
            "Epoch 78/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4007 - accuracy: 0.3988 - val_loss: 1.6955 - val_accuracy: 0.2471\n",
            "Epoch 79/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4204 - accuracy: 0.3971 - val_loss: 1.9214 - val_accuracy: 0.3069\n",
            "Epoch 80/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3918 - accuracy: 0.4207 - val_loss: 1.7830 - val_accuracy: 0.2529\n",
            "Epoch 81/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4328 - accuracy: 0.3833 - val_loss: 1.8988 - val_accuracy: 0.3185\n",
            "Epoch 82/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3670 - accuracy: 0.4094 - val_loss: 2.5940 - val_accuracy: 0.2278\n",
            "Epoch 83/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4676 - accuracy: 0.4131 - val_loss: 1.8677 - val_accuracy: 0.2992\n",
            "Epoch 84/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4132 - accuracy: 0.3971 - val_loss: 1.8333 - val_accuracy: 0.3185\n",
            "Epoch 85/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3864 - accuracy: 0.3977 - val_loss: 1.6982 - val_accuracy: 0.2529\n",
            "Epoch 86/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3987 - accuracy: 0.4087 - val_loss: 2.0385 - val_accuracy: 0.2992\n",
            "Epoch 87/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4343 - accuracy: 0.3953 - val_loss: 1.7560 - val_accuracy: 0.2471\n",
            "Epoch 88/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3749 - accuracy: 0.4072 - val_loss: 2.2934 - val_accuracy: 0.2973\n",
            "Epoch 89/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3975 - accuracy: 0.4091 - val_loss: 1.8049 - val_accuracy: 0.2973\n",
            "Epoch 90/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4243 - accuracy: 0.3808 - val_loss: 2.5961 - val_accuracy: 0.2027\n",
            "Epoch 91/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4190 - accuracy: 0.4038 - val_loss: 1.7163 - val_accuracy: 0.2645\n",
            "Epoch 92/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4228 - accuracy: 0.4032 - val_loss: 1.7454 - val_accuracy: 0.2761\n",
            "Epoch 93/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3787 - accuracy: 0.3884 - val_loss: 1.8156 - val_accuracy: 0.3147\n",
            "Epoch 94/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3999 - accuracy: 0.3915 - val_loss: 1.9647 - val_accuracy: 0.3185\n",
            "Epoch 95/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3970 - accuracy: 0.4044 - val_loss: 2.0202 - val_accuracy: 0.3108\n",
            "Epoch 96/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4095 - accuracy: 0.4151 - val_loss: 1.9151 - val_accuracy: 0.2548\n",
            "Epoch 97/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3971 - accuracy: 0.3904 - val_loss: 3.0209 - val_accuracy: 0.1988\n",
            "Epoch 98/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4353 - accuracy: 0.4154 - val_loss: 1.7241 - val_accuracy: 0.2317\n",
            "Epoch 99/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4073 - accuracy: 0.4132 - val_loss: 1.7229 - val_accuracy: 0.2510\n",
            "Epoch 100/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3563 - accuracy: 0.4334 - val_loss: 1.8578 - val_accuracy: 0.3340\n",
            "processing fold # 1\n",
            "Epoch 1/100\n",
            "260/260 [==============================] - 2s 5ms/step - loss: 1.9294 - accuracy: 0.2413 - val_loss: 1.5918 - val_accuracy: 0.2085\n",
            "Epoch 2/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.6226 - accuracy: 0.2877 - val_loss: 1.9359 - val_accuracy: 0.2780\n",
            "Epoch 3/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.6228 - accuracy: 0.2503 - val_loss: 1.6288 - val_accuracy: 0.2934\n",
            "Epoch 4/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5952 - accuracy: 0.2660 - val_loss: 1.5478 - val_accuracy: 0.2703\n",
            "Epoch 5/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.6090 - accuracy: 0.2686 - val_loss: 1.5291 - val_accuracy: 0.3089\n",
            "Epoch 6/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5873 - accuracy: 0.2850 - val_loss: 1.5200 - val_accuracy: 0.3147\n",
            "Epoch 7/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5963 - accuracy: 0.2640 - val_loss: 2.0939 - val_accuracy: 0.2838\n",
            "Epoch 8/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5891 - accuracy: 0.2842 - val_loss: 1.5379 - val_accuracy: 0.3340\n",
            "Epoch 9/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5551 - accuracy: 0.2703 - val_loss: 1.5623 - val_accuracy: 0.2432\n",
            "Epoch 10/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5543 - accuracy: 0.3146 - val_loss: 1.5628 - val_accuracy: 0.3012\n",
            "Epoch 11/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5411 - accuracy: 0.2994 - val_loss: 1.5525 - val_accuracy: 0.2876\n",
            "Epoch 12/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5448 - accuracy: 0.2804 - val_loss: 1.5370 - val_accuracy: 0.2703\n",
            "Epoch 13/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5349 - accuracy: 0.3083 - val_loss: 1.5417 - val_accuracy: 0.2683\n",
            "Epoch 14/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5160 - accuracy: 0.3178 - val_loss: 1.5351 - val_accuracy: 0.2992\n",
            "Epoch 15/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5226 - accuracy: 0.3062 - val_loss: 1.5599 - val_accuracy: 0.3031\n",
            "Epoch 16/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4928 - accuracy: 0.3194 - val_loss: 1.5546 - val_accuracy: 0.2896\n",
            "Epoch 17/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4904 - accuracy: 0.3228 - val_loss: 1.5503 - val_accuracy: 0.3166\n",
            "Epoch 18/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5168 - accuracy: 0.3105 - val_loss: 1.6920 - val_accuracy: 0.3147\n",
            "Epoch 19/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5125 - accuracy: 0.3214 - val_loss: 1.6595 - val_accuracy: 0.3012\n",
            "Epoch 20/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5081 - accuracy: 0.3289 - val_loss: 1.6054 - val_accuracy: 0.2722\n",
            "Epoch 21/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4988 - accuracy: 0.3342 - val_loss: 1.5843 - val_accuracy: 0.2683\n",
            "Epoch 22/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4962 - accuracy: 0.3364 - val_loss: 1.6877 - val_accuracy: 0.3069\n",
            "Epoch 23/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4969 - accuracy: 0.3121 - val_loss: 1.5519 - val_accuracy: 0.3359\n",
            "Epoch 24/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4873 - accuracy: 0.3253 - val_loss: 1.6051 - val_accuracy: 0.2780\n",
            "Epoch 25/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4692 - accuracy: 0.3398 - val_loss: 1.5584 - val_accuracy: 0.3340\n",
            "Epoch 26/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4725 - accuracy: 0.3540 - val_loss: 1.5559 - val_accuracy: 0.2799\n",
            "Epoch 27/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4745 - accuracy: 0.3431 - val_loss: 1.5786 - val_accuracy: 0.2915\n",
            "Epoch 28/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4570 - accuracy: 0.3833 - val_loss: 1.5721 - val_accuracy: 0.2722\n",
            "Epoch 29/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4639 - accuracy: 0.3396 - val_loss: 1.6729 - val_accuracy: 0.3012\n",
            "Epoch 30/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4745 - accuracy: 0.3509 - val_loss: 1.5951 - val_accuracy: 0.2915\n",
            "Epoch 31/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4761 - accuracy: 0.3636 - val_loss: 1.5713 - val_accuracy: 0.3108\n",
            "Epoch 32/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4698 - accuracy: 0.3716 - val_loss: 1.5848 - val_accuracy: 0.2934\n",
            "Epoch 33/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4695 - accuracy: 0.3482 - val_loss: 1.6053 - val_accuracy: 0.2664\n",
            "Epoch 34/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4407 - accuracy: 0.3651 - val_loss: 1.6310 - val_accuracy: 0.3340\n",
            "Epoch 35/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4455 - accuracy: 0.3775 - val_loss: 1.6444 - val_accuracy: 0.3127\n",
            "Epoch 36/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4347 - accuracy: 0.3815 - val_loss: 1.6691 - val_accuracy: 0.2741\n",
            "Epoch 37/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4611 - accuracy: 0.3685 - val_loss: 1.7171 - val_accuracy: 0.2857\n",
            "Epoch 38/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4530 - accuracy: 0.3782 - val_loss: 1.5782 - val_accuracy: 0.2761\n",
            "Epoch 39/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4323 - accuracy: 0.3561 - val_loss: 1.6355 - val_accuracy: 0.2664\n",
            "Epoch 40/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4497 - accuracy: 0.3737 - val_loss: 1.8093 - val_accuracy: 0.3050\n",
            "Epoch 41/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4523 - accuracy: 0.3719 - val_loss: 1.6527 - val_accuracy: 0.2819\n",
            "Epoch 42/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4128 - accuracy: 0.4071 - val_loss: 1.6000 - val_accuracy: 0.2568\n",
            "Epoch 43/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4600 - accuracy: 0.3742 - val_loss: 1.8162 - val_accuracy: 0.3340\n",
            "Epoch 44/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4451 - accuracy: 0.3965 - val_loss: 1.7101 - val_accuracy: 0.2664\n",
            "Epoch 45/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4208 - accuracy: 0.4131 - val_loss: 2.0900 - val_accuracy: 0.2008\n",
            "Epoch 46/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4130 - accuracy: 0.3862 - val_loss: 1.6590 - val_accuracy: 0.3050\n",
            "Epoch 47/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4127 - accuracy: 0.3948 - val_loss: 1.6143 - val_accuracy: 0.2471\n",
            "Epoch 48/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4324 - accuracy: 0.3760 - val_loss: 1.7152 - val_accuracy: 0.2876\n",
            "Epoch 49/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4171 - accuracy: 0.3839 - val_loss: 1.6149 - val_accuracy: 0.2375\n",
            "Epoch 50/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4089 - accuracy: 0.3766 - val_loss: 2.0411 - val_accuracy: 0.2510\n",
            "Epoch 51/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4412 - accuracy: 0.4166 - val_loss: 1.8241 - val_accuracy: 0.3243\n",
            "Epoch 52/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3805 - accuracy: 0.4090 - val_loss: 1.6137 - val_accuracy: 0.2683\n",
            "Epoch 53/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4080 - accuracy: 0.3930 - val_loss: 1.6766 - val_accuracy: 0.3127\n",
            "Epoch 54/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3879 - accuracy: 0.4064 - val_loss: 1.7093 - val_accuracy: 0.2471\n",
            "Epoch 55/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3961 - accuracy: 0.4142 - val_loss: 1.8338 - val_accuracy: 0.2741\n",
            "Epoch 56/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4144 - accuracy: 0.4217 - val_loss: 1.8235 - val_accuracy: 0.2741\n",
            "Epoch 57/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4201 - accuracy: 0.4056 - val_loss: 1.7353 - val_accuracy: 0.2529\n",
            "Epoch 58/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3941 - accuracy: 0.4124 - val_loss: 2.6050 - val_accuracy: 0.2413\n",
            "Epoch 59/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4446 - accuracy: 0.4289 - val_loss: 1.8717 - val_accuracy: 0.2780\n",
            "Epoch 60/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4087 - accuracy: 0.4131 - val_loss: 1.7209 - val_accuracy: 0.2104\n",
            "Epoch 61/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3781 - accuracy: 0.4176 - val_loss: 1.6633 - val_accuracy: 0.2394\n",
            "Epoch 62/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3986 - accuracy: 0.4065 - val_loss: 1.6340 - val_accuracy: 0.2664\n",
            "Epoch 63/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3836 - accuracy: 0.4396 - val_loss: 1.9689 - val_accuracy: 0.2452\n",
            "Epoch 64/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4136 - accuracy: 0.4058 - val_loss: 1.6983 - val_accuracy: 0.3031\n",
            "Epoch 65/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3944 - accuracy: 0.4116 - val_loss: 1.9720 - val_accuracy: 0.2529\n",
            "Epoch 66/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3779 - accuracy: 0.4252 - val_loss: 1.8039 - val_accuracy: 0.2413\n",
            "Epoch 67/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3682 - accuracy: 0.4307 - val_loss: 2.5417 - val_accuracy: 0.2915\n",
            "Epoch 68/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4041 - accuracy: 0.4335 - val_loss: 1.9147 - val_accuracy: 0.2683\n",
            "Epoch 69/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3521 - accuracy: 0.4415 - val_loss: 1.8234 - val_accuracy: 0.3031\n",
            "Epoch 70/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3735 - accuracy: 0.4299 - val_loss: 1.8483 - val_accuracy: 0.2838\n",
            "Epoch 71/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3734 - accuracy: 0.4242 - val_loss: 1.6672 - val_accuracy: 0.2317\n",
            "Epoch 72/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3542 - accuracy: 0.4329 - val_loss: 1.7538 - val_accuracy: 0.2741\n",
            "Epoch 73/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3416 - accuracy: 0.4260 - val_loss: 1.8188 - val_accuracy: 0.2181\n",
            "Epoch 74/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3773 - accuracy: 0.4389 - val_loss: 2.0940 - val_accuracy: 0.2606\n",
            "Epoch 75/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4125 - accuracy: 0.4156 - val_loss: 1.7673 - val_accuracy: 0.2606\n",
            "Epoch 76/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3712 - accuracy: 0.4177 - val_loss: 2.1472 - val_accuracy: 0.2934\n",
            "Epoch 77/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3476 - accuracy: 0.4427 - val_loss: 1.7725 - val_accuracy: 0.2915\n",
            "Epoch 78/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3260 - accuracy: 0.4522 - val_loss: 2.0183 - val_accuracy: 0.2548\n",
            "Epoch 79/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3713 - accuracy: 0.4320 - val_loss: 1.7836 - val_accuracy: 0.2741\n",
            "Epoch 80/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3505 - accuracy: 0.4271 - val_loss: 1.7433 - val_accuracy: 0.2336\n",
            "Epoch 81/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3583 - accuracy: 0.4379 - val_loss: 1.9721 - val_accuracy: 0.2336\n",
            "Epoch 82/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3815 - accuracy: 0.4344 - val_loss: 1.8867 - val_accuracy: 0.2915\n",
            "Epoch 83/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3604 - accuracy: 0.4423 - val_loss: 1.7685 - val_accuracy: 0.3166\n",
            "Epoch 84/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3870 - accuracy: 0.4538 - val_loss: 1.7717 - val_accuracy: 0.2838\n",
            "Epoch 85/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3460 - accuracy: 0.4509 - val_loss: 2.4008 - val_accuracy: 0.2606\n",
            "Epoch 86/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4071 - accuracy: 0.4151 - val_loss: 2.3740 - val_accuracy: 0.2336\n",
            "Epoch 87/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3953 - accuracy: 0.4356 - val_loss: 1.9780 - val_accuracy: 0.2568\n",
            "Epoch 88/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3572 - accuracy: 0.4225 - val_loss: 2.0127 - val_accuracy: 0.2799\n",
            "Epoch 89/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3543 - accuracy: 0.4680 - val_loss: 2.1659 - val_accuracy: 0.2490\n",
            "Epoch 90/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3676 - accuracy: 0.4389 - val_loss: 2.1492 - val_accuracy: 0.2819\n",
            "Epoch 91/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3505 - accuracy: 0.4510 - val_loss: 2.3390 - val_accuracy: 0.2896\n",
            "Epoch 92/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3286 - accuracy: 0.4559 - val_loss: 1.9820 - val_accuracy: 0.2876\n",
            "Epoch 93/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3242 - accuracy: 0.4607 - val_loss: 1.7609 - val_accuracy: 0.2664\n",
            "Epoch 94/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3641 - accuracy: 0.4163 - val_loss: 1.8397 - val_accuracy: 0.3089\n",
            "Epoch 95/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3623 - accuracy: 0.4517 - val_loss: 1.8585 - val_accuracy: 0.2220\n",
            "Epoch 96/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3627 - accuracy: 0.4347 - val_loss: 2.1181 - val_accuracy: 0.2645\n",
            "Epoch 97/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3630 - accuracy: 0.4662 - val_loss: 2.0028 - val_accuracy: 0.2432\n",
            "Epoch 98/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3041 - accuracy: 0.4552 - val_loss: 2.6358 - val_accuracy: 0.2587\n",
            "Epoch 99/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3544 - accuracy: 0.4539 - val_loss: 1.9848 - val_accuracy: 0.2915\n",
            "Epoch 100/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3468 - accuracy: 0.4510 - val_loss: 1.9465 - val_accuracy: 0.2761\n",
            "processing fold # 2\n",
            "Epoch 1/100\n",
            "260/260 [==============================] - 2s 5ms/step - loss: 2.1534 - accuracy: 0.2218 - val_loss: 2.2050 - val_accuracy: 0.2278\n",
            "Epoch 2/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.6493 - accuracy: 0.2605 - val_loss: 1.5681 - val_accuracy: 0.2838\n",
            "Epoch 3/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.6296 - accuracy: 0.2578 - val_loss: 1.6401 - val_accuracy: 0.2278\n",
            "Epoch 4/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.6014 - accuracy: 0.2840 - val_loss: 1.5577 - val_accuracy: 0.2761\n",
            "Epoch 5/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5904 - accuracy: 0.2698 - val_loss: 1.5706 - val_accuracy: 0.2548\n",
            "Epoch 6/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5943 - accuracy: 0.2915 - val_loss: 1.5553 - val_accuracy: 0.2780\n",
            "Epoch 7/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5582 - accuracy: 0.2918 - val_loss: 1.5432 - val_accuracy: 0.2896\n",
            "Epoch 8/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5737 - accuracy: 0.2797 - val_loss: 2.1945 - val_accuracy: 0.2278\n",
            "Epoch 9/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5702 - accuracy: 0.3007 - val_loss: 2.0377 - val_accuracy: 0.2046\n",
            "Epoch 10/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5823 - accuracy: 0.2827 - val_loss: 1.5779 - val_accuracy: 0.2934\n",
            "Epoch 11/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5634 - accuracy: 0.2908 - val_loss: 1.5919 - val_accuracy: 0.2529\n",
            "Epoch 12/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5379 - accuracy: 0.3238 - val_loss: 1.5416 - val_accuracy: 0.2915\n",
            "Epoch 13/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5675 - accuracy: 0.2933 - val_loss: 1.5489 - val_accuracy: 0.3127\n",
            "Epoch 14/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5210 - accuracy: 0.3295 - val_loss: 1.5557 - val_accuracy: 0.2915\n",
            "Epoch 15/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5394 - accuracy: 0.3077 - val_loss: 1.7559 - val_accuracy: 0.2394\n",
            "Epoch 16/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5315 - accuracy: 0.3088 - val_loss: 1.5484 - val_accuracy: 0.3108\n",
            "Epoch 17/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5274 - accuracy: 0.3327 - val_loss: 1.5635 - val_accuracy: 0.2915\n",
            "Epoch 18/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5174 - accuracy: 0.3248 - val_loss: 1.5464 - val_accuracy: 0.2992\n",
            "Epoch 19/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5279 - accuracy: 0.3417 - val_loss: 1.5528 - val_accuracy: 0.3069\n",
            "Epoch 20/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4762 - accuracy: 0.3587 - val_loss: 1.6522 - val_accuracy: 0.2510\n",
            "Epoch 21/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5112 - accuracy: 0.3396 - val_loss: 1.5432 - val_accuracy: 0.2992\n",
            "Epoch 22/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4848 - accuracy: 0.3634 - val_loss: 1.5767 - val_accuracy: 0.2510\n",
            "Epoch 23/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4851 - accuracy: 0.3525 - val_loss: 1.5672 - val_accuracy: 0.2722\n",
            "Epoch 24/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4767 - accuracy: 0.3612 - val_loss: 1.5718 - val_accuracy: 0.3166\n",
            "Epoch 25/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4705 - accuracy: 0.3772 - val_loss: 1.5617 - val_accuracy: 0.2587\n",
            "Epoch 26/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4875 - accuracy: 0.3496 - val_loss: 1.5677 - val_accuracy: 0.2896\n",
            "Epoch 27/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4835 - accuracy: 0.3701 - val_loss: 1.5842 - val_accuracy: 0.2606\n",
            "Epoch 28/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4690 - accuracy: 0.3621 - val_loss: 1.6957 - val_accuracy: 0.2568\n",
            "Epoch 29/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5010 - accuracy: 0.3442 - val_loss: 1.5772 - val_accuracy: 0.2857\n",
            "Epoch 30/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4679 - accuracy: 0.3509 - val_loss: 1.6030 - val_accuracy: 0.2973\n",
            "Epoch 31/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4794 - accuracy: 0.3653 - val_loss: 1.7073 - val_accuracy: 0.2510\n",
            "Epoch 32/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4676 - accuracy: 0.3714 - val_loss: 1.6043 - val_accuracy: 0.3050\n",
            "Epoch 33/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4787 - accuracy: 0.3767 - val_loss: 1.8459 - val_accuracy: 0.2587\n",
            "Epoch 34/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4763 - accuracy: 0.3713 - val_loss: 1.5895 - val_accuracy: 0.2780\n",
            "Epoch 35/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4666 - accuracy: 0.3704 - val_loss: 1.8685 - val_accuracy: 0.2413\n",
            "Epoch 36/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4715 - accuracy: 0.3802 - val_loss: 1.5861 - val_accuracy: 0.2934\n",
            "Epoch 37/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4520 - accuracy: 0.3922 - val_loss: 1.7738 - val_accuracy: 0.2722\n",
            "Epoch 38/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4530 - accuracy: 0.3958 - val_loss: 1.9573 - val_accuracy: 0.3031\n",
            "Epoch 39/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4706 - accuracy: 0.3726 - val_loss: 1.8209 - val_accuracy: 0.3089\n",
            "Epoch 40/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4646 - accuracy: 0.4166 - val_loss: 1.7261 - val_accuracy: 0.3050\n",
            "Epoch 41/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4357 - accuracy: 0.4055 - val_loss: 1.9806 - val_accuracy: 0.2625\n",
            "Epoch 42/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4421 - accuracy: 0.3869 - val_loss: 2.0763 - val_accuracy: 0.2683\n",
            "Epoch 43/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4570 - accuracy: 0.3945 - val_loss: 1.6272 - val_accuracy: 0.2992\n",
            "Epoch 44/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4417 - accuracy: 0.4049 - val_loss: 2.1312 - val_accuracy: 0.2568\n",
            "Epoch 45/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4433 - accuracy: 0.4148 - val_loss: 1.6268 - val_accuracy: 0.3320\n",
            "Epoch 46/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4298 - accuracy: 0.4169 - val_loss: 1.6506 - val_accuracy: 0.3147\n",
            "Epoch 47/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4310 - accuracy: 0.4186 - val_loss: 1.6572 - val_accuracy: 0.3108\n",
            "Epoch 48/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4400 - accuracy: 0.4051 - val_loss: 1.9196 - val_accuracy: 0.2625\n",
            "Epoch 49/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4298 - accuracy: 0.4157 - val_loss: 1.6757 - val_accuracy: 0.2606\n",
            "Epoch 50/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4454 - accuracy: 0.4037 - val_loss: 1.6465 - val_accuracy: 0.2915\n",
            "Epoch 51/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4261 - accuracy: 0.4287 - val_loss: 1.6642 - val_accuracy: 0.3166\n",
            "Epoch 52/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4153 - accuracy: 0.4169 - val_loss: 1.7861 - val_accuracy: 0.2683\n",
            "Epoch 53/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4107 - accuracy: 0.4383 - val_loss: 1.7540 - val_accuracy: 0.3031\n",
            "Epoch 54/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4373 - accuracy: 0.4189 - val_loss: 1.7284 - val_accuracy: 0.2857\n",
            "Epoch 55/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4045 - accuracy: 0.4235 - val_loss: 2.0187 - val_accuracy: 0.2722\n",
            "Epoch 56/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4385 - accuracy: 0.4219 - val_loss: 1.6755 - val_accuracy: 0.2973\n",
            "Epoch 57/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3748 - accuracy: 0.4525 - val_loss: 1.9792 - val_accuracy: 0.2568\n",
            "Epoch 58/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4486 - accuracy: 0.4045 - val_loss: 1.7505 - val_accuracy: 0.3108\n",
            "Epoch 59/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4083 - accuracy: 0.4212 - val_loss: 1.6759 - val_accuracy: 0.2741\n",
            "Epoch 60/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4001 - accuracy: 0.4382 - val_loss: 1.7204 - val_accuracy: 0.2722\n",
            "Epoch 61/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4051 - accuracy: 0.4415 - val_loss: 1.8154 - val_accuracy: 0.2876\n",
            "Epoch 62/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3923 - accuracy: 0.4306 - val_loss: 1.6844 - val_accuracy: 0.2510\n",
            "Epoch 63/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3868 - accuracy: 0.4431 - val_loss: 2.9293 - val_accuracy: 0.2259\n",
            "Epoch 64/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4569 - accuracy: 0.4231 - val_loss: 1.9936 - val_accuracy: 0.3031\n",
            "Epoch 65/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3798 - accuracy: 0.4390 - val_loss: 1.6792 - val_accuracy: 0.3147\n",
            "Epoch 66/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3849 - accuracy: 0.4567 - val_loss: 1.9542 - val_accuracy: 0.2934\n",
            "Epoch 67/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4051 - accuracy: 0.4384 - val_loss: 1.7128 - val_accuracy: 0.2799\n",
            "Epoch 68/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3878 - accuracy: 0.4554 - val_loss: 1.7603 - val_accuracy: 0.3127\n",
            "Epoch 69/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3937 - accuracy: 0.4412 - val_loss: 2.1857 - val_accuracy: 0.2529\n",
            "Epoch 70/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3842 - accuracy: 0.4633 - val_loss: 1.7607 - val_accuracy: 0.3127\n",
            "Epoch 71/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3864 - accuracy: 0.4455 - val_loss: 1.7286 - val_accuracy: 0.2934\n",
            "Epoch 72/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3605 - accuracy: 0.4752 - val_loss: 1.7139 - val_accuracy: 0.3127\n",
            "Epoch 73/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3773 - accuracy: 0.4572 - val_loss: 1.9457 - val_accuracy: 0.3243\n",
            "Epoch 74/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3929 - accuracy: 0.4668 - val_loss: 2.6799 - val_accuracy: 0.2934\n",
            "Epoch 75/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3780 - accuracy: 0.4631 - val_loss: 1.7206 - val_accuracy: 0.2683\n",
            "Epoch 76/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4102 - accuracy: 0.4339 - val_loss: 1.7806 - val_accuracy: 0.2838\n",
            "Epoch 77/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3992 - accuracy: 0.4484 - val_loss: 1.7416 - val_accuracy: 0.2529\n",
            "Epoch 78/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3634 - accuracy: 0.4786 - val_loss: 1.7764 - val_accuracy: 0.2973\n",
            "Epoch 79/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3585 - accuracy: 0.4770 - val_loss: 1.8369 - val_accuracy: 0.2606\n",
            "Epoch 80/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3664 - accuracy: 0.4925 - val_loss: 1.7421 - val_accuracy: 0.2548\n",
            "Epoch 81/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3827 - accuracy: 0.4700 - val_loss: 1.9196 - val_accuracy: 0.3069\n",
            "Epoch 82/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3933 - accuracy: 0.4611 - val_loss: 2.7575 - val_accuracy: 0.2355\n",
            "Epoch 83/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3936 - accuracy: 0.4707 - val_loss: 1.8950 - val_accuracy: 0.2683\n",
            "Epoch 84/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3795 - accuracy: 0.4557 - val_loss: 1.7877 - val_accuracy: 0.3127\n",
            "Epoch 85/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3885 - accuracy: 0.4434 - val_loss: 1.7841 - val_accuracy: 0.3031\n",
            "Epoch 86/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4135 - accuracy: 0.4719 - val_loss: 1.8123 - val_accuracy: 0.3069\n",
            "Epoch 87/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3700 - accuracy: 0.4691 - val_loss: 1.8571 - val_accuracy: 0.2915\n",
            "Epoch 88/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3653 - accuracy: 0.4782 - val_loss: 1.9182 - val_accuracy: 0.2722\n",
            "Epoch 89/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3563 - accuracy: 0.4681 - val_loss: 1.8658 - val_accuracy: 0.3263\n",
            "Epoch 90/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3452 - accuracy: 0.4892 - val_loss: 1.9246 - val_accuracy: 0.2819\n",
            "Epoch 91/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3679 - accuracy: 0.4764 - val_loss: 2.2142 - val_accuracy: 0.2683\n",
            "Epoch 92/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3089 - accuracy: 0.5034 - val_loss: 1.8406 - val_accuracy: 0.2761\n",
            "Epoch 93/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3481 - accuracy: 0.4738 - val_loss: 1.7739 - val_accuracy: 0.2896\n",
            "Epoch 94/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3769 - accuracy: 0.4739 - val_loss: 1.8923 - val_accuracy: 0.3185\n",
            "Epoch 95/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4011 - accuracy: 0.4685 - val_loss: 1.8151 - val_accuracy: 0.2355\n",
            "Epoch 96/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3291 - accuracy: 0.4907 - val_loss: 2.0816 - val_accuracy: 0.2625\n",
            "Epoch 97/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3662 - accuracy: 0.4456 - val_loss: 1.7840 - val_accuracy: 0.2896\n",
            "Epoch 98/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3858 - accuracy: 0.4758 - val_loss: 2.6256 - val_accuracy: 0.2664\n",
            "Epoch 99/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3587 - accuracy: 0.4925 - val_loss: 1.7911 - val_accuracy: 0.2838\n",
            "Epoch 100/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4006 - accuracy: 0.4694 - val_loss: 1.7740 - val_accuracy: 0.2780\n",
            "processing fold # 3\n",
            "Epoch 1/100\n",
            "260/260 [==============================] - 2s 6ms/step - loss: 1.9866 - accuracy: 0.2456 - val_loss: 1.6123 - val_accuracy: 0.2027\n",
            "Epoch 2/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.6450 - accuracy: 0.2764 - val_loss: 1.6558 - val_accuracy: 0.2239\n",
            "Epoch 3/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.6083 - accuracy: 0.2471 - val_loss: 1.5529 - val_accuracy: 0.3301\n",
            "Epoch 4/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.6095 - accuracy: 0.2843 - val_loss: 1.9276 - val_accuracy: 0.2201\n",
            "Epoch 5/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.6086 - accuracy: 0.2791 - val_loss: 2.1220 - val_accuracy: 0.1795\n",
            "Epoch 6/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5949 - accuracy: 0.2900 - val_loss: 1.6523 - val_accuracy: 0.2220\n",
            "Epoch 7/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5616 - accuracy: 0.2763 - val_loss: 1.5738 - val_accuracy: 0.3050\n",
            "Epoch 8/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5407 - accuracy: 0.3223 - val_loss: 1.6345 - val_accuracy: 0.2394\n",
            "Epoch 9/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5438 - accuracy: 0.3153 - val_loss: 1.7129 - val_accuracy: 0.1853\n",
            "Epoch 10/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5123 - accuracy: 0.3129 - val_loss: 1.6961 - val_accuracy: 0.2587\n",
            "Epoch 11/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5417 - accuracy: 0.2929 - val_loss: 1.7336 - val_accuracy: 0.2645\n",
            "Epoch 12/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5548 - accuracy: 0.3055 - val_loss: 1.6155 - val_accuracy: 0.2819\n",
            "Epoch 13/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5418 - accuracy: 0.3067 - val_loss: 1.5799 - val_accuracy: 0.3069\n",
            "Epoch 14/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4892 - accuracy: 0.3398 - val_loss: 1.6615 - val_accuracy: 0.2220\n",
            "Epoch 15/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4839 - accuracy: 0.3460 - val_loss: 1.5938 - val_accuracy: 0.3147\n",
            "Epoch 16/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5005 - accuracy: 0.3404 - val_loss: 1.6065 - val_accuracy: 0.2490\n",
            "Epoch 17/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4784 - accuracy: 0.3496 - val_loss: 1.6345 - val_accuracy: 0.2876\n",
            "Epoch 18/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4833 - accuracy: 0.3435 - val_loss: 1.6732 - val_accuracy: 0.2741\n",
            "Epoch 19/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4443 - accuracy: 0.3821 - val_loss: 1.6758 - val_accuracy: 0.2915\n",
            "Epoch 20/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4650 - accuracy: 0.3549 - val_loss: 1.7248 - val_accuracy: 0.2452\n",
            "Epoch 21/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4556 - accuracy: 0.3629 - val_loss: 1.8065 - val_accuracy: 0.2529\n",
            "Epoch 22/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4634 - accuracy: 0.3621 - val_loss: 1.6225 - val_accuracy: 0.2838\n",
            "Epoch 23/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4527 - accuracy: 0.3833 - val_loss: 1.8831 - val_accuracy: 0.2896\n",
            "Epoch 24/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4575 - accuracy: 0.3786 - val_loss: 1.7983 - val_accuracy: 0.2896\n",
            "Epoch 25/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4621 - accuracy: 0.3685 - val_loss: 1.7758 - val_accuracy: 0.2239\n",
            "Epoch 26/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4503 - accuracy: 0.3803 - val_loss: 1.6716 - val_accuracy: 0.3127\n",
            "Epoch 27/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4224 - accuracy: 0.4000 - val_loss: 1.7778 - val_accuracy: 0.2297\n",
            "Epoch 28/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4400 - accuracy: 0.3784 - val_loss: 1.6900 - val_accuracy: 0.3012\n",
            "Epoch 29/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4139 - accuracy: 0.4039 - val_loss: 1.7064 - val_accuracy: 0.3050\n",
            "Epoch 30/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4544 - accuracy: 0.3909 - val_loss: 1.9520 - val_accuracy: 0.2278\n",
            "Epoch 31/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4305 - accuracy: 0.3952 - val_loss: 1.6758 - val_accuracy: 0.2741\n",
            "Epoch 32/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4404 - accuracy: 0.3959 - val_loss: 1.7154 - val_accuracy: 0.2722\n",
            "Epoch 33/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4452 - accuracy: 0.3839 - val_loss: 1.9263 - val_accuracy: 0.2413\n",
            "Epoch 34/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4088 - accuracy: 0.4166 - val_loss: 2.0036 - val_accuracy: 0.2432\n",
            "Epoch 35/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4050 - accuracy: 0.3937 - val_loss: 1.7760 - val_accuracy: 0.2973\n",
            "Epoch 36/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4208 - accuracy: 0.4101 - val_loss: 1.6288 - val_accuracy: 0.2954\n",
            "Epoch 37/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4154 - accuracy: 0.3933 - val_loss: 2.0991 - val_accuracy: 0.2085\n",
            "Epoch 38/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.4093 - accuracy: 0.4078 - val_loss: 1.7494 - val_accuracy: 0.2819\n",
            "Epoch 39/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4132 - accuracy: 0.4206 - val_loss: 1.7011 - val_accuracy: 0.2992\n",
            "Epoch 40/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3928 - accuracy: 0.4114 - val_loss: 1.8610 - val_accuracy: 0.2915\n",
            "Epoch 41/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3915 - accuracy: 0.4332 - val_loss: 2.0567 - val_accuracy: 0.2297\n",
            "Epoch 42/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3853 - accuracy: 0.4401 - val_loss: 1.6737 - val_accuracy: 0.2896\n",
            "Epoch 43/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3977 - accuracy: 0.4243 - val_loss: 1.7206 - val_accuracy: 0.2722\n",
            "Epoch 44/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3813 - accuracy: 0.4356 - val_loss: 1.7061 - val_accuracy: 0.2934\n",
            "Epoch 45/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3831 - accuracy: 0.4173 - val_loss: 1.8718 - val_accuracy: 0.2413\n",
            "Epoch 46/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3829 - accuracy: 0.4484 - val_loss: 1.9194 - val_accuracy: 0.2876\n",
            "Epoch 47/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3935 - accuracy: 0.4012 - val_loss: 1.9766 - val_accuracy: 0.2645\n",
            "Epoch 48/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3564 - accuracy: 0.4554 - val_loss: 1.7208 - val_accuracy: 0.2683\n",
            "Epoch 49/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3577 - accuracy: 0.4544 - val_loss: 1.7418 - val_accuracy: 0.2915\n",
            "Epoch 50/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3620 - accuracy: 0.4356 - val_loss: 4.7776 - val_accuracy: 0.1718\n",
            "Epoch 51/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5396 - accuracy: 0.4204 - val_loss: 1.7905 - val_accuracy: 0.3089\n",
            "Epoch 52/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3568 - accuracy: 0.4570 - val_loss: 1.8993 - val_accuracy: 0.2510\n",
            "Epoch 53/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3610 - accuracy: 0.4573 - val_loss: 1.7839 - val_accuracy: 0.2896\n",
            "Epoch 54/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3525 - accuracy: 0.4565 - val_loss: 2.0027 - val_accuracy: 0.2510\n",
            "Epoch 55/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3592 - accuracy: 0.4780 - val_loss: 1.9007 - val_accuracy: 0.2761\n",
            "Epoch 56/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3501 - accuracy: 0.4523 - val_loss: 1.8203 - val_accuracy: 0.2973\n",
            "Epoch 57/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3374 - accuracy: 0.4617 - val_loss: 2.6590 - val_accuracy: 0.2027\n",
            "Epoch 58/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3658 - accuracy: 0.4596 - val_loss: 2.0750 - val_accuracy: 0.2355\n",
            "Epoch 59/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3812 - accuracy: 0.4334 - val_loss: 1.7239 - val_accuracy: 0.2857\n",
            "Epoch 60/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3489 - accuracy: 0.4554 - val_loss: 2.1191 - val_accuracy: 0.2201\n",
            "Epoch 61/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3496 - accuracy: 0.4577 - val_loss: 2.0670 - val_accuracy: 0.2625\n",
            "Epoch 62/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3716 - accuracy: 0.4293 - val_loss: 1.7861 - val_accuracy: 0.2954\n",
            "Epoch 63/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3259 - accuracy: 0.4636 - val_loss: 2.0265 - val_accuracy: 0.2838\n",
            "Epoch 64/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3359 - accuracy: 0.4634 - val_loss: 2.3353 - val_accuracy: 0.2220\n",
            "Epoch 65/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3581 - accuracy: 0.4576 - val_loss: 1.9120 - val_accuracy: 0.2780\n",
            "Epoch 66/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3355 - accuracy: 0.4549 - val_loss: 2.7141 - val_accuracy: 0.2375\n",
            "Epoch 67/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3560 - accuracy: 0.4727 - val_loss: 1.9361 - val_accuracy: 0.2625\n",
            "Epoch 68/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3135 - accuracy: 0.4625 - val_loss: 1.8254 - val_accuracy: 0.2664\n",
            "Epoch 69/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3500 - accuracy: 0.4938 - val_loss: 2.2702 - val_accuracy: 0.2587\n",
            "Epoch 70/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3405 - accuracy: 0.4753 - val_loss: 2.2464 - val_accuracy: 0.2548\n",
            "Epoch 71/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3392 - accuracy: 0.4738 - val_loss: 2.0066 - val_accuracy: 0.2838\n",
            "Epoch 72/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3289 - accuracy: 0.4628 - val_loss: 1.9968 - val_accuracy: 0.2625\n",
            "Epoch 73/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3327 - accuracy: 0.4834 - val_loss: 2.0881 - val_accuracy: 0.2336\n",
            "Epoch 74/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3537 - accuracy: 0.4598 - val_loss: 1.9730 - val_accuracy: 0.3069\n",
            "Epoch 75/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3199 - accuracy: 0.4847 - val_loss: 1.7329 - val_accuracy: 0.2915\n",
            "Epoch 76/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3348 - accuracy: 0.4569 - val_loss: 1.8208 - val_accuracy: 0.2896\n",
            "Epoch 77/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3102 - accuracy: 0.4955 - val_loss: 1.7608 - val_accuracy: 0.2973\n",
            "Epoch 78/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3154 - accuracy: 0.4811 - val_loss: 2.3791 - val_accuracy: 0.2432\n",
            "Epoch 79/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3034 - accuracy: 0.4817 - val_loss: 1.9882 - val_accuracy: 0.2857\n",
            "Epoch 80/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3479 - accuracy: 0.4723 - val_loss: 2.0354 - val_accuracy: 0.2529\n",
            "Epoch 81/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3115 - accuracy: 0.4841 - val_loss: 2.5068 - val_accuracy: 0.2548\n",
            "Epoch 82/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3628 - accuracy: 0.4823 - val_loss: 2.1853 - val_accuracy: 0.2606\n",
            "Epoch 83/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3489 - accuracy: 0.4827 - val_loss: 1.9199 - val_accuracy: 0.2992\n",
            "Epoch 84/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.2869 - accuracy: 0.4982 - val_loss: 2.2210 - val_accuracy: 0.2413\n",
            "Epoch 85/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3662 - accuracy: 0.4868 - val_loss: 2.5776 - val_accuracy: 0.2181\n",
            "Epoch 86/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3608 - accuracy: 0.4801 - val_loss: 1.9674 - val_accuracy: 0.2896\n",
            "Epoch 87/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.2888 - accuracy: 0.5096 - val_loss: 3.0635 - val_accuracy: 0.2355\n",
            "Epoch 88/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3140 - accuracy: 0.4937 - val_loss: 1.9843 - val_accuracy: 0.2741\n",
            "Epoch 89/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.2962 - accuracy: 0.4903 - val_loss: 1.8180 - val_accuracy: 0.2761\n",
            "Epoch 90/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.2553 - accuracy: 0.5205 - val_loss: 2.6455 - val_accuracy: 0.2085\n",
            "Epoch 91/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3322 - accuracy: 0.5002 - val_loss: 2.4314 - val_accuracy: 0.2876\n",
            "Epoch 92/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3369 - accuracy: 0.4807 - val_loss: 2.4654 - val_accuracy: 0.2297\n",
            "Epoch 93/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3105 - accuracy: 0.4935 - val_loss: 2.6065 - val_accuracy: 0.2413\n",
            "Epoch 94/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.2971 - accuracy: 0.4951 - val_loss: 2.4494 - val_accuracy: 0.2510\n",
            "Epoch 95/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.3110 - accuracy: 0.5062 - val_loss: 2.0688 - val_accuracy: 0.2375\n",
            "Epoch 96/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3431 - accuracy: 0.4703 - val_loss: 2.5769 - val_accuracy: 0.2394\n",
            "Epoch 97/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3200 - accuracy: 0.4996 - val_loss: 1.7842 - val_accuracy: 0.2703\n",
            "Epoch 98/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.2924 - accuracy: 0.5057 - val_loss: 2.5791 - val_accuracy: 0.2375\n",
            "Epoch 99/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3043 - accuracy: 0.5194 - val_loss: 2.0646 - val_accuracy: 0.2664\n",
            "Epoch 100/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.2699 - accuracy: 0.5041 - val_loss: 1.9953 - val_accuracy: 0.2780\n",
            "processing fold # 4\n",
            "Epoch 1/100\n",
            "260/260 [==============================] - 2s 6ms/step - loss: 1.8590 - accuracy: 0.2502 - val_loss: 1.5691 - val_accuracy: 0.3069\n",
            "Epoch 2/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.6329 - accuracy: 0.2540 - val_loss: 1.5652 - val_accuracy: 0.2471\n",
            "Epoch 3/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.6180 - accuracy: 0.2627 - val_loss: 1.5891 - val_accuracy: 0.2587\n",
            "Epoch 4/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.6504 - accuracy: 0.2585 - val_loss: 1.5327 - val_accuracy: 0.2799\n",
            "Epoch 5/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5910 - accuracy: 0.2958 - val_loss: 1.5338 - val_accuracy: 0.2876\n",
            "Epoch 6/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5975 - accuracy: 0.3056 - val_loss: 1.5558 - val_accuracy: 0.2587\n",
            "Epoch 7/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.6060 - accuracy: 0.2937 - val_loss: 1.7004 - val_accuracy: 0.2104\n",
            "Epoch 8/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5479 - accuracy: 0.3054 - val_loss: 1.5303 - val_accuracy: 0.2915\n",
            "Epoch 9/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5705 - accuracy: 0.3067 - val_loss: 1.5732 - val_accuracy: 0.2625\n",
            "Epoch 10/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5672 - accuracy: 0.3002 - val_loss: 1.6793 - val_accuracy: 0.2799\n",
            "Epoch 11/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5463 - accuracy: 0.3053 - val_loss: 1.6129 - val_accuracy: 0.2645\n",
            "Epoch 12/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5580 - accuracy: 0.3021 - val_loss: 1.5532 - val_accuracy: 0.2703\n",
            "Epoch 13/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5323 - accuracy: 0.3215 - val_loss: 1.5810 - val_accuracy: 0.3031\n",
            "Epoch 14/100\n",
            "260/260 [==============================] - 1s 4ms/step - loss: 1.5437 - accuracy: 0.2958 - val_loss: 1.5679 - val_accuracy: 0.2587\n",
            "Epoch 15/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5562 - accuracy: 0.2979 - val_loss: 1.5255 - val_accuracy: 0.2973\n",
            "Epoch 16/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5446 - accuracy: 0.3166 - val_loss: 1.5647 - val_accuracy: 0.2799\n",
            "Epoch 17/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5379 - accuracy: 0.3297 - val_loss: 1.5423 - val_accuracy: 0.2722\n",
            "Epoch 18/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5141 - accuracy: 0.3204 - val_loss: 1.7517 - val_accuracy: 0.2645\n",
            "Epoch 19/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5068 - accuracy: 0.3259 - val_loss: 1.5287 - val_accuracy: 0.3089\n",
            "Epoch 20/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5084 - accuracy: 0.3442 - val_loss: 1.5358 - val_accuracy: 0.3224\n",
            "Epoch 21/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5213 - accuracy: 0.3459 - val_loss: 1.5488 - val_accuracy: 0.2838\n",
            "Epoch 22/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4828 - accuracy: 0.3382 - val_loss: 1.5310 - val_accuracy: 0.3147\n",
            "Epoch 23/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.5012 - accuracy: 0.3264 - val_loss: 1.5377 - val_accuracy: 0.3610\n",
            "Epoch 24/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4887 - accuracy: 0.3363 - val_loss: 1.5646 - val_accuracy: 0.3359\n",
            "Epoch 25/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4583 - accuracy: 0.3577 - val_loss: 1.5512 - val_accuracy: 0.2954\n",
            "Epoch 26/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4678 - accuracy: 0.3496 - val_loss: 1.6061 - val_accuracy: 0.3050\n",
            "Epoch 27/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4766 - accuracy: 0.3417 - val_loss: 1.6361 - val_accuracy: 0.3205\n",
            "Epoch 28/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4616 - accuracy: 0.3516 - val_loss: 1.5629 - val_accuracy: 0.2780\n",
            "Epoch 29/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4674 - accuracy: 0.3582 - val_loss: 1.5653 - val_accuracy: 0.2973\n",
            "Epoch 30/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4470 - accuracy: 0.3684 - val_loss: 1.6918 - val_accuracy: 0.2896\n",
            "Epoch 31/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4728 - accuracy: 0.3585 - val_loss: 1.7854 - val_accuracy: 0.3050\n",
            "Epoch 32/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4701 - accuracy: 0.3637 - val_loss: 1.6047 - val_accuracy: 0.2992\n",
            "Epoch 33/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4456 - accuracy: 0.3651 - val_loss: 1.5604 - val_accuracy: 0.3050\n",
            "Epoch 34/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4765 - accuracy: 0.3555 - val_loss: 1.5930 - val_accuracy: 0.2973\n",
            "Epoch 35/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4601 - accuracy: 0.3694 - val_loss: 1.7060 - val_accuracy: 0.2876\n",
            "Epoch 36/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4613 - accuracy: 0.3821 - val_loss: 1.5756 - val_accuracy: 0.2838\n",
            "Epoch 37/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4522 - accuracy: 0.3767 - val_loss: 1.6782 - val_accuracy: 0.2838\n",
            "Epoch 38/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4035 - accuracy: 0.4005 - val_loss: 1.6458 - val_accuracy: 0.2934\n",
            "Epoch 39/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4542 - accuracy: 0.3867 - val_loss: 1.9714 - val_accuracy: 0.2394\n",
            "Epoch 40/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4392 - accuracy: 0.3811 - val_loss: 1.6483 - val_accuracy: 0.2838\n",
            "Epoch 41/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4455 - accuracy: 0.3673 - val_loss: 1.6046 - val_accuracy: 0.2857\n",
            "Epoch 42/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4388 - accuracy: 0.3785 - val_loss: 1.5736 - val_accuracy: 0.2819\n",
            "Epoch 43/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4467 - accuracy: 0.3827 - val_loss: 1.6022 - val_accuracy: 0.2819\n",
            "Epoch 44/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4078 - accuracy: 0.4056 - val_loss: 1.8919 - val_accuracy: 0.2625\n",
            "Epoch 45/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4698 - accuracy: 0.3655 - val_loss: 1.6000 - val_accuracy: 0.2819\n",
            "Epoch 46/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4343 - accuracy: 0.3951 - val_loss: 1.6089 - val_accuracy: 0.2992\n",
            "Epoch 47/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3942 - accuracy: 0.4054 - val_loss: 1.8056 - val_accuracy: 0.2838\n",
            "Epoch 48/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4081 - accuracy: 0.4397 - val_loss: 1.7372 - val_accuracy: 0.2934\n",
            "Epoch 49/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4338 - accuracy: 0.3912 - val_loss: 1.8183 - val_accuracy: 0.2741\n",
            "Epoch 50/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4277 - accuracy: 0.4024 - val_loss: 1.7816 - val_accuracy: 0.2992\n",
            "Epoch 51/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4451 - accuracy: 0.3891 - val_loss: 1.7529 - val_accuracy: 0.2722\n",
            "Epoch 52/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4200 - accuracy: 0.4034 - val_loss: 1.7966 - val_accuracy: 0.2625\n",
            "Epoch 53/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3942 - accuracy: 0.4182 - val_loss: 1.6566 - val_accuracy: 0.2915\n",
            "Epoch 54/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4031 - accuracy: 0.4071 - val_loss: 1.6210 - val_accuracy: 0.2857\n",
            "Epoch 55/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3952 - accuracy: 0.4075 - val_loss: 1.6791 - val_accuracy: 0.3147\n",
            "Epoch 56/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3786 - accuracy: 0.4191 - val_loss: 1.7854 - val_accuracy: 0.3282\n",
            "Epoch 57/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4185 - accuracy: 0.4168 - val_loss: 1.6665 - val_accuracy: 0.2896\n",
            "Epoch 58/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3874 - accuracy: 0.4221 - val_loss: 1.6275 - val_accuracy: 0.2799\n",
            "Epoch 59/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4026 - accuracy: 0.4146 - val_loss: 1.6347 - val_accuracy: 0.2915\n",
            "Epoch 60/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3882 - accuracy: 0.4090 - val_loss: 1.9221 - val_accuracy: 0.2896\n",
            "Epoch 61/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3903 - accuracy: 0.4391 - val_loss: 1.6400 - val_accuracy: 0.2683\n",
            "Epoch 62/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3403 - accuracy: 0.4325 - val_loss: 1.9060 - val_accuracy: 0.2799\n",
            "Epoch 63/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3412 - accuracy: 0.4211 - val_loss: 1.8192 - val_accuracy: 0.2741\n",
            "Epoch 64/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4225 - accuracy: 0.3833 - val_loss: 1.6680 - val_accuracy: 0.2722\n",
            "Epoch 65/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3636 - accuracy: 0.4123 - val_loss: 1.7688 - val_accuracy: 0.3050\n",
            "Epoch 66/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3745 - accuracy: 0.4018 - val_loss: 1.7280 - val_accuracy: 0.2876\n",
            "Epoch 67/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3821 - accuracy: 0.4247 - val_loss: 2.0360 - val_accuracy: 0.3108\n",
            "Epoch 68/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3878 - accuracy: 0.4177 - val_loss: 2.2692 - val_accuracy: 0.2645\n",
            "Epoch 69/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4076 - accuracy: 0.4494 - val_loss: 3.1348 - val_accuracy: 0.2355\n",
            "Epoch 70/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4443 - accuracy: 0.4099 - val_loss: 1.8621 - val_accuracy: 0.2973\n",
            "Epoch 71/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3869 - accuracy: 0.4303 - val_loss: 1.9273 - val_accuracy: 0.2703\n",
            "Epoch 72/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3736 - accuracy: 0.4114 - val_loss: 2.0621 - val_accuracy: 0.2819\n",
            "Epoch 73/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3192 - accuracy: 0.4440 - val_loss: 1.6986 - val_accuracy: 0.2915\n",
            "Epoch 74/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4144 - accuracy: 0.4195 - val_loss: 3.0217 - val_accuracy: 0.2741\n",
            "Epoch 75/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4046 - accuracy: 0.4217 - val_loss: 1.9296 - val_accuracy: 0.2587\n",
            "Epoch 76/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3644 - accuracy: 0.4192 - val_loss: 2.4711 - val_accuracy: 0.2375\n",
            "Epoch 77/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3499 - accuracy: 0.4125 - val_loss: 1.8141 - val_accuracy: 0.2857\n",
            "Epoch 78/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3592 - accuracy: 0.4319 - val_loss: 1.7701 - val_accuracy: 0.2490\n",
            "Epoch 79/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3351 - accuracy: 0.4465 - val_loss: 1.9258 - val_accuracy: 0.2780\n",
            "Epoch 80/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3525 - accuracy: 0.4383 - val_loss: 1.6979 - val_accuracy: 0.2761\n",
            "Epoch 81/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3869 - accuracy: 0.4296 - val_loss: 1.7837 - val_accuracy: 0.2548\n",
            "Epoch 82/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3902 - accuracy: 0.4147 - val_loss: 1.7117 - val_accuracy: 0.2934\n",
            "Epoch 83/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3273 - accuracy: 0.4600 - val_loss: 1.7266 - val_accuracy: 0.2568\n",
            "Epoch 84/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.2940 - accuracy: 0.4591 - val_loss: 1.9456 - val_accuracy: 0.3012\n",
            "Epoch 85/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3617 - accuracy: 0.4200 - val_loss: 2.0221 - val_accuracy: 0.2992\n",
            "Epoch 86/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4124 - accuracy: 0.4009 - val_loss: 2.0271 - val_accuracy: 0.2857\n",
            "Epoch 87/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3423 - accuracy: 0.4332 - val_loss: 2.3387 - val_accuracy: 0.2741\n",
            "Epoch 88/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3531 - accuracy: 0.4554 - val_loss: 3.9733 - val_accuracy: 0.2548\n",
            "Epoch 89/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.4436 - accuracy: 0.4158 - val_loss: 1.8330 - val_accuracy: 0.2876\n",
            "Epoch 90/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3145 - accuracy: 0.4442 - val_loss: 2.0997 - val_accuracy: 0.2934\n",
            "Epoch 91/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3546 - accuracy: 0.4548 - val_loss: 2.0716 - val_accuracy: 0.2664\n",
            "Epoch 92/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3007 - accuracy: 0.4797 - val_loss: 2.2849 - val_accuracy: 0.2413\n",
            "Epoch 93/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3676 - accuracy: 0.4318 - val_loss: 1.8760 - val_accuracy: 0.2876\n",
            "Epoch 94/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3659 - accuracy: 0.4379 - val_loss: 1.7718 - val_accuracy: 0.2838\n",
            "Epoch 95/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3881 - accuracy: 0.4251 - val_loss: 1.8077 - val_accuracy: 0.2645\n",
            "Epoch 96/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3455 - accuracy: 0.4522 - val_loss: 2.0841 - val_accuracy: 0.3031\n",
            "Epoch 97/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.2917 - accuracy: 0.4779 - val_loss: 3.1074 - val_accuracy: 0.2741\n",
            "Epoch 98/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3952 - accuracy: 0.4493 - val_loss: 1.7150 - val_accuracy: 0.2220\n",
            "Epoch 99/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3633 - accuracy: 0.4429 - val_loss: 1.7484 - val_accuracy: 0.2297\n",
            "Epoch 100/100\n",
            "260/260 [==============================] - 1s 5ms/step - loss: 1.3346 - accuracy: 0.4490 - val_loss: 1.7681 - val_accuracy: 0.2606\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.3020833432674408,\n",
              " 0.28356480598449707,\n",
              " 0.265625,\n",
              " 0.30960649251937866,\n",
              " 0.28125]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 37
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "UF0KOSbTRbtg",
        "outputId": "93811da5-17a4-4bef-f6a6-5197935ac60d"
      },
      "source": [
        "plt.plot(range(1, len(ave_val_loss_hist)+1)[:], ave_val_loss_hist[:], \"bo\", label=\"Validation Loss\")\n",
        "plt.plot(range(1, len(ave_val_loss_hist)+1)[:], ave_loss_hist[:], \"b\", label=\"Training Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "gboveCttRc_O",
        "outputId": "272ae140-0f53-4345-f527-23581dc8335c"
      },
      "source": [
        "plt.plot(range(1, len(ave_val_acc_hist)+1)[:], ave_val_acc_hist[:], \"bo\", label=\"Validation Accuracy\")\n",
        "plt.plot(range(1, len(ave_val_acc_hist)+1)[:], ave_acc_hist[:], \"b\", label=\"Training Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "godACOVI2oBI"
      },
      "source": [
        "**Best Fit Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P7k-jWYrWln"
      },
      "source": [
        "model=models.Sequential()\n",
        "model.add(layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(l2=.002), input_shape=(pic_size*pic_size,)))\n",
        "model.add(layers.Dropout(.2))\n",
        "model.add(layers.Dense(5, activation=\"softmax\"))\n",
        "model.compile(optimizer=\"rmsprop\", loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXJ_IXyl2BMi"
      },
      "source": [
        "**Training the model with all samples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xl78IydtPae",
        "outputId": "dee5641f-b92f-45e0-cb46-0299dd3dafba"
      },
      "source": [
        "model.fit(train_data,train_labels, epochs=10,verbose=1, batch_size=8)"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "325/325 [==============================] - 2s 4ms/step - loss: 1.8686 - accuracy: 0.2186\n",
            "Epoch 2/10\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 1.6562 - accuracy: 0.2606\n",
            "Epoch 3/10\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 1.6138 - accuracy: 0.2692\n",
            "Epoch 4/10\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 1.6078 - accuracy: 0.2826\n",
            "Epoch 5/10\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 1.5887 - accuracy: 0.2945\n",
            "Epoch 6/10\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 1.5690 - accuracy: 0.2904\n",
            "Epoch 7/10\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 1.6107 - accuracy: 0.2783\n",
            "Epoch 8/10\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 1.5737 - accuracy: 0.2798\n",
            "Epoch 9/10\n",
            "325/325 [==============================] - 1s 4ms/step - loss: 1.5580 - accuracy: 0.2998\n",
            "Epoch 10/10\n",
            "325/325 [==============================] - 1s 3ms/step - loss: 1.5597 - accuracy: 0.2967\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7fb6d8bb99d0>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FtImZvW12r5"
      },
      "source": [
        "**Evaluating the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrzyW7Virabl",
        "outputId": "c9599378-3522-40c0-f39d-547e8e6836c0"
      },
      "source": [
        "test_loss_score, test_acc_score=model.evaluate(test_data, test_labels)"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "54/54 [==============================] - 0s 2ms/step - loss: 1.5408 - accuracy: 0.3241\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKAbqYNB0x83"
      },
      "source": [
        "**Predicting the values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkJiTqCkrzx8"
      },
      "source": [
        "prediction=model.predict(test_data)"
      ],
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frYGw5G9vje0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ddbd2df8-2a7d-495e-8080-2118f110c49c"
      },
      "source": [
        "prediction_result = []\n",
        "for i, v in enumerate(prediction):\n",
        "  prediction_result.append(np.argmax(prediction[i]))\n",
        "print(prediction_result[:35])\n",
        "\n",
        "test_labels_result = []\n",
        "for i, v in enumerate(test_labels):\n",
        "  test_labels_result.append(np.argmax(test_labels[i]))\n",
        "print(test_labels_result[:35])"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 3, 3, 4, 1, 0, 4, 0, 4, 3, 3, 1, 4, 1, 4, 3, 1, 1, 1, 3, 1, 1, 4, 3, 1, 1, 4, 1, 4, 4, 4, 0, 4, 4, 4]\n",
            "[1, 2, 0, 0, 0, 0, 4, 0, 2, 3, 3, 2, 3, 0, 3, 0, 2, 1, 4, 3, 2, 1, 4, 0, 1, 0, 4, 1, 0, 0, 2, 0, 1, 4, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7CFLYgfwydF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b79df9cf-684e-498e-c4af-90620bfe1fc7"
      },
      "source": [
        "matched = []\n",
        "for i, v in enumerate(test_labels_result):\n",
        "  if  prediction_result[i] == test_labels_result[i]:\n",
        "    matched.append(\"Yes\")\n",
        "  else:\n",
        "    matched.append(\"No\")\n",
        "\n",
        "print(\"There are\",matched.count(\"Yes\"), \"matched flowers and there are\", matched.count(\"No\"), \"unmatched flowers \")"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 560 matched flowers and there are 1168 unmatched flowers \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}