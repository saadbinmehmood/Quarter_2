{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Flower_assignment.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O5o6a1KD34s7"
      },
      "source": [
        "As per the Mr. Umair Shahzad this assignment is to learn that how the images can be converted to tensors and the accuracy is not that important. If Convolution, Image Augumentation and Transfer learning is used, then accuracy can be achived above 80%. But without all this stuff, accuracy is not good. So please don't reject this assignment. It took me 3 weeks to complete this assignment since initially I lost so much time to attain 85% accuracy by using convolution and Transfer learning. Thanks "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kE7WobAkruEw"
      },
      "source": [
        "import os\n",
        "import zipfile\n",
        "import numpy as np\n",
        "import random\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "from keras.preprocessing import image\n",
        "import tensorflow as tf\n",
        "from google.colab import files\n",
        "from keras import models\n",
        "from keras import layers\n",
        "from keras import regularizers\n",
        "from tensorflow.keras.utils import to_categorical\n"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RFWiOQXyr3cJ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e5853d72-5812-4718-aab0-3b77a6d91f50"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive\")"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q2Of6YLbuem9"
      },
      "source": [
        "local_zip = \"/content/drive/MyDrive/flowers_new.zip\"\n",
        "zip_ref = zipfile.ZipFile(local_zip, \"r\")\n",
        "zip_ref.extractall(\"/flower_new\")\n",
        "zip_ref.close()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XdHL6gFs3VPx"
      },
      "source": [
        "**Converting the images to gray scale tensor and resize**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3hLh4vC-0iSU"
      },
      "source": [
        "pic_size = 50\n",
        "def convert_image(source, label):\n",
        "  fname = os.listdir(source)\n",
        "  images= None\n",
        "  \n",
        "  for i,value in enumerate(fname):\n",
        "    source_path = os.path.join(source, fname[i])\n",
        "    if os.path.getsize(source_path)==0:\n",
        "      print(\"file has zero size\")\n",
        "    else:\n",
        "      img=image.load_img(source_path, target_size=(pic_size, pic_size)) # Load image\n",
        "      x=image.img_to_array(img)                               # Image to array RGB\n",
        "      x=tf.image.rgb_to_grayscale(x)                          # gray scale conversion\n",
        "      x=np.expand_dims(x, axis=0)\n",
        "      if images is None:\n",
        "        images = x\n",
        "      else:\n",
        "        images = np.vstack((images, x))\n",
        "  print(images.shape)\n",
        "  images=images.reshape(images.shape[0], pic_size*pic_size*1)\n",
        "  print(images.shape)\n",
        "  label_in = np.ones((len(fname), 1))\n",
        "  label_in.fill(label)\n",
        "  print(label_in.shape)\n",
        "  images_new=np.hstack((images,label_in))\n",
        "  print(images_new.shape)\n",
        "  return images_new\n"
      ],
      "execution_count": 198,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ea28Aa7106bF"
      },
      "source": [
        "daisy_source_dir = \"/flower_new/flowers/daisy\"\n",
        "\n",
        "dandelion_source_dir = \"/flower_new/flowers/dandelion\"\n",
        "\n",
        "rose_source_dir = \"/flower_new/flowers/rose\"\n",
        "\n",
        "sunflower_source_dir = \"/flower_new/flowers/sunflower\"\n",
        "\n",
        "tulip_source_dir = \"/flower_new/flowers/tulip\""
      ],
      "execution_count": 199,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_2KZtw7A3N7l"
      },
      "source": [
        "**Converting the Images into tensor**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wI_0ht--10CQ",
        "outputId": "d1417d37-d57d-4c8b-8477-8fbf69989b74"
      },
      "source": [
        "daisy = convert_image(daisy_source_dir, 0)\n",
        "dandelion = convert_image(dandelion_source_dir, 1)\n",
        "rose = convert_image(rose_source_dir, 2)\n",
        "sunflower = convert_image(sunflower_source_dir, 3)\n",
        "tulip = convert_image(tulip_source_dir, 4)"
      ],
      "execution_count": 200,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(769, 50, 50, 1)\n",
            "(769, 2500)\n",
            "(769, 1)\n",
            "(769, 2501)\n",
            "(1052, 50, 50, 1)\n",
            "(1052, 2500)\n",
            "(1052, 1)\n",
            "(1052, 2501)\n",
            "(784, 50, 50, 1)\n",
            "(784, 2500)\n",
            "(784, 1)\n",
            "(784, 2501)\n",
            "(734, 50, 50, 1)\n",
            "(734, 2500)\n",
            "(734, 1)\n",
            "(734, 2501)\n",
            "(984, 50, 50, 1)\n",
            "(984, 2500)\n",
            "(984, 1)\n",
            "(984, 2501)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hR4y7Pgy7LkA",
        "outputId": "a7e4fc3f-ba21-4036-e5e6-7a0c6ded82a2"
      },
      "source": [
        "data_array = np.vstack((daisy, dandelion, rose, sunflower, tulip))\n",
        "data_array.shape\n"
      ],
      "execution_count": 201,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(4323, 2501)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 201
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uswIk-O9hMz-"
      },
      "source": [
        "import pandas as pd\n",
        "df = pd.DataFrame(data_array)"
      ],
      "execution_count": 202,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "chqXV_2-hRa9",
        "outputId": "1462bda0-0936-41f1-d025-426e0c73ce01"
      },
      "source": [
        "df.head(3)"
      ],
      "execution_count": 203,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "      <th>2500</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1.9397</td>\n",
              "      <td>2.3804</td>\n",
              "      <td>7.706500</td>\n",
              "      <td>6.8206</td>\n",
              "      <td>25.164000</td>\n",
              "      <td>14.804501</td>\n",
              "      <td>30.898500</td>\n",
              "      <td>25.794102</td>\n",
              "      <td>6.4570</td>\n",
              "      <td>6.771300</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>24.963701</td>\n",
              "      <td>5.304600</td>\n",
              "      <td>11.875601</td>\n",
              "      <td>33.734901</td>\n",
              "      <td>6.108800</td>\n",
              "      <td>2.999700</td>\n",
              "      <td>23.869900</td>\n",
              "      <td>3.070600</td>\n",
              "      <td>10.043700</td>\n",
              "      <td>21.104301</td>\n",
              "      <td>14.810600</td>\n",
              "      <td>13.983200</td>\n",
              "      <td>15.8922</td>\n",
              "      <td>80.208305</td>\n",
              "      <td>13.370101</td>\n",
              "      <td>42.871201</td>\n",
              "      <td>5.130500</td>\n",
              "      <td>5.7437</td>\n",
              "      <td>48.784500</td>\n",
              "      <td>11.787800</td>\n",
              "      <td>48.343903</td>\n",
              "      <td>61.972603</td>\n",
              "      <td>8.999100</td>\n",
              "      <td>10.576800</td>\n",
              "      <td>1.227900</td>\n",
              "      <td>6.999300</td>\n",
              "      <td>2.021400</td>\n",
              "      <td>17.630100</td>\n",
              "      <td>62.855499</td>\n",
              "      <td>...</td>\n",
              "      <td>40.871403</td>\n",
              "      <td>14.348400</td>\n",
              "      <td>31.055599</td>\n",
              "      <td>14.081900</td>\n",
              "      <td>32.311302</td>\n",
              "      <td>7.309000</td>\n",
              "      <td>13.065001</td>\n",
              "      <td>35.865700</td>\n",
              "      <td>10.353400</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>52.153801</td>\n",
              "      <td>17.902802</td>\n",
              "      <td>5.977800</td>\n",
              "      <td>20.092100</td>\n",
              "      <td>13.146701</td>\n",
              "      <td>49.842701</td>\n",
              "      <td>19.038200</td>\n",
              "      <td>2.771700</td>\n",
              "      <td>3.771600</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>2.999700</td>\n",
              "      <td>1.999800</td>\n",
              "      <td>12.108200</td>\n",
              "      <td>31.052299</td>\n",
              "      <td>27.880201</td>\n",
              "      <td>24.102400</td>\n",
              "      <td>17.152401</td>\n",
              "      <td>11.983400</td>\n",
              "      <td>10.7493</td>\n",
              "      <td>13.075800</td>\n",
              "      <td>0.885900</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>1.999800</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.9999</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>2.8687</td>\n",
              "      <td>0.5870</td>\n",
              "      <td>15.073800</td>\n",
              "      <td>0.0000</td>\n",
              "      <td>2.945800</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>2.059900</td>\n",
              "      <td>5.108900</td>\n",
              "      <td>1.1740</td>\n",
              "      <td>2.059900</td>\n",
              "      <td>5.283000</td>\n",
              "      <td>4.109000</td>\n",
              "      <td>4.994900</td>\n",
              "      <td>4.109000</td>\n",
              "      <td>5.870000</td>\n",
              "      <td>4.994900</td>\n",
              "      <td>7.044000</td>\n",
              "      <td>12.327001</td>\n",
              "      <td>6.983901</td>\n",
              "      <td>0.587000</td>\n",
              "      <td>1.174000</td>\n",
              "      <td>1.174000</td>\n",
              "      <td>7.631000</td>\n",
              "      <td>6.4570</td>\n",
              "      <td>21.826899</td>\n",
              "      <td>18.109200</td>\n",
              "      <td>10.331800</td>\n",
              "      <td>41.879902</td>\n",
              "      <td>76.1782</td>\n",
              "      <td>77.862099</td>\n",
              "      <td>79.861900</td>\n",
              "      <td>80.334908</td>\n",
              "      <td>81.046700</td>\n",
              "      <td>79.334999</td>\n",
              "      <td>78.862007</td>\n",
              "      <td>75.737503</td>\n",
              "      <td>52.296303</td>\n",
              "      <td>65.908104</td>\n",
              "      <td>73.851700</td>\n",
              "      <td>11.967999</td>\n",
              "      <td>...</td>\n",
              "      <td>30.239100</td>\n",
              "      <td>44.749199</td>\n",
              "      <td>51.281902</td>\n",
              "      <td>41.048504</td>\n",
              "      <td>25.288902</td>\n",
              "      <td>27.974300</td>\n",
              "      <td>26.718601</td>\n",
              "      <td>18.190901</td>\n",
              "      <td>4.696000</td>\n",
              "      <td>3.928700</td>\n",
              "      <td>17.191000</td>\n",
              "      <td>49.157303</td>\n",
              "      <td>45.864803</td>\n",
              "      <td>28.849401</td>\n",
              "      <td>33.788902</td>\n",
              "      <td>38.038101</td>\n",
              "      <td>19.941101</td>\n",
              "      <td>26.224100</td>\n",
              "      <td>32.141903</td>\n",
              "      <td>32.126499</td>\n",
              "      <td>35.816402</td>\n",
              "      <td>37.098301</td>\n",
              "      <td>28.821701</td>\n",
              "      <td>12.152901</td>\n",
              "      <td>15.027801</td>\n",
              "      <td>16.120201</td>\n",
              "      <td>213.642899</td>\n",
              "      <td>226.929703</td>\n",
              "      <td>207.283295</td>\n",
              "      <td>129.622009</td>\n",
              "      <td>113.3536</td>\n",
              "      <td>186.998306</td>\n",
              "      <td>190.133606</td>\n",
              "      <td>79.109604</td>\n",
              "      <td>75.871101</td>\n",
              "      <td>65.003204</td>\n",
              "      <td>60.003700</td>\n",
              "      <td>61.231602</td>\n",
              "      <td>40.8036</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>9.1255</td>\n",
              "      <td>7.7558</td>\n",
              "      <td>8.353601</td>\n",
              "      <td>10.8973</td>\n",
              "      <td>15.141901</td>\n",
              "      <td>16.902901</td>\n",
              "      <td>130.888306</td>\n",
              "      <td>162.373505</td>\n",
              "      <td>149.6306</td>\n",
              "      <td>75.687302</td>\n",
              "      <td>182.915405</td>\n",
              "      <td>191.131805</td>\n",
              "      <td>139.235703</td>\n",
              "      <td>183.915314</td>\n",
              "      <td>135.855408</td>\n",
              "      <td>13.060401</td>\n",
              "      <td>188.914795</td>\n",
              "      <td>202.625305</td>\n",
              "      <td>214.624100</td>\n",
              "      <td>224.270309</td>\n",
              "      <td>216.982895</td>\n",
              "      <td>202.600906</td>\n",
              "      <td>121.067902</td>\n",
              "      <td>75.8274</td>\n",
              "      <td>189.996399</td>\n",
              "      <td>193.985214</td>\n",
              "      <td>197.273010</td>\n",
              "      <td>15.853700</td>\n",
              "      <td>9.1147</td>\n",
              "      <td>213.673508</td>\n",
              "      <td>194.903397</td>\n",
              "      <td>185.626999</td>\n",
              "      <td>195.675308</td>\n",
              "      <td>191.626404</td>\n",
              "      <td>2.935000</td>\n",
              "      <td>189.964005</td>\n",
              "      <td>179.133011</td>\n",
              "      <td>170.611511</td>\n",
              "      <td>133.148407</td>\n",
              "      <td>204.831497</td>\n",
              "      <td>...</td>\n",
              "      <td>84.843506</td>\n",
              "      <td>103.233101</td>\n",
              "      <td>206.912994</td>\n",
              "      <td>204.141296</td>\n",
              "      <td>17.948900</td>\n",
              "      <td>158.162903</td>\n",
              "      <td>205.130402</td>\n",
              "      <td>212.140503</td>\n",
              "      <td>212.912399</td>\n",
              "      <td>208.852707</td>\n",
              "      <td>97.022697</td>\n",
              "      <td>205.902298</td>\n",
              "      <td>208.912796</td>\n",
              "      <td>217.221695</td>\n",
              "      <td>134.713806</td>\n",
              "      <td>211.912506</td>\n",
              "      <td>206.852890</td>\n",
              "      <td>181.225311</td>\n",
              "      <td>197.881607</td>\n",
              "      <td>179.274811</td>\n",
              "      <td>212.912399</td>\n",
              "      <td>212.129700</td>\n",
              "      <td>214.129501</td>\n",
              "      <td>190.370712</td>\n",
              "      <td>190.914612</td>\n",
              "      <td>187.773102</td>\n",
              "      <td>196.114304</td>\n",
              "      <td>19.468201</td>\n",
              "      <td>29.022001</td>\n",
              "      <td>8.815800</td>\n",
              "      <td>5.9840</td>\n",
              "      <td>6.457000</td>\n",
              "      <td>33.095303</td>\n",
              "      <td>161.063904</td>\n",
              "      <td>162.775604</td>\n",
              "      <td>121.916901</td>\n",
              "      <td>15.131101</td>\n",
              "      <td>18.674702</td>\n",
              "      <td>24.0009</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 2501 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "     0       1          2        3     ...       2497       2498     2499  2500\n",
              "0  1.9397  2.3804   7.706500   6.8206  ...   0.999900   0.999900   0.9999   0.0\n",
              "1  2.8687  0.5870  15.073800   0.0000  ...  60.003700  61.231602  40.8036   0.0\n",
              "2  9.1255  7.7558   8.353601  10.8973  ...  15.131101  18.674702  24.0009   0.0\n",
              "\n",
              "[3 rows x 2501 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 203
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "4ULRcxfPkq4N",
        "outputId": "706ae723-98ca-4eba-d32e-9054be67ce74"
      },
      "source": [
        "df.tail(3)"
      ],
      "execution_count": 204,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "      <th>2500</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>4320</th>\n",
              "      <td>149.933105</td>\n",
              "      <td>187.744202</td>\n",
              "      <td>200.721298</td>\n",
              "      <td>196.102402</td>\n",
              "      <td>191.967300</td>\n",
              "      <td>196.874298</td>\n",
              "      <td>195.031601</td>\n",
              "      <td>181.218002</td>\n",
              "      <td>164.142700</td>\n",
              "      <td>155.659698</td>\n",
              "      <td>162.865402</td>\n",
              "      <td>153.904907</td>\n",
              "      <td>148.954697</td>\n",
              "      <td>136.179398</td>\n",
              "      <td>123.835503</td>\n",
              "      <td>116.278404</td>\n",
              "      <td>116.181404</td>\n",
              "      <td>122.125404</td>\n",
              "      <td>130.078400</td>\n",
              "      <td>128.761093</td>\n",
              "      <td>125.994003</td>\n",
              "      <td>105.148804</td>\n",
              "      <td>96.237701</td>\n",
              "      <td>94.902100</td>\n",
              "      <td>132.032104</td>\n",
              "      <td>152.937408</td>\n",
              "      <td>162.577301</td>\n",
              "      <td>162.577301</td>\n",
              "      <td>153.855591</td>\n",
              "      <td>178.837616</td>\n",
              "      <td>199.091309</td>\n",
              "      <td>199.194595</td>\n",
              "      <td>189.939789</td>\n",
              "      <td>178.951706</td>\n",
              "      <td>158.992203</td>\n",
              "      <td>155.211304</td>\n",
              "      <td>158.988998</td>\n",
              "      <td>157.881210</td>\n",
              "      <td>153.920212</td>\n",
              "      <td>156.967712</td>\n",
              "      <td>...</td>\n",
              "      <td>98.877899</td>\n",
              "      <td>114.947205</td>\n",
              "      <td>116.983803</td>\n",
              "      <td>100.147102</td>\n",
              "      <td>101.076103</td>\n",
              "      <td>120.661308</td>\n",
              "      <td>123.047905</td>\n",
              "      <td>115.755905</td>\n",
              "      <td>117.114799</td>\n",
              "      <td>105.339401</td>\n",
              "      <td>78.625603</td>\n",
              "      <td>72.848000</td>\n",
              "      <td>90.274605</td>\n",
              "      <td>101.740303</td>\n",
              "      <td>103.045303</td>\n",
              "      <td>143.911896</td>\n",
              "      <td>185.636719</td>\n",
              "      <td>179.181503</td>\n",
              "      <td>185.191605</td>\n",
              "      <td>154.951004</td>\n",
              "      <td>53.310600</td>\n",
              "      <td>83.899101</td>\n",
              "      <td>115.780296</td>\n",
              "      <td>123.785706</td>\n",
              "      <td>79.060402</td>\n",
              "      <td>89.222404</td>\n",
              "      <td>82.982498</td>\n",
              "      <td>102.847900</td>\n",
              "      <td>114.720306</td>\n",
              "      <td>85.898903</td>\n",
              "      <td>97.635902</td>\n",
              "      <td>68.788902</td>\n",
              "      <td>70.925705</td>\n",
              "      <td>53.770203</td>\n",
              "      <td>36.799599</td>\n",
              "      <td>36.006100</td>\n",
              "      <td>39.293900</td>\n",
              "      <td>53.927402</td>\n",
              "      <td>70.218399</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4321</th>\n",
              "      <td>106.086700</td>\n",
              "      <td>140.731598</td>\n",
              "      <td>154.343399</td>\n",
              "      <td>147.017395</td>\n",
              "      <td>115.743103</td>\n",
              "      <td>75.112503</td>\n",
              "      <td>98.832802</td>\n",
              "      <td>156.300201</td>\n",
              "      <td>173.820908</td>\n",
              "      <td>139.907608</td>\n",
              "      <td>86.385902</td>\n",
              "      <td>87.125404</td>\n",
              "      <td>123.392899</td>\n",
              "      <td>171.244705</td>\n",
              "      <td>192.083893</td>\n",
              "      <td>183.845901</td>\n",
              "      <td>173.178207</td>\n",
              "      <td>189.198196</td>\n",
              "      <td>202.136810</td>\n",
              "      <td>211.570404</td>\n",
              "      <td>216.569901</td>\n",
              "      <td>221.569412</td>\n",
              "      <td>219.569611</td>\n",
              "      <td>214.939911</td>\n",
              "      <td>209.217804</td>\n",
              "      <td>211.217590</td>\n",
              "      <td>212.217514</td>\n",
              "      <td>214.858200</td>\n",
              "      <td>214.570099</td>\n",
              "      <td>218.569717</td>\n",
              "      <td>225.166901</td>\n",
              "      <td>229.568604</td>\n",
              "      <td>226.568893</td>\n",
              "      <td>228.166595</td>\n",
              "      <td>228.166595</td>\n",
              "      <td>225.857101</td>\n",
              "      <td>152.147903</td>\n",
              "      <td>182.254501</td>\n",
              "      <td>223.108704</td>\n",
              "      <td>218.767105</td>\n",
              "      <td>...</td>\n",
              "      <td>67.822304</td>\n",
              "      <td>66.790199</td>\n",
              "      <td>120.146904</td>\n",
              "      <td>120.153000</td>\n",
              "      <td>116.903702</td>\n",
              "      <td>162.131805</td>\n",
              "      <td>190.263000</td>\n",
              "      <td>166.746292</td>\n",
              "      <td>122.072800</td>\n",
              "      <td>208.766602</td>\n",
              "      <td>205.042801</td>\n",
              "      <td>207.270599</td>\n",
              "      <td>215.976898</td>\n",
              "      <td>220.954803</td>\n",
              "      <td>219.987305</td>\n",
              "      <td>221.965500</td>\n",
              "      <td>221.242905</td>\n",
              "      <td>202.707016</td>\n",
              "      <td>73.854004</td>\n",
              "      <td>57.812500</td>\n",
              "      <td>59.915501</td>\n",
              "      <td>55.039200</td>\n",
              "      <td>78.978401</td>\n",
              "      <td>138.146805</td>\n",
              "      <td>175.119812</td>\n",
              "      <td>175.152100</td>\n",
              "      <td>139.083496</td>\n",
              "      <td>145.939407</td>\n",
              "      <td>92.399200</td>\n",
              "      <td>34.828602</td>\n",
              "      <td>35.127499</td>\n",
              "      <td>37.817501</td>\n",
              "      <td>48.979698</td>\n",
              "      <td>78.978203</td>\n",
              "      <td>59.795300</td>\n",
              "      <td>67.078102</td>\n",
              "      <td>89.829399</td>\n",
              "      <td>101.102402</td>\n",
              "      <td>44.747501</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4322</th>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.746506</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>250.746902</td>\n",
              "      <td>99.964005</td>\n",
              "      <td>91.209900</td>\n",
              "      <td>103.920502</td>\n",
              "      <td>107.930908</td>\n",
              "      <td>129.223099</td>\n",
              "      <td>152.747711</td>\n",
              "      <td>253.720505</td>\n",
              "      <td>254.262695</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>250.817810</td>\n",
              "      <td>234.705414</td>\n",
              "      <td>203.708511</td>\n",
              "      <td>187.129318</td>\n",
              "      <td>173.990509</td>\n",
              "      <td>254.105606</td>\n",
              "      <td>254.387512</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>...</td>\n",
              "      <td>34.234001</td>\n",
              "      <td>18.549900</td>\n",
              "      <td>17.674801</td>\n",
              "      <td>12.082100</td>\n",
              "      <td>114.625198</td>\n",
              "      <td>98.865601</td>\n",
              "      <td>108.787498</td>\n",
              "      <td>59.022202</td>\n",
              "      <td>69.249207</td>\n",
              "      <td>130.217407</td>\n",
              "      <td>46.268501</td>\n",
              "      <td>38.158501</td>\n",
              "      <td>64.857803</td>\n",
              "      <td>18.674702</td>\n",
              "      <td>100.837601</td>\n",
              "      <td>97.090805</td>\n",
              "      <td>94.095703</td>\n",
              "      <td>84.742195</td>\n",
              "      <td>92.872398</td>\n",
              "      <td>61.874599</td>\n",
              "      <td>28.961800</td>\n",
              "      <td>51.141602</td>\n",
              "      <td>67.139999</td>\n",
              "      <td>25.059200</td>\n",
              "      <td>13.853900</td>\n",
              "      <td>11.609100</td>\n",
              "      <td>14.206700</td>\n",
              "      <td>17.212601</td>\n",
              "      <td>20.962599</td>\n",
              "      <td>23.130301</td>\n",
              "      <td>17.608501</td>\n",
              "      <td>24.891300</td>\n",
              "      <td>28.847799</td>\n",
              "      <td>14.864600</td>\n",
              "      <td>20.190701</td>\n",
              "      <td>12.082100</td>\n",
              "      <td>16.369801</td>\n",
              "      <td>25.848101</td>\n",
              "      <td>23.674200</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 2501 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0           1           2     ...        2498       2499  2500\n",
              "4320  149.933105  187.744202  200.721298  ...   53.927402  70.218399   4.0\n",
              "4321  106.086700  140.731598  154.343399  ...  101.102402  44.747501   4.0\n",
              "4322  254.974518  254.974518  254.974518  ...   25.848101  23.674200   4.0\n",
              "\n",
              "[3 rows x 2501 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 204
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L3ux-iza3hNm"
      },
      "source": [
        "**Splittig the Training and Testing Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xq7uYQe2kwLv"
      },
      "source": [
        "x_data = df.sample(frac=1) # Shuffling the data\n",
        "\n",
        "train_data_full= x_data.iloc[:2540,:]\n",
        "test_data_full = x_data.iloc[2540:, :]\n",
        "train_data = train_data_full.iloc[:, :-1]\n",
        "train_labels = train_data_full.iloc[:, -1]\n",
        "test_data = test_data_full.iloc[:, :-1]\n",
        "test_labels = test_data_full.iloc[:, -1]\n"
      ],
      "execution_count": 206,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nAwHh-PITTzI",
        "outputId": "387fc2cb-84b8-4e60-ad3a-315360be035b"
      },
      "source": [
        "print(train_data.shape)\n",
        "print(test_data.shape)\n",
        "print(train_labels.shape)\n",
        "print(test_labels.shape)"
      ],
      "execution_count": 207,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(2540, 2500)\n",
            "(1783, 2500)\n",
            "(2540,)\n",
            "(1783,)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "qUKvBM9mlB7K",
        "outputId": "c3692e39-8b1f-4513-c5ff-26b5774e4197"
      },
      "source": [
        "train_data.head(3)"
      ],
      "execution_count": 208,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2460</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3110</th>\n",
              "      <td>65.115303</td>\n",
              "      <td>67.766800</td>\n",
              "      <td>69.848305</td>\n",
              "      <td>68.951706</td>\n",
              "      <td>73.261002</td>\n",
              "      <td>73.880295</td>\n",
              "      <td>69.087204</td>\n",
              "      <td>60.088100</td>\n",
              "      <td>67.914803</td>\n",
              "      <td>82.039803</td>\n",
              "      <td>86.860603</td>\n",
              "      <td>83.822403</td>\n",
              "      <td>77.044907</td>\n",
              "      <td>70.839203</td>\n",
              "      <td>114.174606</td>\n",
              "      <td>221.853195</td>\n",
              "      <td>63.818199</td>\n",
              "      <td>70.105698</td>\n",
              "      <td>81.119904</td>\n",
              "      <td>76.826004</td>\n",
              "      <td>67.364700</td>\n",
              "      <td>61.202000</td>\n",
              "      <td>61.913799</td>\n",
              "      <td>63.141701</td>\n",
              "      <td>66.042702</td>\n",
              "      <td>67.683502</td>\n",
              "      <td>63.010700</td>\n",
              "      <td>59.783001</td>\n",
              "      <td>66.669899</td>\n",
              "      <td>69.665001</td>\n",
              "      <td>68.936203</td>\n",
              "      <td>66.142998</td>\n",
              "      <td>81.958099</td>\n",
              "      <td>85.224304</td>\n",
              "      <td>70.931404</td>\n",
              "      <td>53.182800</td>\n",
              "      <td>52.845402</td>\n",
              "      <td>65.246300</td>\n",
              "      <td>70.903694</td>\n",
              "      <td>75.023499</td>\n",
              "      <td>...</td>\n",
              "      <td>100.130699</td>\n",
              "      <td>73.762001</td>\n",
              "      <td>75.453705</td>\n",
              "      <td>72.082603</td>\n",
              "      <td>79.761406</td>\n",
              "      <td>69.072105</td>\n",
              "      <td>68.328003</td>\n",
              "      <td>68.950401</td>\n",
              "      <td>87.256798</td>\n",
              "      <td>79.969398</td>\n",
              "      <td>110.690598</td>\n",
              "      <td>77.148399</td>\n",
              "      <td>68.988907</td>\n",
              "      <td>67.862595</td>\n",
              "      <td>101.089096</td>\n",
              "      <td>107.812706</td>\n",
              "      <td>103.095100</td>\n",
              "      <td>99.947403</td>\n",
              "      <td>99.040001</td>\n",
              "      <td>121.155006</td>\n",
              "      <td>114.921402</td>\n",
              "      <td>152.931305</td>\n",
              "      <td>161.329803</td>\n",
              "      <td>183.204102</td>\n",
              "      <td>188.403397</td>\n",
              "      <td>162.105301</td>\n",
              "      <td>153.974396</td>\n",
              "      <td>156.080811</td>\n",
              "      <td>105.880508</td>\n",
              "      <td>123.903503</td>\n",
              "      <td>144.940002</td>\n",
              "      <td>129.165009</td>\n",
              "      <td>222.069305</td>\n",
              "      <td>214.278503</td>\n",
              "      <td>74.322601</td>\n",
              "      <td>74.066902</td>\n",
              "      <td>73.049995</td>\n",
              "      <td>77.034203</td>\n",
              "      <td>69.018005</td>\n",
              "      <td>47.915199</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2825</th>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>...</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "      <td>254.974518</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1312</th>\n",
              "      <td>29.037401</td>\n",
              "      <td>18.113901</td>\n",
              "      <td>13.918600</td>\n",
              "      <td>10.858800</td>\n",
              "      <td>14.146600</td>\n",
              "      <td>18.918100</td>\n",
              "      <td>24.934500</td>\n",
              "      <td>48.161903</td>\n",
              "      <td>60.741501</td>\n",
              "      <td>54.340000</td>\n",
              "      <td>48.118801</td>\n",
              "      <td>44.130001</td>\n",
              "      <td>43.358101</td>\n",
              "      <td>42.825001</td>\n",
              "      <td>38.743702</td>\n",
              "      <td>34.178600</td>\n",
              "      <td>39.178101</td>\n",
              "      <td>48.661003</td>\n",
              "      <td>49.660900</td>\n",
              "      <td>38.868401</td>\n",
              "      <td>38.010300</td>\n",
              "      <td>39.141201</td>\n",
              "      <td>34.771801</td>\n",
              "      <td>34.690102</td>\n",
              "      <td>35.689999</td>\n",
              "      <td>40.771202</td>\n",
              "      <td>42.928101</td>\n",
              "      <td>45.145103</td>\n",
              "      <td>44.270000</td>\n",
              "      <td>45.003399</td>\n",
              "      <td>46.031101</td>\n",
              "      <td>46.112804</td>\n",
              "      <td>42.184101</td>\n",
              "      <td>46.901699</td>\n",
              "      <td>47.836899</td>\n",
              "      <td>32.853699</td>\n",
              "      <td>20.934900</td>\n",
              "      <td>37.820900</td>\n",
              "      <td>55.195198</td>\n",
              "      <td>57.885204</td>\n",
              "      <td>...</td>\n",
              "      <td>46.325397</td>\n",
              "      <td>43.385799</td>\n",
              "      <td>28.157600</td>\n",
              "      <td>26.934299</td>\n",
              "      <td>26.162401</td>\n",
              "      <td>23.102600</td>\n",
              "      <td>29.645901</td>\n",
              "      <td>41.259598</td>\n",
              "      <td>41.889702</td>\n",
              "      <td>34.004501</td>\n",
              "      <td>21.205999</td>\n",
              "      <td>15.836700</td>\n",
              "      <td>13.836900</td>\n",
              "      <td>10.837200</td>\n",
              "      <td>9.723300</td>\n",
              "      <td>60.878803</td>\n",
              "      <td>16.124800</td>\n",
              "      <td>11.353300</td>\n",
              "      <td>11.353300</td>\n",
              "      <td>9.353499</td>\n",
              "      <td>7.994600</td>\n",
              "      <td>6.820600</td>\n",
              "      <td>10.065300</td>\n",
              "      <td>10.147000</td>\n",
              "      <td>10.918900</td>\n",
              "      <td>11.918800</td>\n",
              "      <td>13.804600</td>\n",
              "      <td>19.163099</td>\n",
              "      <td>19.874901</td>\n",
              "      <td>19.103001</td>\n",
              "      <td>19.113800</td>\n",
              "      <td>17.103201</td>\n",
              "      <td>14.630401</td>\n",
              "      <td>13.918600</td>\n",
              "      <td>15.918400</td>\n",
              "      <td>19.113800</td>\n",
              "      <td>19.885700</td>\n",
              "      <td>18.113901</td>\n",
              "      <td>15.630301</td>\n",
              "      <td>15.918400</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 2500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0           1           2     ...        2497        2498        2499\n",
              "3110   65.115303   67.766800   69.848305  ...   77.034203   69.018005   47.915199\n",
              "2825  254.974518  254.974518  254.974518  ...  254.974518  254.974518  254.974518\n",
              "1312   29.037401   18.113901   13.918600  ...   18.113901   15.630301   15.918400\n",
              "\n",
              "[3 rows x 2500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 208
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "t0nW6mFqleBM",
        "outputId": "737f0af3-c665-4552-8fce-e67aa8c5512b"
      },
      "source": [
        "test_data.tail(3)"
      ],
      "execution_count": 209,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2460</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2826</th>\n",
              "      <td>93.735695</td>\n",
              "      <td>106.104309</td>\n",
              "      <td>86.007500</td>\n",
              "      <td>80.105003</td>\n",
              "      <td>77.176201</td>\n",
              "      <td>80.855400</td>\n",
              "      <td>81.855301</td>\n",
              "      <td>81.094200</td>\n",
              "      <td>80.773705</td>\n",
              "      <td>79.773796</td>\n",
              "      <td>78.773903</td>\n",
              "      <td>78.773903</td>\n",
              "      <td>79.062004</td>\n",
              "      <td>79.371704</td>\n",
              "      <td>80.855400</td>\n",
              "      <td>81.866096</td>\n",
              "      <td>80.132797</td>\n",
              "      <td>79.371704</td>\n",
              "      <td>74.774300</td>\n",
              "      <td>72.774506</td>\n",
              "      <td>75.133301</td>\n",
              "      <td>74.062500</td>\n",
              "      <td>74.13340</td>\n",
              "      <td>75.372101</td>\n",
              "      <td>73.372299</td>\n",
              "      <td>72.774506</td>\n",
              "      <td>72.774506</td>\n",
              "      <td>71.774597</td>\n",
              "      <td>71.372505</td>\n",
              "      <td>71.062805</td>\n",
              "      <td>69.774803</td>\n",
              "      <td>69.372704</td>\n",
              "      <td>69.144699</td>\n",
              "      <td>71.144501</td>\n",
              "      <td>67.856705</td>\n",
              "      <td>64.835396</td>\n",
              "      <td>65.895401</td>\n",
              "      <td>65.856903</td>\n",
              "      <td>65.145096</td>\n",
              "      <td>65.145096</td>\n",
              "      <td>...</td>\n",
              "      <td>5.603500</td>\n",
              "      <td>25.392200</td>\n",
              "      <td>44.886501</td>\n",
              "      <td>69.092300</td>\n",
              "      <td>29.901602</td>\n",
              "      <td>36.900997</td>\n",
              "      <td>19.201700</td>\n",
              "      <td>60.097702</td>\n",
              "      <td>71.004204</td>\n",
              "      <td>24.452301</td>\n",
              "      <td>48.681301</td>\n",
              "      <td>36.705299</td>\n",
              "      <td>45.949398</td>\n",
              "      <td>64.845901</td>\n",
              "      <td>74.960602</td>\n",
              "      <td>115.882706</td>\n",
              "      <td>95.846107</td>\n",
              "      <td>108.943504</td>\n",
              "      <td>84.160301</td>\n",
              "      <td>80.083603</td>\n",
              "      <td>56.951599</td>\n",
              "      <td>35.304901</td>\n",
              "      <td>56.956200</td>\n",
              "      <td>61.219002</td>\n",
              "      <td>24.794401</td>\n",
              "      <td>51.838203</td>\n",
              "      <td>63.917000</td>\n",
              "      <td>65.616302</td>\n",
              "      <td>59.695801</td>\n",
              "      <td>8.315000</td>\n",
              "      <td>55.302898</td>\n",
              "      <td>29.140499</td>\n",
              "      <td>5.870000</td>\n",
              "      <td>2.331000</td>\n",
              "      <td>13.864700</td>\n",
              "      <td>16.993700</td>\n",
              "      <td>58.925404</td>\n",
              "      <td>69.138206</td>\n",
              "      <td>76.197701</td>\n",
              "      <td>105.208900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>103.215004</td>\n",
              "      <td>162.211014</td>\n",
              "      <td>168.808304</td>\n",
              "      <td>154.373505</td>\n",
              "      <td>115.940903</td>\n",
              "      <td>63.072601</td>\n",
              "      <td>39.908901</td>\n",
              "      <td>32.954502</td>\n",
              "      <td>30.303101</td>\n",
              "      <td>28.075300</td>\n",
              "      <td>20.787899</td>\n",
              "      <td>20.026800</td>\n",
              "      <td>30.742001</td>\n",
              "      <td>55.186401</td>\n",
              "      <td>88.893906</td>\n",
              "      <td>116.118095</td>\n",
              "      <td>119.233505</td>\n",
              "      <td>114.075104</td>\n",
              "      <td>89.845901</td>\n",
              "      <td>53.339001</td>\n",
              "      <td>47.452000</td>\n",
              "      <td>41.233799</td>\n",
              "      <td>54.17680</td>\n",
              "      <td>69.142799</td>\n",
              "      <td>82.680901</td>\n",
              "      <td>97.139099</td>\n",
              "      <td>97.832901</td>\n",
              "      <td>101.221207</td>\n",
              "      <td>121.592400</td>\n",
              "      <td>130.780899</td>\n",
              "      <td>112.871704</td>\n",
              "      <td>73.887398</td>\n",
              "      <td>47.918800</td>\n",
              "      <td>78.764000</td>\n",
              "      <td>128.116501</td>\n",
              "      <td>158.267899</td>\n",
              "      <td>153.057404</td>\n",
              "      <td>106.827599</td>\n",
              "      <td>81.937805</td>\n",
              "      <td>88.255905</td>\n",
              "      <td>...</td>\n",
              "      <td>36.156002</td>\n",
              "      <td>37.138901</td>\n",
              "      <td>40.099998</td>\n",
              "      <td>47.224003</td>\n",
              "      <td>47.680000</td>\n",
              "      <td>44.095001</td>\n",
              "      <td>46.853001</td>\n",
              "      <td>55.292801</td>\n",
              "      <td>86.115196</td>\n",
              "      <td>116.116905</td>\n",
              "      <td>141.918701</td>\n",
              "      <td>151.152008</td>\n",
              "      <td>155.906494</td>\n",
              "      <td>154.401199</td>\n",
              "      <td>143.064804</td>\n",
              "      <td>164.215393</td>\n",
              "      <td>143.097198</td>\n",
              "      <td>188.051407</td>\n",
              "      <td>111.237709</td>\n",
              "      <td>133.837799</td>\n",
              "      <td>70.727104</td>\n",
              "      <td>65.416000</td>\n",
              "      <td>79.799606</td>\n",
              "      <td>69.189400</td>\n",
              "      <td>69.924599</td>\n",
              "      <td>70.305199</td>\n",
              "      <td>75.055199</td>\n",
              "      <td>87.258804</td>\n",
              "      <td>89.116707</td>\n",
              "      <td>114.813400</td>\n",
              "      <td>122.887901</td>\n",
              "      <td>79.588600</td>\n",
              "      <td>45.926903</td>\n",
              "      <td>36.114601</td>\n",
              "      <td>54.880001</td>\n",
              "      <td>118.167999</td>\n",
              "      <td>164.068695</td>\n",
              "      <td>186.140808</td>\n",
              "      <td>205.647110</td>\n",
              "      <td>197.978012</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1692</th>\n",
              "      <td>75.881500</td>\n",
              "      <td>71.789505</td>\n",
              "      <td>87.140900</td>\n",
              "      <td>104.877205</td>\n",
              "      <td>112.881004</td>\n",
              "      <td>116.592499</td>\n",
              "      <td>116.837502</td>\n",
              "      <td>120.951103</td>\n",
              "      <td>119.652306</td>\n",
              "      <td>122.591904</td>\n",
              "      <td>125.707306</td>\n",
              "      <td>144.855103</td>\n",
              "      <td>156.623093</td>\n",
              "      <td>144.822800</td>\n",
              "      <td>129.066010</td>\n",
              "      <td>122.652000</td>\n",
              "      <td>119.037506</td>\n",
              "      <td>88.965103</td>\n",
              "      <td>107.219002</td>\n",
              "      <td>127.072708</td>\n",
              "      <td>141.084503</td>\n",
              "      <td>114.592705</td>\n",
              "      <td>121.59201</td>\n",
              "      <td>126.950500</td>\n",
              "      <td>127.879501</td>\n",
              "      <td>133.395111</td>\n",
              "      <td>135.106796</td>\n",
              "      <td>141.035309</td>\n",
              "      <td>146.105698</td>\n",
              "      <td>134.878799</td>\n",
              "      <td>120.961906</td>\n",
              "      <td>117.940605</td>\n",
              "      <td>120.820107</td>\n",
              "      <td>123.879906</td>\n",
              "      <td>128.748398</td>\n",
              "      <td>133.747894</td>\n",
              "      <td>131.167313</td>\n",
              "      <td>125.107803</td>\n",
              "      <td>129.879303</td>\n",
              "      <td>142.138397</td>\n",
              "      <td>...</td>\n",
              "      <td>71.002304</td>\n",
              "      <td>84.158104</td>\n",
              "      <td>91.118904</td>\n",
              "      <td>92.797005</td>\n",
              "      <td>107.185303</td>\n",
              "      <td>80.074104</td>\n",
              "      <td>74.835594</td>\n",
              "      <td>89.812508</td>\n",
              "      <td>78.996902</td>\n",
              "      <td>54.876202</td>\n",
              "      <td>69.696007</td>\n",
              "      <td>77.712204</td>\n",
              "      <td>101.030403</td>\n",
              "      <td>98.739700</td>\n",
              "      <td>99.842804</td>\n",
              "      <td>106.784805</td>\n",
              "      <td>84.227699</td>\n",
              "      <td>69.272301</td>\n",
              "      <td>61.195999</td>\n",
              "      <td>67.064400</td>\n",
              "      <td>68.073502</td>\n",
              "      <td>57.020699</td>\n",
              "      <td>56.136402</td>\n",
              "      <td>73.726303</td>\n",
              "      <td>87.817299</td>\n",
              "      <td>116.225800</td>\n",
              "      <td>130.788208</td>\n",
              "      <td>114.937798</td>\n",
              "      <td>113.047401</td>\n",
              "      <td>105.820198</td>\n",
              "      <td>123.832008</td>\n",
              "      <td>126.885704</td>\n",
              "      <td>96.998199</td>\n",
              "      <td>75.924797</td>\n",
              "      <td>71.657104</td>\n",
              "      <td>60.974102</td>\n",
              "      <td>82.119804</td>\n",
              "      <td>71.897499</td>\n",
              "      <td>61.032501</td>\n",
              "      <td>61.114201</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 2500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "            0           1           2     ...        2497        2498        2499\n",
              "2826   93.735695  106.104309   86.007500  ...   69.138206   76.197701  105.208900\n",
              "450   103.215004  162.211014  168.808304  ...  186.140808  205.647110  197.978012\n",
              "1692   75.881500   71.789505   87.140900  ...   71.897499   61.032501   61.114201\n",
              "\n",
              "[3 rows x 2500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 209
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rv2OuJJXlelv",
        "outputId": "38a3b2ec-d01d-477a-de51-65dcd9da4e93"
      },
      "source": [
        "train_labels.head(3)"
      ],
      "execution_count": 210,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3110    3.0\n",
              "2825    3.0\n",
              "1312    1.0\n",
              "Name: 2500, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 210
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UyBhp4tfle8-",
        "outputId": "7d393f1a-3a91-441b-9d66-decafb21d67f"
      },
      "source": [
        "test_labels.tail(3)"
      ],
      "execution_count": 211,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2826    3.0\n",
              "450     0.0\n",
              "1692    1.0\n",
              "Name: 2500, dtype: float64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 211
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2agMcZm66QWP"
      },
      "source": [
        "**One Hot encoding**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "guIhg7cdJ1XV"
      },
      "source": [
        "train_labels = to_categorical(train_labels)\n",
        "test_labels = to_categorical(test_labels)"
      ],
      "execution_count": 212,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AzPx3re4l5Fu",
        "outputId": "b312232d-e57b-4430-b087-45fba6edbbfc"
      },
      "source": [
        "train_labels[:3]"
      ],
      "execution_count": 213,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0.],\n",
              "       [0., 0., 0., 1., 0.],\n",
              "       [0., 1., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 213
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MyLY2V7mmG5w",
        "outputId": "d63afcf9-792b-4249-c265-04f4dadc21c2"
      },
      "source": [
        "test_labels[-3:]"
      ],
      "execution_count": 214,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([[0., 0., 0., 1., 0.],\n",
              "       [1., 0., 0., 0., 0.],\n",
              "       [0., 1., 0., 0., 0.]], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 214
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tr5rYzqP6ck0"
      },
      "source": [
        "**Normalizing the Data**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WQyRPKppQRs7"
      },
      "source": [
        "train_data=train_data.astype(\"float64\")/255\n",
        "test_data = test_data.astype(\"float64\")/255"
      ],
      "execution_count": 215,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "ohzFR13FmZ1O",
        "outputId": "1f27e9ae-575e-4670-b208-95ccdacc99a0"
      },
      "source": [
        "train_data.head(3)"
      ],
      "execution_count": 216,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2460</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>3110</th>\n",
              "      <td>0.255354</td>\n",
              "      <td>0.265752</td>\n",
              "      <td>0.273915</td>\n",
              "      <td>0.270399</td>\n",
              "      <td>0.287298</td>\n",
              "      <td>0.289727</td>\n",
              "      <td>0.270930</td>\n",
              "      <td>0.23564</td>\n",
              "      <td>0.266333</td>\n",
              "      <td>0.321725</td>\n",
              "      <td>0.340630</td>\n",
              "      <td>0.328715</td>\n",
              "      <td>0.302137</td>\n",
              "      <td>0.277801</td>\n",
              "      <td>0.447744</td>\n",
              "      <td>0.870013</td>\n",
              "      <td>0.250267</td>\n",
              "      <td>0.274924</td>\n",
              "      <td>0.318117</td>\n",
              "      <td>0.301278</td>\n",
              "      <td>0.264175</td>\n",
              "      <td>0.240008</td>\n",
              "      <td>0.242799</td>\n",
              "      <td>0.247615</td>\n",
              "      <td>0.258991</td>\n",
              "      <td>0.265425</td>\n",
              "      <td>0.247101</td>\n",
              "      <td>0.234443</td>\n",
              "      <td>0.261451</td>\n",
              "      <td>0.273196</td>\n",
              "      <td>0.270338</td>\n",
              "      <td>0.259384</td>\n",
              "      <td>0.321404</td>\n",
              "      <td>0.334213</td>\n",
              "      <td>0.278162</td>\n",
              "      <td>0.208560</td>\n",
              "      <td>0.207237</td>\n",
              "      <td>0.255868</td>\n",
              "      <td>0.278054</td>\n",
              "      <td>0.294210</td>\n",
              "      <td>...</td>\n",
              "      <td>0.392669</td>\n",
              "      <td>0.289263</td>\n",
              "      <td>0.295897</td>\n",
              "      <td>0.282677</td>\n",
              "      <td>0.312790</td>\n",
              "      <td>0.270871</td>\n",
              "      <td>0.267953</td>\n",
              "      <td>0.270394</td>\n",
              "      <td>0.342184</td>\n",
              "      <td>0.313605</td>\n",
              "      <td>0.434081</td>\n",
              "      <td>0.302543</td>\n",
              "      <td>0.270545</td>\n",
              "      <td>0.266128</td>\n",
              "      <td>0.396428</td>\n",
              "      <td>0.422795</td>\n",
              "      <td>0.404295</td>\n",
              "      <td>0.391951</td>\n",
              "      <td>0.388392</td>\n",
              "      <td>0.475118</td>\n",
              "      <td>0.450672</td>\n",
              "      <td>0.599731</td>\n",
              "      <td>0.632666</td>\n",
              "      <td>0.718447</td>\n",
              "      <td>0.738837</td>\n",
              "      <td>0.635707</td>\n",
              "      <td>0.603821</td>\n",
              "      <td>0.612082</td>\n",
              "      <td>0.415218</td>\n",
              "      <td>0.485896</td>\n",
              "      <td>0.568392</td>\n",
              "      <td>0.506529</td>\n",
              "      <td>0.870860</td>\n",
              "      <td>0.840308</td>\n",
              "      <td>0.291461</td>\n",
              "      <td>0.290458</td>\n",
              "      <td>0.286471</td>\n",
              "      <td>0.302095</td>\n",
              "      <td>0.270659</td>\n",
              "      <td>0.187903</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2825</th>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.99990</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>...</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "      <td>0.999900</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1312</th>\n",
              "      <td>0.113872</td>\n",
              "      <td>0.071035</td>\n",
              "      <td>0.054583</td>\n",
              "      <td>0.042584</td>\n",
              "      <td>0.055477</td>\n",
              "      <td>0.074189</td>\n",
              "      <td>0.097782</td>\n",
              "      <td>0.18887</td>\n",
              "      <td>0.238202</td>\n",
              "      <td>0.213098</td>\n",
              "      <td>0.188701</td>\n",
              "      <td>0.173059</td>\n",
              "      <td>0.170032</td>\n",
              "      <td>0.167941</td>\n",
              "      <td>0.151936</td>\n",
              "      <td>0.134034</td>\n",
              "      <td>0.153640</td>\n",
              "      <td>0.190827</td>\n",
              "      <td>0.194749</td>\n",
              "      <td>0.152425</td>\n",
              "      <td>0.149060</td>\n",
              "      <td>0.153495</td>\n",
              "      <td>0.136360</td>\n",
              "      <td>0.136040</td>\n",
              "      <td>0.139961</td>\n",
              "      <td>0.159887</td>\n",
              "      <td>0.168345</td>\n",
              "      <td>0.177040</td>\n",
              "      <td>0.173608</td>\n",
              "      <td>0.176484</td>\n",
              "      <td>0.180514</td>\n",
              "      <td>0.180835</td>\n",
              "      <td>0.165428</td>\n",
              "      <td>0.183928</td>\n",
              "      <td>0.187596</td>\n",
              "      <td>0.128838</td>\n",
              "      <td>0.082098</td>\n",
              "      <td>0.148317</td>\n",
              "      <td>0.216452</td>\n",
              "      <td>0.227001</td>\n",
              "      <td>...</td>\n",
              "      <td>0.181668</td>\n",
              "      <td>0.170140</td>\n",
              "      <td>0.110422</td>\n",
              "      <td>0.105625</td>\n",
              "      <td>0.102598</td>\n",
              "      <td>0.090598</td>\n",
              "      <td>0.116258</td>\n",
              "      <td>0.161802</td>\n",
              "      <td>0.164273</td>\n",
              "      <td>0.133351</td>\n",
              "      <td>0.083161</td>\n",
              "      <td>0.062105</td>\n",
              "      <td>0.054262</td>\n",
              "      <td>0.042499</td>\n",
              "      <td>0.038131</td>\n",
              "      <td>0.238740</td>\n",
              "      <td>0.063235</td>\n",
              "      <td>0.044523</td>\n",
              "      <td>0.044523</td>\n",
              "      <td>0.036680</td>\n",
              "      <td>0.031351</td>\n",
              "      <td>0.026747</td>\n",
              "      <td>0.039472</td>\n",
              "      <td>0.039792</td>\n",
              "      <td>0.042819</td>\n",
              "      <td>0.046740</td>\n",
              "      <td>0.054136</td>\n",
              "      <td>0.075149</td>\n",
              "      <td>0.077941</td>\n",
              "      <td>0.074914</td>\n",
              "      <td>0.074956</td>\n",
              "      <td>0.067071</td>\n",
              "      <td>0.057374</td>\n",
              "      <td>0.054583</td>\n",
              "      <td>0.062425</td>\n",
              "      <td>0.074956</td>\n",
              "      <td>0.077983</td>\n",
              "      <td>0.071035</td>\n",
              "      <td>0.061295</td>\n",
              "      <td>0.062425</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 2500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2     ...      2497      2498      2499\n",
              "3110  0.255354  0.265752  0.273915  ...  0.302095  0.270659  0.187903\n",
              "2825  0.999900  0.999900  0.999900  ...  0.999900  0.999900  0.999900\n",
              "1312  0.113872  0.071035  0.054583  ...  0.071035  0.061295  0.062425\n",
              "\n",
              "[3 rows x 2500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 216
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "id": "WFREk3P1miXa",
        "outputId": "d9066071-50f2-428d-dd48-55b048006050"
      },
      "source": [
        "test_data.tail(3)"
      ],
      "execution_count": 217,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>2460</th>\n",
              "      <th>2461</th>\n",
              "      <th>2462</th>\n",
              "      <th>2463</th>\n",
              "      <th>2464</th>\n",
              "      <th>2465</th>\n",
              "      <th>2466</th>\n",
              "      <th>2467</th>\n",
              "      <th>2468</th>\n",
              "      <th>2469</th>\n",
              "      <th>2470</th>\n",
              "      <th>2471</th>\n",
              "      <th>2472</th>\n",
              "      <th>2473</th>\n",
              "      <th>2474</th>\n",
              "      <th>2475</th>\n",
              "      <th>2476</th>\n",
              "      <th>2477</th>\n",
              "      <th>2478</th>\n",
              "      <th>2479</th>\n",
              "      <th>2480</th>\n",
              "      <th>2481</th>\n",
              "      <th>2482</th>\n",
              "      <th>2483</th>\n",
              "      <th>2484</th>\n",
              "      <th>2485</th>\n",
              "      <th>2486</th>\n",
              "      <th>2487</th>\n",
              "      <th>2488</th>\n",
              "      <th>2489</th>\n",
              "      <th>2490</th>\n",
              "      <th>2491</th>\n",
              "      <th>2492</th>\n",
              "      <th>2493</th>\n",
              "      <th>2494</th>\n",
              "      <th>2495</th>\n",
              "      <th>2496</th>\n",
              "      <th>2497</th>\n",
              "      <th>2498</th>\n",
              "      <th>2499</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2826</th>\n",
              "      <td>0.367591</td>\n",
              "      <td>0.416095</td>\n",
              "      <td>0.337284</td>\n",
              "      <td>0.314137</td>\n",
              "      <td>0.302652</td>\n",
              "      <td>0.317080</td>\n",
              "      <td>0.321001</td>\n",
              "      <td>0.318016</td>\n",
              "      <td>0.316760</td>\n",
              "      <td>0.312838</td>\n",
              "      <td>0.308917</td>\n",
              "      <td>0.308917</td>\n",
              "      <td>0.310047</td>\n",
              "      <td>0.311262</td>\n",
              "      <td>0.317080</td>\n",
              "      <td>0.321044</td>\n",
              "      <td>0.314246</td>\n",
              "      <td>0.311262</td>\n",
              "      <td>0.293233</td>\n",
              "      <td>0.285390</td>\n",
              "      <td>0.294640</td>\n",
              "      <td>0.290441</td>\n",
              "      <td>0.290719</td>\n",
              "      <td>0.295577</td>\n",
              "      <td>0.287735</td>\n",
              "      <td>0.285390</td>\n",
              "      <td>0.285390</td>\n",
              "      <td>0.281469</td>\n",
              "      <td>0.279892</td>\n",
              "      <td>0.278678</td>\n",
              "      <td>0.273627</td>\n",
              "      <td>0.272050</td>\n",
              "      <td>0.271156</td>\n",
              "      <td>0.278998</td>\n",
              "      <td>0.266105</td>\n",
              "      <td>0.254256</td>\n",
              "      <td>0.258413</td>\n",
              "      <td>0.258262</td>\n",
              "      <td>0.255471</td>\n",
              "      <td>0.255471</td>\n",
              "      <td>...</td>\n",
              "      <td>0.021975</td>\n",
              "      <td>0.099577</td>\n",
              "      <td>0.176025</td>\n",
              "      <td>0.270950</td>\n",
              "      <td>0.117261</td>\n",
              "      <td>0.144710</td>\n",
              "      <td>0.075301</td>\n",
              "      <td>0.235677</td>\n",
              "      <td>0.278448</td>\n",
              "      <td>0.095891</td>\n",
              "      <td>0.190907</td>\n",
              "      <td>0.143942</td>\n",
              "      <td>0.180194</td>\n",
              "      <td>0.254298</td>\n",
              "      <td>0.293963</td>\n",
              "      <td>0.454442</td>\n",
              "      <td>0.375867</td>\n",
              "      <td>0.427229</td>\n",
              "      <td>0.330040</td>\n",
              "      <td>0.314053</td>\n",
              "      <td>0.223340</td>\n",
              "      <td>0.138451</td>\n",
              "      <td>0.223358</td>\n",
              "      <td>0.240075</td>\n",
              "      <td>0.097233</td>\n",
              "      <td>0.203287</td>\n",
              "      <td>0.250655</td>\n",
              "      <td>0.257319</td>\n",
              "      <td>0.234101</td>\n",
              "      <td>0.032608</td>\n",
              "      <td>0.216874</td>\n",
              "      <td>0.114276</td>\n",
              "      <td>0.023020</td>\n",
              "      <td>0.009141</td>\n",
              "      <td>0.054371</td>\n",
              "      <td>0.066642</td>\n",
              "      <td>0.231080</td>\n",
              "      <td>0.271130</td>\n",
              "      <td>0.298815</td>\n",
              "      <td>0.412584</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>450</th>\n",
              "      <td>0.404765</td>\n",
              "      <td>0.636122</td>\n",
              "      <td>0.661993</td>\n",
              "      <td>0.605386</td>\n",
              "      <td>0.454670</td>\n",
              "      <td>0.247344</td>\n",
              "      <td>0.156505</td>\n",
              "      <td>0.129233</td>\n",
              "      <td>0.118836</td>\n",
              "      <td>0.110099</td>\n",
              "      <td>0.081521</td>\n",
              "      <td>0.078536</td>\n",
              "      <td>0.120557</td>\n",
              "      <td>0.216417</td>\n",
              "      <td>0.348604</td>\n",
              "      <td>0.455365</td>\n",
              "      <td>0.467582</td>\n",
              "      <td>0.447353</td>\n",
              "      <td>0.352337</td>\n",
              "      <td>0.209173</td>\n",
              "      <td>0.186086</td>\n",
              "      <td>0.161701</td>\n",
              "      <td>0.212458</td>\n",
              "      <td>0.271148</td>\n",
              "      <td>0.324239</td>\n",
              "      <td>0.380938</td>\n",
              "      <td>0.383658</td>\n",
              "      <td>0.396946</td>\n",
              "      <td>0.476833</td>\n",
              "      <td>0.512866</td>\n",
              "      <td>0.442634</td>\n",
              "      <td>0.289755</td>\n",
              "      <td>0.187917</td>\n",
              "      <td>0.308878</td>\n",
              "      <td>0.502418</td>\n",
              "      <td>0.620658</td>\n",
              "      <td>0.600225</td>\n",
              "      <td>0.418932</td>\n",
              "      <td>0.321325</td>\n",
              "      <td>0.346102</td>\n",
              "      <td>...</td>\n",
              "      <td>0.141788</td>\n",
              "      <td>0.145643</td>\n",
              "      <td>0.157255</td>\n",
              "      <td>0.185192</td>\n",
              "      <td>0.186980</td>\n",
              "      <td>0.172922</td>\n",
              "      <td>0.183737</td>\n",
              "      <td>0.216835</td>\n",
              "      <td>0.337707</td>\n",
              "      <td>0.455360</td>\n",
              "      <td>0.556544</td>\n",
              "      <td>0.592753</td>\n",
              "      <td>0.611398</td>\n",
              "      <td>0.605495</td>\n",
              "      <td>0.561038</td>\n",
              "      <td>0.643982</td>\n",
              "      <td>0.561165</td>\n",
              "      <td>0.737456</td>\n",
              "      <td>0.436226</td>\n",
              "      <td>0.524854</td>\n",
              "      <td>0.277361</td>\n",
              "      <td>0.256533</td>\n",
              "      <td>0.312940</td>\n",
              "      <td>0.271331</td>\n",
              "      <td>0.274214</td>\n",
              "      <td>0.275707</td>\n",
              "      <td>0.294334</td>\n",
              "      <td>0.342191</td>\n",
              "      <td>0.349477</td>\n",
              "      <td>0.450249</td>\n",
              "      <td>0.481913</td>\n",
              "      <td>0.312112</td>\n",
              "      <td>0.180106</td>\n",
              "      <td>0.141626</td>\n",
              "      <td>0.215216</td>\n",
              "      <td>0.463404</td>\n",
              "      <td>0.643407</td>\n",
              "      <td>0.729964</td>\n",
              "      <td>0.806459</td>\n",
              "      <td>0.776384</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1692</th>\n",
              "      <td>0.297575</td>\n",
              "      <td>0.281527</td>\n",
              "      <td>0.341729</td>\n",
              "      <td>0.411283</td>\n",
              "      <td>0.442671</td>\n",
              "      <td>0.457225</td>\n",
              "      <td>0.458186</td>\n",
              "      <td>0.474318</td>\n",
              "      <td>0.469225</td>\n",
              "      <td>0.480753</td>\n",
              "      <td>0.492970</td>\n",
              "      <td>0.568059</td>\n",
              "      <td>0.614208</td>\n",
              "      <td>0.567933</td>\n",
              "      <td>0.506141</td>\n",
              "      <td>0.480988</td>\n",
              "      <td>0.466814</td>\n",
              "      <td>0.348883</td>\n",
              "      <td>0.420467</td>\n",
              "      <td>0.498324</td>\n",
              "      <td>0.553273</td>\n",
              "      <td>0.449383</td>\n",
              "      <td>0.476831</td>\n",
              "      <td>0.497845</td>\n",
              "      <td>0.501488</td>\n",
              "      <td>0.523118</td>\n",
              "      <td>0.529831</td>\n",
              "      <td>0.553080</td>\n",
              "      <td>0.572964</td>\n",
              "      <td>0.528936</td>\n",
              "      <td>0.474360</td>\n",
              "      <td>0.462512</td>\n",
              "      <td>0.473804</td>\n",
              "      <td>0.485804</td>\n",
              "      <td>0.504896</td>\n",
              "      <td>0.524502</td>\n",
              "      <td>0.514382</td>\n",
              "      <td>0.490619</td>\n",
              "      <td>0.509331</td>\n",
              "      <td>0.557405</td>\n",
              "      <td>...</td>\n",
              "      <td>0.278440</td>\n",
              "      <td>0.330032</td>\n",
              "      <td>0.357329</td>\n",
              "      <td>0.363910</td>\n",
              "      <td>0.420335</td>\n",
              "      <td>0.314016</td>\n",
              "      <td>0.293473</td>\n",
              "      <td>0.352206</td>\n",
              "      <td>0.309792</td>\n",
              "      <td>0.215201</td>\n",
              "      <td>0.273318</td>\n",
              "      <td>0.304754</td>\n",
              "      <td>0.396198</td>\n",
              "      <td>0.387215</td>\n",
              "      <td>0.391540</td>\n",
              "      <td>0.418764</td>\n",
              "      <td>0.330305</td>\n",
              "      <td>0.271656</td>\n",
              "      <td>0.239984</td>\n",
              "      <td>0.262998</td>\n",
              "      <td>0.266955</td>\n",
              "      <td>0.223611</td>\n",
              "      <td>0.220143</td>\n",
              "      <td>0.289123</td>\n",
              "      <td>0.344382</td>\n",
              "      <td>0.455787</td>\n",
              "      <td>0.512895</td>\n",
              "      <td>0.450736</td>\n",
              "      <td>0.443323</td>\n",
              "      <td>0.414981</td>\n",
              "      <td>0.485616</td>\n",
              "      <td>0.497591</td>\n",
              "      <td>0.380385</td>\n",
              "      <td>0.297744</td>\n",
              "      <td>0.281008</td>\n",
              "      <td>0.239114</td>\n",
              "      <td>0.322038</td>\n",
              "      <td>0.281951</td>\n",
              "      <td>0.239343</td>\n",
              "      <td>0.239664</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>3 rows × 2500 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "          0         1         2     ...      2497      2498      2499\n",
              "2826  0.367591  0.416095  0.337284  ...  0.271130  0.298815  0.412584\n",
              "450   0.404765  0.636122  0.661993  ...  0.729964  0.806459  0.776384\n",
              "1692  0.297575  0.281527  0.341729  ...  0.281951  0.239343  0.239664\n",
              "\n",
              "[3 rows x 2500 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 217
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GN7t-Yp03Aq6"
      },
      "source": [
        "**Building the Network**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TA1kj1SirGOl"
      },
      "source": [
        "def build_model():\n",
        "  model=models.Sequential()\n",
        "  \n",
        "  model.add(layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(l2=.002), input_shape=(pic_size*pic_size,)))\n",
        "  model.add(layers.Dropout(.2))\n",
        "\n",
        "  model.add(layers.Dense(5, activation=\"softmax\"))\n",
        "  model.compile(optimizer=\"rmsprop\", loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "\n",
        "  return model"
      ],
      "execution_count": 220,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gJIIzSwg27Co"
      },
      "source": [
        "**K fold Validation**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xh-zIkt0RJZH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15964620-213a-442a-b46a-7012e1e3d4f8"
      },
      "source": [
        "k= 5\n",
        "num_val_sample = len(train_data) // k\n",
        "num_epochs =100\n",
        "all_scores = []\n",
        "all_val_loss_histories = []\n",
        "all_val_acc_histories = []\n",
        "all_loss_histories =[]\n",
        "all_acc_histories = []\n",
        "\n",
        "for i in range(k):\n",
        "  print(\"processing fold #\",i)\n",
        "  val_data = train_data[i*num_val_sample:(i+1)*num_val_sample]\n",
        "  val_labels = train_labels[i*num_val_sample: (i+1)*num_val_sample]\n",
        "\n",
        "  partial_train_data=np.concatenate([train_data[:i*num_val_sample], train_data[(i+1)*num_val_sample:]], axis=0)\n",
        "  partial_train_labels=np.concatenate([train_labels[:i*num_val_sample], train_labels[(i+1)*num_val_sample:]], axis=0)\n",
        "\n",
        "  model=build_model()\n",
        "  history = model.fit(partial_train_data, partial_train_labels, validation_data = (val_data, val_labels), epochs= num_epochs,batch_size=8, verbose=1)\n",
        "  val_loss, val_acc = model.evaluate(test_data, test_labels, verbose=0)\n",
        "  val_loss_history = history.history[\"val_loss\"]\n",
        "  val_acc_history = history.history[\"val_accuracy\"]\n",
        "  loss_history = history.history[\"loss\"]\n",
        "  acc_history = history.history[\"accuracy\"]\n",
        "  all_loss_histories.append(loss_history)\n",
        "  all_acc_histories.append(acc_history)\n",
        "  all_val_loss_histories.append(val_loss_history)\n",
        "  all_val_acc_histories.append(val_acc_history)\n",
        "  all_scores.append(val_acc)\n",
        "ave_val_loss_hist = [np.mean([x[i] for x in all_val_loss_histories]) for i in range(num_epochs)]\n",
        "ave_loss_hist = [np.mean([x[i] for x in all_loss_histories]) for i in range(num_epochs)]\n",
        "ave_val_acc_hist = [np.mean([x[i] for x in all_val_acc_histories]) for i in range(num_epochs)]\n",
        "ave_acc_hist = [np.mean([x[i] for x in all_acc_histories]) for i in range(num_epochs)]\n",
        "all_scores\n"
      ],
      "execution_count": 221,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "processing fold # 0\n",
            "Epoch 1/100\n",
            "254/254 [==============================] - 2s 5ms/step - loss: 2.0160 - accuracy: 0.2384 - val_loss: 1.6566 - val_accuracy: 0.2382\n",
            "Epoch 2/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6111 - accuracy: 0.2617 - val_loss: 1.6028 - val_accuracy: 0.2618\n",
            "Epoch 3/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6026 - accuracy: 0.2837 - val_loss: 1.6019 - val_accuracy: 0.2717\n",
            "Epoch 4/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6085 - accuracy: 0.2811 - val_loss: 1.5750 - val_accuracy: 0.2776\n",
            "Epoch 5/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6285 - accuracy: 0.2684 - val_loss: 1.6221 - val_accuracy: 0.1949\n",
            "Epoch 6/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5827 - accuracy: 0.2832 - val_loss: 1.6046 - val_accuracy: 0.2598\n",
            "Epoch 7/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5849 - accuracy: 0.2825 - val_loss: 1.5894 - val_accuracy: 0.2874\n",
            "Epoch 8/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5526 - accuracy: 0.3165 - val_loss: 1.6485 - val_accuracy: 0.2736\n",
            "Epoch 9/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5484 - accuracy: 0.2967 - val_loss: 1.5766 - val_accuracy: 0.2835\n",
            "Epoch 10/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5545 - accuracy: 0.3112 - val_loss: 1.6904 - val_accuracy: 0.2598\n",
            "Epoch 11/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5599 - accuracy: 0.2990 - val_loss: 1.5695 - val_accuracy: 0.2736\n",
            "Epoch 12/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5469 - accuracy: 0.2903 - val_loss: 1.6104 - val_accuracy: 0.2638\n",
            "Epoch 13/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5361 - accuracy: 0.2990 - val_loss: 1.6125 - val_accuracy: 0.2815\n",
            "Epoch 14/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5073 - accuracy: 0.3324 - val_loss: 1.5898 - val_accuracy: 0.2638\n",
            "Epoch 15/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5079 - accuracy: 0.3197 - val_loss: 1.5871 - val_accuracy: 0.2756\n",
            "Epoch 16/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5114 - accuracy: 0.2899 - val_loss: 1.6025 - val_accuracy: 0.2539\n",
            "Epoch 17/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4950 - accuracy: 0.3331 - val_loss: 1.6062 - val_accuracy: 0.2480\n",
            "Epoch 18/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4855 - accuracy: 0.3118 - val_loss: 1.6067 - val_accuracy: 0.2579\n",
            "Epoch 19/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4981 - accuracy: 0.3285 - val_loss: 1.6888 - val_accuracy: 0.2953\n",
            "Epoch 20/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5116 - accuracy: 0.3181 - val_loss: 1.6158 - val_accuracy: 0.2795\n",
            "Epoch 21/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4915 - accuracy: 0.3173 - val_loss: 1.5976 - val_accuracy: 0.2795\n",
            "Epoch 22/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4875 - accuracy: 0.3487 - val_loss: 1.7205 - val_accuracy: 0.2756\n",
            "Epoch 23/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4806 - accuracy: 0.3494 - val_loss: 1.6155 - val_accuracy: 0.2520\n",
            "Epoch 24/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4492 - accuracy: 0.3590 - val_loss: 1.6941 - val_accuracy: 0.2894\n",
            "Epoch 25/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4668 - accuracy: 0.3618 - val_loss: 1.7658 - val_accuracy: 0.2815\n",
            "Epoch 26/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4711 - accuracy: 0.3694 - val_loss: 1.6175 - val_accuracy: 0.2480\n",
            "Epoch 27/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4380 - accuracy: 0.3917 - val_loss: 1.6609 - val_accuracy: 0.2697\n",
            "Epoch 28/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4432 - accuracy: 0.3818 - val_loss: 1.8366 - val_accuracy: 0.2441\n",
            "Epoch 29/100\n",
            "254/254 [==============================] - 1s 3ms/step - loss: 1.4519 - accuracy: 0.3570 - val_loss: 1.6478 - val_accuracy: 0.2717\n",
            "Epoch 30/100\n",
            "254/254 [==============================] - 1s 3ms/step - loss: 1.4328 - accuracy: 0.3718 - val_loss: 1.7133 - val_accuracy: 0.2657\n",
            "Epoch 31/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4314 - accuracy: 0.3889 - val_loss: 1.6490 - val_accuracy: 0.2736\n",
            "Epoch 32/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4423 - accuracy: 0.3942 - val_loss: 1.6807 - val_accuracy: 0.2598\n",
            "Epoch 33/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4072 - accuracy: 0.4015 - val_loss: 1.6449 - val_accuracy: 0.2480\n",
            "Epoch 34/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4435 - accuracy: 0.3878 - val_loss: 1.7198 - val_accuracy: 0.2933\n",
            "Epoch 35/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3978 - accuracy: 0.4191 - val_loss: 1.7572 - val_accuracy: 0.2795\n",
            "Epoch 36/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4316 - accuracy: 0.3812 - val_loss: 1.6558 - val_accuracy: 0.2638\n",
            "Epoch 37/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4146 - accuracy: 0.4116 - val_loss: 1.8507 - val_accuracy: 0.2992\n",
            "Epoch 38/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4240 - accuracy: 0.3984 - val_loss: 1.8265 - val_accuracy: 0.2441\n",
            "Epoch 39/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4004 - accuracy: 0.3985 - val_loss: 1.8954 - val_accuracy: 0.2500\n",
            "Epoch 40/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4267 - accuracy: 0.4252 - val_loss: 1.6960 - val_accuracy: 0.2638\n",
            "Epoch 41/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4012 - accuracy: 0.4225 - val_loss: 1.7875 - val_accuracy: 0.2421\n",
            "Epoch 42/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4204 - accuracy: 0.4103 - val_loss: 1.7056 - val_accuracy: 0.2520\n",
            "Epoch 43/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4001 - accuracy: 0.4033 - val_loss: 2.0175 - val_accuracy: 0.2480\n",
            "Epoch 44/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4163 - accuracy: 0.4006 - val_loss: 1.9014 - val_accuracy: 0.2835\n",
            "Epoch 45/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3824 - accuracy: 0.4475 - val_loss: 1.8005 - val_accuracy: 0.2638\n",
            "Epoch 46/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4003 - accuracy: 0.4313 - val_loss: 2.2823 - val_accuracy: 0.2756\n",
            "Epoch 47/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4073 - accuracy: 0.4204 - val_loss: 1.7074 - val_accuracy: 0.2539\n",
            "Epoch 48/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3938 - accuracy: 0.4057 - val_loss: 1.9546 - val_accuracy: 0.2638\n",
            "Epoch 49/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3859 - accuracy: 0.4279 - val_loss: 1.7187 - val_accuracy: 0.2539\n",
            "Epoch 50/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3809 - accuracy: 0.4371 - val_loss: 1.9463 - val_accuracy: 0.2559\n",
            "Epoch 51/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3750 - accuracy: 0.4350 - val_loss: 1.7624 - val_accuracy: 0.2835\n",
            "Epoch 52/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4001 - accuracy: 0.4283 - val_loss: 1.9581 - val_accuracy: 0.2677\n",
            "Epoch 53/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3648 - accuracy: 0.4296 - val_loss: 1.9057 - val_accuracy: 0.2776\n",
            "Epoch 54/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3651 - accuracy: 0.4503 - val_loss: 2.4278 - val_accuracy: 0.2283\n",
            "Epoch 55/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3718 - accuracy: 0.4415 - val_loss: 1.8605 - val_accuracy: 0.2618\n",
            "Epoch 56/100\n",
            "254/254 [==============================] - 1s 3ms/step - loss: 1.3610 - accuracy: 0.4580 - val_loss: 2.1574 - val_accuracy: 0.2717\n",
            "Epoch 57/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3981 - accuracy: 0.4415 - val_loss: 1.9253 - val_accuracy: 0.2697\n",
            "Epoch 58/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3971 - accuracy: 0.4342 - val_loss: 1.9003 - val_accuracy: 0.2894\n",
            "Epoch 59/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3800 - accuracy: 0.4476 - val_loss: 2.0401 - val_accuracy: 0.2874\n",
            "Epoch 60/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3840 - accuracy: 0.4372 - val_loss: 1.9778 - val_accuracy: 0.2559\n",
            "Epoch 61/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3743 - accuracy: 0.4457 - val_loss: 2.1276 - val_accuracy: 0.2776\n",
            "Epoch 62/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3767 - accuracy: 0.4292 - val_loss: 1.8302 - val_accuracy: 0.2283\n",
            "Epoch 63/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3425 - accuracy: 0.4452 - val_loss: 1.7636 - val_accuracy: 0.2480\n",
            "Epoch 64/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3596 - accuracy: 0.4424 - val_loss: 1.8418 - val_accuracy: 0.2480\n",
            "Epoch 65/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3596 - accuracy: 0.4413 - val_loss: 1.8652 - val_accuracy: 0.2559\n",
            "Epoch 66/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3394 - accuracy: 0.4642 - val_loss: 1.8430 - val_accuracy: 0.2579\n",
            "Epoch 67/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3647 - accuracy: 0.4378 - val_loss: 1.8009 - val_accuracy: 0.2421\n",
            "Epoch 68/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3580 - accuracy: 0.4530 - val_loss: 2.0378 - val_accuracy: 0.2402\n",
            "Epoch 69/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3700 - accuracy: 0.4536 - val_loss: 1.7799 - val_accuracy: 0.2067\n",
            "Epoch 70/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3505 - accuracy: 0.4458 - val_loss: 2.0438 - val_accuracy: 0.2343\n",
            "Epoch 71/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3488 - accuracy: 0.4649 - val_loss: 1.9710 - val_accuracy: 0.2520\n",
            "Epoch 72/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3237 - accuracy: 0.4838 - val_loss: 1.9188 - val_accuracy: 0.2539\n",
            "Epoch 73/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3844 - accuracy: 0.4223 - val_loss: 1.8874 - val_accuracy: 0.2520\n",
            "Epoch 74/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3036 - accuracy: 0.4948 - val_loss: 1.9310 - val_accuracy: 0.2323\n",
            "Epoch 75/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3324 - accuracy: 0.4784 - val_loss: 1.8563 - val_accuracy: 0.2618\n",
            "Epoch 76/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3145 - accuracy: 0.4865 - val_loss: 1.9949 - val_accuracy: 0.2776\n",
            "Epoch 77/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3170 - accuracy: 0.4803 - val_loss: 2.1814 - val_accuracy: 0.2776\n",
            "Epoch 78/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3594 - accuracy: 0.4539 - val_loss: 1.8485 - val_accuracy: 0.2362\n",
            "Epoch 79/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3185 - accuracy: 0.4886 - val_loss: 2.1509 - val_accuracy: 0.2461\n",
            "Epoch 80/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3585 - accuracy: 0.4582 - val_loss: 2.4623 - val_accuracy: 0.2697\n",
            "Epoch 81/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3205 - accuracy: 0.4922 - val_loss: 2.0633 - val_accuracy: 0.2638\n",
            "Epoch 82/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.2983 - accuracy: 0.4889 - val_loss: 1.9954 - val_accuracy: 0.2697\n",
            "Epoch 83/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3303 - accuracy: 0.4780 - val_loss: 1.9773 - val_accuracy: 0.2441\n",
            "Epoch 84/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3546 - accuracy: 0.4623 - val_loss: 2.0708 - val_accuracy: 0.2500\n",
            "Epoch 85/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.2967 - accuracy: 0.4908 - val_loss: 1.9170 - val_accuracy: 0.2657\n",
            "Epoch 86/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3316 - accuracy: 0.4890 - val_loss: 1.9176 - val_accuracy: 0.2776\n",
            "Epoch 87/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.2943 - accuracy: 0.4997 - val_loss: 1.9869 - val_accuracy: 0.2835\n",
            "Epoch 88/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3089 - accuracy: 0.4955 - val_loss: 1.9987 - val_accuracy: 0.2539\n",
            "Epoch 89/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3193 - accuracy: 0.4801 - val_loss: 1.8383 - val_accuracy: 0.2343\n",
            "Epoch 90/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3103 - accuracy: 0.4903 - val_loss: 1.8961 - val_accuracy: 0.2402\n",
            "Epoch 91/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3458 - accuracy: 0.4851 - val_loss: 2.1600 - val_accuracy: 0.2500\n",
            "Epoch 92/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3278 - accuracy: 0.4864 - val_loss: 2.2512 - val_accuracy: 0.2657\n",
            "Epoch 93/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3567 - accuracy: 0.4710 - val_loss: 1.9855 - val_accuracy: 0.2520\n",
            "Epoch 94/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3166 - accuracy: 0.4945 - val_loss: 2.0922 - val_accuracy: 0.2697\n",
            "Epoch 95/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3141 - accuracy: 0.4882 - val_loss: 2.2549 - val_accuracy: 0.2677\n",
            "Epoch 96/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.2949 - accuracy: 0.4835 - val_loss: 1.9083 - val_accuracy: 0.2677\n",
            "Epoch 97/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.2982 - accuracy: 0.5016 - val_loss: 1.9034 - val_accuracy: 0.2382\n",
            "Epoch 98/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3389 - accuracy: 0.4954 - val_loss: 1.9960 - val_accuracy: 0.2362\n",
            "Epoch 99/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3430 - accuracy: 0.4743 - val_loss: 2.7281 - val_accuracy: 0.2618\n",
            "Epoch 100/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3426 - accuracy: 0.4899 - val_loss: 1.9393 - val_accuracy: 0.2303\n",
            "processing fold # 1\n",
            "Epoch 1/100\n",
            "254/254 [==============================] - 2s 5ms/step - loss: 1.9686 - accuracy: 0.2330 - val_loss: 1.6894 - val_accuracy: 0.2303\n",
            "Epoch 2/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6234 - accuracy: 0.2476 - val_loss: 1.5968 - val_accuracy: 0.2972\n",
            "Epoch 3/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6265 - accuracy: 0.2480 - val_loss: 1.9945 - val_accuracy: 0.2441\n",
            "Epoch 4/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6195 - accuracy: 0.2569 - val_loss: 1.5607 - val_accuracy: 0.3071\n",
            "Epoch 5/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5970 - accuracy: 0.2718 - val_loss: 1.5353 - val_accuracy: 0.3189\n",
            "Epoch 6/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5767 - accuracy: 0.2776 - val_loss: 1.5725 - val_accuracy: 0.2598\n",
            "Epoch 7/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5680 - accuracy: 0.2723 - val_loss: 1.5586 - val_accuracy: 0.2894\n",
            "Epoch 8/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5886 - accuracy: 0.2705 - val_loss: 1.5574 - val_accuracy: 0.3031\n",
            "Epoch 9/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5674 - accuracy: 0.2936 - val_loss: 1.7451 - val_accuracy: 0.1909\n",
            "Epoch 10/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5584 - accuracy: 0.3021 - val_loss: 1.5387 - val_accuracy: 0.3091\n",
            "Epoch 11/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5806 - accuracy: 0.2720 - val_loss: 1.7386 - val_accuracy: 0.2815\n",
            "Epoch 12/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5613 - accuracy: 0.2867 - val_loss: 1.5868 - val_accuracy: 0.2815\n",
            "Epoch 13/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5255 - accuracy: 0.3197 - val_loss: 1.6411 - val_accuracy: 0.2106\n",
            "Epoch 14/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5143 - accuracy: 0.3439 - val_loss: 1.5419 - val_accuracy: 0.3307\n",
            "Epoch 15/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5083 - accuracy: 0.3475 - val_loss: 1.5848 - val_accuracy: 0.3209\n",
            "Epoch 16/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5239 - accuracy: 0.3304 - val_loss: 1.5352 - val_accuracy: 0.3287\n",
            "Epoch 17/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4990 - accuracy: 0.3186 - val_loss: 1.5664 - val_accuracy: 0.2736\n",
            "Epoch 18/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5138 - accuracy: 0.3304 - val_loss: 1.5689 - val_accuracy: 0.2756\n",
            "Epoch 19/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5001 - accuracy: 0.3286 - val_loss: 1.7101 - val_accuracy: 0.2598\n",
            "Epoch 20/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5074 - accuracy: 0.3475 - val_loss: 1.5552 - val_accuracy: 0.3091\n",
            "Epoch 21/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5100 - accuracy: 0.3209 - val_loss: 1.5866 - val_accuracy: 0.2894\n",
            "Epoch 22/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5037 - accuracy: 0.3494 - val_loss: 1.5874 - val_accuracy: 0.3189\n",
            "Epoch 23/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5112 - accuracy: 0.3564 - val_loss: 1.5464 - val_accuracy: 0.3248\n",
            "Epoch 24/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4689 - accuracy: 0.3650 - val_loss: 1.5618 - val_accuracy: 0.3130\n",
            "Epoch 25/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4903 - accuracy: 0.3330 - val_loss: 1.5722 - val_accuracy: 0.2815\n",
            "Epoch 26/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4812 - accuracy: 0.3393 - val_loss: 1.5955 - val_accuracy: 0.3268\n",
            "Epoch 27/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4615 - accuracy: 0.3660 - val_loss: 1.5927 - val_accuracy: 0.3169\n",
            "Epoch 28/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4847 - accuracy: 0.3564 - val_loss: 1.5908 - val_accuracy: 0.2953\n",
            "Epoch 29/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4494 - accuracy: 0.3896 - val_loss: 1.6023 - val_accuracy: 0.2854\n",
            "Epoch 30/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4987 - accuracy: 0.3474 - val_loss: 1.5990 - val_accuracy: 0.3130\n",
            "Epoch 31/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4739 - accuracy: 0.3532 - val_loss: 1.6606 - val_accuracy: 0.2913\n",
            "Epoch 32/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4637 - accuracy: 0.3663 - val_loss: 1.6153 - val_accuracy: 0.3130\n",
            "Epoch 33/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4727 - accuracy: 0.3535 - val_loss: 1.6119 - val_accuracy: 0.3228\n",
            "Epoch 34/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4547 - accuracy: 0.3866 - val_loss: 1.6330 - val_accuracy: 0.3465\n",
            "Epoch 35/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4480 - accuracy: 0.3744 - val_loss: 1.6211 - val_accuracy: 0.2972\n",
            "Epoch 36/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4421 - accuracy: 0.3882 - val_loss: 1.6165 - val_accuracy: 0.3012\n",
            "Epoch 37/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4573 - accuracy: 0.3856 - val_loss: 1.6181 - val_accuracy: 0.3228\n",
            "Epoch 38/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4340 - accuracy: 0.3924 - val_loss: 1.7598 - val_accuracy: 0.2815\n",
            "Epoch 39/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4317 - accuracy: 0.3847 - val_loss: 1.6232 - val_accuracy: 0.3051\n",
            "Epoch 40/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4381 - accuracy: 0.3914 - val_loss: 1.6093 - val_accuracy: 0.2953\n",
            "Epoch 41/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4266 - accuracy: 0.4171 - val_loss: 1.7670 - val_accuracy: 0.3110\n",
            "Epoch 42/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4478 - accuracy: 0.4011 - val_loss: 1.6114 - val_accuracy: 0.2953\n",
            "Epoch 43/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4447 - accuracy: 0.3953 - val_loss: 1.6217 - val_accuracy: 0.3248\n",
            "Epoch 44/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4324 - accuracy: 0.3979 - val_loss: 1.7311 - val_accuracy: 0.2697\n",
            "Epoch 45/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4249 - accuracy: 0.4069 - val_loss: 1.6229 - val_accuracy: 0.3406\n",
            "Epoch 46/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3947 - accuracy: 0.4290 - val_loss: 1.8032 - val_accuracy: 0.2953\n",
            "Epoch 47/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4148 - accuracy: 0.4100 - val_loss: 2.0951 - val_accuracy: 0.2461\n",
            "Epoch 48/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4157 - accuracy: 0.4100 - val_loss: 1.6795 - val_accuracy: 0.3189\n",
            "Epoch 49/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4143 - accuracy: 0.4161 - val_loss: 1.8856 - val_accuracy: 0.2638\n",
            "Epoch 50/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4066 - accuracy: 0.4084 - val_loss: 1.6624 - val_accuracy: 0.3189\n",
            "Epoch 51/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3838 - accuracy: 0.4316 - val_loss: 1.6872 - val_accuracy: 0.2795\n",
            "Epoch 52/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4278 - accuracy: 0.4104 - val_loss: 1.6277 - val_accuracy: 0.2815\n",
            "Epoch 53/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3885 - accuracy: 0.4376 - val_loss: 1.6506 - val_accuracy: 0.3091\n",
            "Epoch 54/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4143 - accuracy: 0.4216 - val_loss: 1.7803 - val_accuracy: 0.3130\n",
            "Epoch 55/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4132 - accuracy: 0.4017 - val_loss: 1.7329 - val_accuracy: 0.3110\n",
            "Epoch 56/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3726 - accuracy: 0.4524 - val_loss: 2.0046 - val_accuracy: 0.2343\n",
            "Epoch 57/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3642 - accuracy: 0.4393 - val_loss: 1.6731 - val_accuracy: 0.3091\n",
            "Epoch 58/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4040 - accuracy: 0.4303 - val_loss: 1.6929 - val_accuracy: 0.3307\n",
            "Epoch 59/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4184 - accuracy: 0.4092 - val_loss: 1.7290 - val_accuracy: 0.3130\n",
            "Epoch 60/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4000 - accuracy: 0.4355 - val_loss: 1.6684 - val_accuracy: 0.2874\n",
            "Epoch 61/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4278 - accuracy: 0.4354 - val_loss: 1.7887 - val_accuracy: 0.3091\n",
            "Epoch 62/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3969 - accuracy: 0.4324 - val_loss: 1.7128 - val_accuracy: 0.3287\n",
            "Epoch 63/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3638 - accuracy: 0.4536 - val_loss: 1.7687 - val_accuracy: 0.3327\n",
            "Epoch 64/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3683 - accuracy: 0.4314 - val_loss: 1.6825 - val_accuracy: 0.3091\n",
            "Epoch 65/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4220 - accuracy: 0.4376 - val_loss: 1.6921 - val_accuracy: 0.2913\n",
            "Epoch 66/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3665 - accuracy: 0.4646 - val_loss: 1.8957 - val_accuracy: 0.3287\n",
            "Epoch 67/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3933 - accuracy: 0.4513 - val_loss: 1.6821 - val_accuracy: 0.3091\n",
            "Epoch 68/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3932 - accuracy: 0.4668 - val_loss: 1.8854 - val_accuracy: 0.3209\n",
            "Epoch 69/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3884 - accuracy: 0.4322 - val_loss: 1.7327 - val_accuracy: 0.2933\n",
            "Epoch 70/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3872 - accuracy: 0.4554 - val_loss: 2.4060 - val_accuracy: 0.2539\n",
            "Epoch 71/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3878 - accuracy: 0.4428 - val_loss: 1.7340 - val_accuracy: 0.3169\n",
            "Epoch 72/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3389 - accuracy: 0.4675 - val_loss: 2.0235 - val_accuracy: 0.2756\n",
            "Epoch 73/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3490 - accuracy: 0.4657 - val_loss: 2.0193 - val_accuracy: 0.3209\n",
            "Epoch 74/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3824 - accuracy: 0.4643 - val_loss: 2.1369 - val_accuracy: 0.3268\n",
            "Epoch 75/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3459 - accuracy: 0.4777 - val_loss: 1.8348 - val_accuracy: 0.3071\n",
            "Epoch 76/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3855 - accuracy: 0.4625 - val_loss: 1.7586 - val_accuracy: 0.3110\n",
            "Epoch 77/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3470 - accuracy: 0.4730 - val_loss: 1.7071 - val_accuracy: 0.3071\n",
            "Epoch 78/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3814 - accuracy: 0.4490 - val_loss: 1.7254 - val_accuracy: 0.3031\n",
            "Epoch 79/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3607 - accuracy: 0.4656 - val_loss: 1.7544 - val_accuracy: 0.3031\n",
            "Epoch 80/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3650 - accuracy: 0.4568 - val_loss: 1.7532 - val_accuracy: 0.3012\n",
            "Epoch 81/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3753 - accuracy: 0.4328 - val_loss: 2.2262 - val_accuracy: 0.2697\n",
            "Epoch 82/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3778 - accuracy: 0.4633 - val_loss: 1.8072 - val_accuracy: 0.2913\n",
            "Epoch 83/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3634 - accuracy: 0.4527 - val_loss: 1.9521 - val_accuracy: 0.2894\n",
            "Epoch 84/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3433 - accuracy: 0.4748 - val_loss: 1.8463 - val_accuracy: 0.2795\n",
            "Epoch 85/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3679 - accuracy: 0.4550 - val_loss: 1.7563 - val_accuracy: 0.3091\n",
            "Epoch 86/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3323 - accuracy: 0.4742 - val_loss: 1.9317 - val_accuracy: 0.2756\n",
            "Epoch 87/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3549 - accuracy: 0.4645 - val_loss: 1.7490 - val_accuracy: 0.2933\n",
            "Epoch 88/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3525 - accuracy: 0.4753 - val_loss: 1.9014 - val_accuracy: 0.3169\n",
            "Epoch 89/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3429 - accuracy: 0.4913 - val_loss: 2.1897 - val_accuracy: 0.3091\n",
            "Epoch 90/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4162 - accuracy: 0.4385 - val_loss: 2.1709 - val_accuracy: 0.2894\n",
            "Epoch 91/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3384 - accuracy: 0.4686 - val_loss: 2.2644 - val_accuracy: 0.3169\n",
            "Epoch 92/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3477 - accuracy: 0.4746 - val_loss: 1.7585 - val_accuracy: 0.3031\n",
            "Epoch 93/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3372 - accuracy: 0.4685 - val_loss: 2.0170 - val_accuracy: 0.3169\n",
            "Epoch 94/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3754 - accuracy: 0.4845 - val_loss: 1.8663 - val_accuracy: 0.3110\n",
            "Epoch 95/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3338 - accuracy: 0.4675 - val_loss: 1.9118 - val_accuracy: 0.3091\n",
            "Epoch 96/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3474 - accuracy: 0.4828 - val_loss: 1.8394 - val_accuracy: 0.3189\n",
            "Epoch 97/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3309 - accuracy: 0.4739 - val_loss: 1.7530 - val_accuracy: 0.3012\n",
            "Epoch 98/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3651 - accuracy: 0.5006 - val_loss: 1.8711 - val_accuracy: 0.2598\n",
            "Epoch 99/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3423 - accuracy: 0.4779 - val_loss: 1.8534 - val_accuracy: 0.3031\n",
            "Epoch 100/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3479 - accuracy: 0.4619 - val_loss: 2.2864 - val_accuracy: 0.2835\n",
            "processing fold # 2\n",
            "Epoch 1/100\n",
            "254/254 [==============================] - 2s 5ms/step - loss: 2.1502 - accuracy: 0.2206 - val_loss: 1.6206 - val_accuracy: 0.2283\n",
            "Epoch 2/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6307 - accuracy: 0.2459 - val_loss: 1.7392 - val_accuracy: 0.2579\n",
            "Epoch 3/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6420 - accuracy: 0.2637 - val_loss: 1.5860 - val_accuracy: 0.2382\n",
            "Epoch 4/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6008 - accuracy: 0.2581 - val_loss: 1.5537 - val_accuracy: 0.2894\n",
            "Epoch 5/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5820 - accuracy: 0.2922 - val_loss: 1.5466 - val_accuracy: 0.2953\n",
            "Epoch 6/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5892 - accuracy: 0.2717 - val_loss: 1.6262 - val_accuracy: 0.2776\n",
            "Epoch 7/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5821 - accuracy: 0.2639 - val_loss: 1.5395 - val_accuracy: 0.3130\n",
            "Epoch 8/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5812 - accuracy: 0.2937 - val_loss: 1.5377 - val_accuracy: 0.3012\n",
            "Epoch 9/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5720 - accuracy: 0.3078 - val_loss: 1.5478 - val_accuracy: 0.2854\n",
            "Epoch 10/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5479 - accuracy: 0.3036 - val_loss: 1.5497 - val_accuracy: 0.2756\n",
            "Epoch 11/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5617 - accuracy: 0.3048 - val_loss: 1.5497 - val_accuracy: 0.2953\n",
            "Epoch 12/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5805 - accuracy: 0.3136 - val_loss: 1.5947 - val_accuracy: 0.2992\n",
            "Epoch 13/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5424 - accuracy: 0.3172 - val_loss: 1.5746 - val_accuracy: 0.2815\n",
            "Epoch 14/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5278 - accuracy: 0.3364 - val_loss: 1.5575 - val_accuracy: 0.2874\n",
            "Epoch 15/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5305 - accuracy: 0.3034 - val_loss: 1.5910 - val_accuracy: 0.2717\n",
            "Epoch 16/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5165 - accuracy: 0.3236 - val_loss: 1.5944 - val_accuracy: 0.2756\n",
            "Epoch 17/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5255 - accuracy: 0.3294 - val_loss: 1.5603 - val_accuracy: 0.2894\n",
            "Epoch 18/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5045 - accuracy: 0.3284 - val_loss: 1.6117 - val_accuracy: 0.3051\n",
            "Epoch 19/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5282 - accuracy: 0.3148 - val_loss: 1.5461 - val_accuracy: 0.2992\n",
            "Epoch 20/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5148 - accuracy: 0.3275 - val_loss: 1.5558 - val_accuracy: 0.2972\n",
            "Epoch 21/100\n",
            "254/254 [==============================] - 1s 5ms/step - loss: 1.4977 - accuracy: 0.3292 - val_loss: 1.5561 - val_accuracy: 0.2913\n",
            "Epoch 22/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5046 - accuracy: 0.3204 - val_loss: 1.5773 - val_accuracy: 0.2894\n",
            "Epoch 23/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4882 - accuracy: 0.3258 - val_loss: 1.5769 - val_accuracy: 0.2815\n",
            "Epoch 24/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4980 - accuracy: 0.3416 - val_loss: 1.5703 - val_accuracy: 0.2894\n",
            "Epoch 25/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4964 - accuracy: 0.3508 - val_loss: 1.6786 - val_accuracy: 0.2579\n",
            "Epoch 26/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4974 - accuracy: 0.3238 - val_loss: 1.5676 - val_accuracy: 0.2835\n",
            "Epoch 27/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4811 - accuracy: 0.3382 - val_loss: 1.5718 - val_accuracy: 0.2854\n",
            "Epoch 28/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4834 - accuracy: 0.3620 - val_loss: 1.6305 - val_accuracy: 0.2933\n",
            "Epoch 29/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5071 - accuracy: 0.3248 - val_loss: 1.5986 - val_accuracy: 0.2874\n",
            "Epoch 30/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4670 - accuracy: 0.3785 - val_loss: 1.8642 - val_accuracy: 0.2657\n",
            "Epoch 31/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4797 - accuracy: 0.3699 - val_loss: 1.5855 - val_accuracy: 0.2697\n",
            "Epoch 32/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4658 - accuracy: 0.3556 - val_loss: 1.6463 - val_accuracy: 0.2697\n",
            "Epoch 33/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4985 - accuracy: 0.3343 - val_loss: 1.6019 - val_accuracy: 0.2736\n",
            "Epoch 34/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4539 - accuracy: 0.3702 - val_loss: 1.5648 - val_accuracy: 0.2972\n",
            "Epoch 35/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4988 - accuracy: 0.3426 - val_loss: 1.7086 - val_accuracy: 0.2854\n",
            "Epoch 36/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4507 - accuracy: 0.3765 - val_loss: 1.6398 - val_accuracy: 0.2795\n",
            "Epoch 37/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4692 - accuracy: 0.3645 - val_loss: 1.5708 - val_accuracy: 0.2913\n",
            "Epoch 38/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4807 - accuracy: 0.3407 - val_loss: 1.6729 - val_accuracy: 0.3031\n",
            "Epoch 39/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4594 - accuracy: 0.3628 - val_loss: 1.7527 - val_accuracy: 0.2854\n",
            "Epoch 40/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4683 - accuracy: 0.3775 - val_loss: 1.6116 - val_accuracy: 0.2657\n",
            "Epoch 41/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4449 - accuracy: 0.3800 - val_loss: 1.5901 - val_accuracy: 0.3031\n",
            "Epoch 42/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4763 - accuracy: 0.3654 - val_loss: 1.6304 - val_accuracy: 0.2835\n",
            "Epoch 43/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4461 - accuracy: 0.3662 - val_loss: 1.7249 - val_accuracy: 0.2894\n",
            "Epoch 44/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4599 - accuracy: 0.3709 - val_loss: 1.8574 - val_accuracy: 0.2441\n",
            "Epoch 45/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4791 - accuracy: 0.3692 - val_loss: 1.6765 - val_accuracy: 0.2736\n",
            "Epoch 46/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4607 - accuracy: 0.3963 - val_loss: 1.6751 - val_accuracy: 0.2854\n",
            "Epoch 47/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4305 - accuracy: 0.4035 - val_loss: 1.6769 - val_accuracy: 0.3031\n",
            "Epoch 48/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4343 - accuracy: 0.3973 - val_loss: 2.4174 - val_accuracy: 0.2185\n",
            "Epoch 49/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4259 - accuracy: 0.3749 - val_loss: 1.5989 - val_accuracy: 0.3110\n",
            "Epoch 50/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4469 - accuracy: 0.3789 - val_loss: 1.6202 - val_accuracy: 0.2776\n",
            "Epoch 51/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4315 - accuracy: 0.3983 - val_loss: 1.8903 - val_accuracy: 0.2598\n",
            "Epoch 52/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4054 - accuracy: 0.4049 - val_loss: 1.7484 - val_accuracy: 0.2776\n",
            "Epoch 53/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4465 - accuracy: 0.3875 - val_loss: 1.6593 - val_accuracy: 0.3091\n",
            "Epoch 54/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4319 - accuracy: 0.3925 - val_loss: 1.6651 - val_accuracy: 0.2972\n",
            "Epoch 55/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4389 - accuracy: 0.3778 - val_loss: 1.6045 - val_accuracy: 0.2874\n",
            "Epoch 56/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4042 - accuracy: 0.4030 - val_loss: 1.6747 - val_accuracy: 0.2835\n",
            "Epoch 57/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4452 - accuracy: 0.4164 - val_loss: 1.7337 - val_accuracy: 0.2776\n",
            "Epoch 58/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4589 - accuracy: 0.3674 - val_loss: 1.6389 - val_accuracy: 0.2697\n",
            "Epoch 59/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4111 - accuracy: 0.4051 - val_loss: 1.6417 - val_accuracy: 0.2874\n",
            "Epoch 60/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4145 - accuracy: 0.4135 - val_loss: 1.7341 - val_accuracy: 0.2913\n",
            "Epoch 61/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3990 - accuracy: 0.4043 - val_loss: 1.7390 - val_accuracy: 0.2972\n",
            "Epoch 62/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4035 - accuracy: 0.4195 - val_loss: 1.7205 - val_accuracy: 0.2933\n",
            "Epoch 63/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4025 - accuracy: 0.4260 - val_loss: 1.6632 - val_accuracy: 0.2913\n",
            "Epoch 64/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4075 - accuracy: 0.4039 - val_loss: 1.6716 - val_accuracy: 0.3071\n",
            "Epoch 65/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4201 - accuracy: 0.3877 - val_loss: 1.6568 - val_accuracy: 0.2992\n",
            "Epoch 66/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4184 - accuracy: 0.4212 - val_loss: 1.8617 - val_accuracy: 0.2579\n",
            "Epoch 67/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4273 - accuracy: 0.4386 - val_loss: 1.6746 - val_accuracy: 0.2933\n",
            "Epoch 68/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4131 - accuracy: 0.4284 - val_loss: 1.8518 - val_accuracy: 0.2618\n",
            "Epoch 69/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3999 - accuracy: 0.4053 - val_loss: 1.6826 - val_accuracy: 0.3071\n",
            "Epoch 70/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4153 - accuracy: 0.4047 - val_loss: 1.7878 - val_accuracy: 0.2854\n",
            "Epoch 71/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4337 - accuracy: 0.3946 - val_loss: 1.6517 - val_accuracy: 0.2992\n",
            "Epoch 72/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4154 - accuracy: 0.4281 - val_loss: 2.1023 - val_accuracy: 0.2677\n",
            "Epoch 73/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4098 - accuracy: 0.4275 - val_loss: 1.7336 - val_accuracy: 0.2953\n",
            "Epoch 74/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4123 - accuracy: 0.4304 - val_loss: 1.8978 - val_accuracy: 0.3071\n",
            "Epoch 75/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4169 - accuracy: 0.4116 - val_loss: 1.6545 - val_accuracy: 0.3130\n",
            "Epoch 76/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4160 - accuracy: 0.4300 - val_loss: 1.7167 - val_accuracy: 0.2953\n",
            "Epoch 77/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3927 - accuracy: 0.4220 - val_loss: 1.7470 - val_accuracy: 0.2598\n",
            "Epoch 78/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3925 - accuracy: 0.4371 - val_loss: 1.6810 - val_accuracy: 0.3071\n",
            "Epoch 79/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4092 - accuracy: 0.4148 - val_loss: 1.7410 - val_accuracy: 0.3130\n",
            "Epoch 80/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3964 - accuracy: 0.4247 - val_loss: 1.8968 - val_accuracy: 0.3012\n",
            "Epoch 81/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3806 - accuracy: 0.4176 - val_loss: 1.6995 - val_accuracy: 0.3031\n",
            "Epoch 82/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3662 - accuracy: 0.4322 - val_loss: 2.1918 - val_accuracy: 0.2677\n",
            "Epoch 83/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3874 - accuracy: 0.4578 - val_loss: 1.7182 - val_accuracy: 0.2559\n",
            "Epoch 84/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3645 - accuracy: 0.4387 - val_loss: 1.8651 - val_accuracy: 0.2559\n",
            "Epoch 85/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4235 - accuracy: 0.4111 - val_loss: 1.7356 - val_accuracy: 0.2500\n",
            "Epoch 86/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4371 - accuracy: 0.4271 - val_loss: 1.7821 - val_accuracy: 0.2441\n",
            "Epoch 87/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4103 - accuracy: 0.4209 - val_loss: 1.8044 - val_accuracy: 0.3169\n",
            "Epoch 88/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3804 - accuracy: 0.4276 - val_loss: 1.6929 - val_accuracy: 0.3091\n",
            "Epoch 89/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4057 - accuracy: 0.4277 - val_loss: 1.7944 - val_accuracy: 0.2480\n",
            "Epoch 90/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4034 - accuracy: 0.4190 - val_loss: 1.6842 - val_accuracy: 0.2795\n",
            "Epoch 91/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4061 - accuracy: 0.4344 - val_loss: 1.9204 - val_accuracy: 0.2520\n",
            "Epoch 92/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3972 - accuracy: 0.4210 - val_loss: 2.5189 - val_accuracy: 0.2421\n",
            "Epoch 93/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3895 - accuracy: 0.4345 - val_loss: 1.7536 - val_accuracy: 0.2756\n",
            "Epoch 94/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3868 - accuracy: 0.4358 - val_loss: 1.7271 - val_accuracy: 0.2933\n",
            "Epoch 95/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3999 - accuracy: 0.4325 - val_loss: 1.8077 - val_accuracy: 0.2992\n",
            "Epoch 96/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4122 - accuracy: 0.4167 - val_loss: 1.9047 - val_accuracy: 0.2598\n",
            "Epoch 97/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3646 - accuracy: 0.4340 - val_loss: 1.8967 - val_accuracy: 0.2598\n",
            "Epoch 98/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4209 - accuracy: 0.4169 - val_loss: 2.0178 - val_accuracy: 0.2854\n",
            "Epoch 99/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3538 - accuracy: 0.4400 - val_loss: 1.7919 - val_accuracy: 0.3130\n",
            "Epoch 100/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3950 - accuracy: 0.4153 - val_loss: 1.8103 - val_accuracy: 0.2972\n",
            "processing fold # 3\n",
            "Epoch 1/100\n",
            "254/254 [==============================] - 2s 5ms/step - loss: 2.1335 - accuracy: 0.2217 - val_loss: 1.5722 - val_accuracy: 0.3110\n",
            "Epoch 2/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6531 - accuracy: 0.2312 - val_loss: 1.7541 - val_accuracy: 0.1535\n",
            "Epoch 3/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6259 - accuracy: 0.2476 - val_loss: 1.5532 - val_accuracy: 0.2264\n",
            "Epoch 4/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6217 - accuracy: 0.2758 - val_loss: 1.5300 - val_accuracy: 0.3189\n",
            "Epoch 5/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6223 - accuracy: 0.2610 - val_loss: 1.5544 - val_accuracy: 0.2933\n",
            "Epoch 6/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5897 - accuracy: 0.2482 - val_loss: 1.5410 - val_accuracy: 0.3287\n",
            "Epoch 7/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5797 - accuracy: 0.2990 - val_loss: 1.7021 - val_accuracy: 0.2598\n",
            "Epoch 8/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5928 - accuracy: 0.2959 - val_loss: 1.5762 - val_accuracy: 0.2933\n",
            "Epoch 9/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6152 - accuracy: 0.2708 - val_loss: 1.8022 - val_accuracy: 0.2835\n",
            "Epoch 10/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5851 - accuracy: 0.2604 - val_loss: 1.5241 - val_accuracy: 0.3524\n",
            "Epoch 11/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5386 - accuracy: 0.3209 - val_loss: 1.5230 - val_accuracy: 0.3543\n",
            "Epoch 12/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5346 - accuracy: 0.3170 - val_loss: 1.6258 - val_accuracy: 0.2362\n",
            "Epoch 13/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5590 - accuracy: 0.3127 - val_loss: 1.5411 - val_accuracy: 0.3150\n",
            "Epoch 14/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5144 - accuracy: 0.3084 - val_loss: 1.5372 - val_accuracy: 0.3346\n",
            "Epoch 15/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5241 - accuracy: 0.3121 - val_loss: 1.5286 - val_accuracy: 0.3287\n",
            "Epoch 16/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5278 - accuracy: 0.2961 - val_loss: 1.5524 - val_accuracy: 0.2894\n",
            "Epoch 17/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5244 - accuracy: 0.3325 - val_loss: 1.5421 - val_accuracy: 0.3268\n",
            "Epoch 18/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5231 - accuracy: 0.3041 - val_loss: 1.5318 - val_accuracy: 0.3406\n",
            "Epoch 19/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4963 - accuracy: 0.3414 - val_loss: 1.5501 - val_accuracy: 0.3406\n",
            "Epoch 20/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5124 - accuracy: 0.2952 - val_loss: 1.5525 - val_accuracy: 0.3228\n",
            "Epoch 21/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4939 - accuracy: 0.3322 - val_loss: 1.5423 - val_accuracy: 0.3209\n",
            "Epoch 22/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5006 - accuracy: 0.3463 - val_loss: 1.5515 - val_accuracy: 0.3150\n",
            "Epoch 23/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4983 - accuracy: 0.3281 - val_loss: 1.5536 - val_accuracy: 0.3031\n",
            "Epoch 24/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5094 - accuracy: 0.3242 - val_loss: 1.5449 - val_accuracy: 0.3268\n",
            "Epoch 25/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4785 - accuracy: 0.3721 - val_loss: 1.5549 - val_accuracy: 0.3189\n",
            "Epoch 26/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4808 - accuracy: 0.3411 - val_loss: 1.5785 - val_accuracy: 0.3150\n",
            "Epoch 27/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4807 - accuracy: 0.3392 - val_loss: 1.5622 - val_accuracy: 0.3268\n",
            "Epoch 28/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4680 - accuracy: 0.3610 - val_loss: 1.8089 - val_accuracy: 0.2992\n",
            "Epoch 29/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4710 - accuracy: 0.3646 - val_loss: 1.5756 - val_accuracy: 0.3406\n",
            "Epoch 30/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4711 - accuracy: 0.3380 - val_loss: 1.7004 - val_accuracy: 0.3110\n",
            "Epoch 31/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4975 - accuracy: 0.3627 - val_loss: 1.6500 - val_accuracy: 0.3110\n",
            "Epoch 32/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4729 - accuracy: 0.3506 - val_loss: 1.5761 - val_accuracy: 0.3425\n",
            "Epoch 33/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4602 - accuracy: 0.3722 - val_loss: 1.6231 - val_accuracy: 0.2854\n",
            "Epoch 34/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4866 - accuracy: 0.3437 - val_loss: 1.7830 - val_accuracy: 0.2756\n",
            "Epoch 35/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4605 - accuracy: 0.3695 - val_loss: 1.5944 - val_accuracy: 0.3130\n",
            "Epoch 36/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4737 - accuracy: 0.3472 - val_loss: 1.5723 - val_accuracy: 0.3307\n",
            "Epoch 37/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4538 - accuracy: 0.3734 - val_loss: 1.5896 - val_accuracy: 0.3209\n",
            "Epoch 38/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4629 - accuracy: 0.3580 - val_loss: 1.6236 - val_accuracy: 0.3307\n",
            "Epoch 39/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4224 - accuracy: 0.3694 - val_loss: 1.5924 - val_accuracy: 0.3327\n",
            "Epoch 40/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4534 - accuracy: 0.3539 - val_loss: 1.6110 - val_accuracy: 0.3287\n",
            "Epoch 41/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4160 - accuracy: 0.3995 - val_loss: 1.6537 - val_accuracy: 0.3091\n",
            "Epoch 42/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4378 - accuracy: 0.3812 - val_loss: 1.6097 - val_accuracy: 0.3031\n",
            "Epoch 43/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4669 - accuracy: 0.3762 - val_loss: 1.6555 - val_accuracy: 0.2854\n",
            "Epoch 44/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4150 - accuracy: 0.3826 - val_loss: 1.6371 - val_accuracy: 0.3425\n",
            "Epoch 45/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4408 - accuracy: 0.3835 - val_loss: 1.9249 - val_accuracy: 0.2717\n",
            "Epoch 46/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3829 - accuracy: 0.4150 - val_loss: 1.9209 - val_accuracy: 0.2894\n",
            "Epoch 47/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4292 - accuracy: 0.3983 - val_loss: 1.9475 - val_accuracy: 0.3091\n",
            "Epoch 48/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4439 - accuracy: 0.4090 - val_loss: 1.6135 - val_accuracy: 0.3189\n",
            "Epoch 49/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4305 - accuracy: 0.4247 - val_loss: 1.9879 - val_accuracy: 0.2874\n",
            "Epoch 50/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4729 - accuracy: 0.3616 - val_loss: 1.6868 - val_accuracy: 0.3268\n",
            "Epoch 51/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4135 - accuracy: 0.4050 - val_loss: 1.7317 - val_accuracy: 0.3071\n",
            "Epoch 52/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4073 - accuracy: 0.3978 - val_loss: 1.6970 - val_accuracy: 0.3150\n",
            "Epoch 53/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4283 - accuracy: 0.4016 - val_loss: 1.6385 - val_accuracy: 0.3071\n",
            "Epoch 54/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4182 - accuracy: 0.4053 - val_loss: 1.6372 - val_accuracy: 0.3012\n",
            "Epoch 55/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3971 - accuracy: 0.4275 - val_loss: 1.6320 - val_accuracy: 0.3051\n",
            "Epoch 56/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3664 - accuracy: 0.4126 - val_loss: 1.6876 - val_accuracy: 0.3268\n",
            "Epoch 57/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4242 - accuracy: 0.4012 - val_loss: 1.6326 - val_accuracy: 0.3150\n",
            "Epoch 58/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3941 - accuracy: 0.3979 - val_loss: 1.6518 - val_accuracy: 0.3228\n",
            "Epoch 59/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3980 - accuracy: 0.4019 - val_loss: 1.7592 - val_accuracy: 0.3209\n",
            "Epoch 60/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3870 - accuracy: 0.3690 - val_loss: 1.7901 - val_accuracy: 0.2500\n",
            "Epoch 61/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3932 - accuracy: 0.4157 - val_loss: 1.6840 - val_accuracy: 0.2913\n",
            "Epoch 62/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3715 - accuracy: 0.4187 - val_loss: 1.6661 - val_accuracy: 0.3209\n",
            "Epoch 63/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3763 - accuracy: 0.4231 - val_loss: 1.7551 - val_accuracy: 0.3248\n",
            "Epoch 64/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3964 - accuracy: 0.4031 - val_loss: 2.0963 - val_accuracy: 0.2736\n",
            "Epoch 65/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3840 - accuracy: 0.4017 - val_loss: 1.8869 - val_accuracy: 0.2657\n",
            "Epoch 66/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3826 - accuracy: 0.4386 - val_loss: 1.7962 - val_accuracy: 0.3031\n",
            "Epoch 67/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3683 - accuracy: 0.4289 - val_loss: 2.2186 - val_accuracy: 0.2618\n",
            "Epoch 68/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3626 - accuracy: 0.4496 - val_loss: 1.7417 - val_accuracy: 0.3051\n",
            "Epoch 69/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4088 - accuracy: 0.4004 - val_loss: 1.7332 - val_accuracy: 0.3012\n",
            "Epoch 70/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3890 - accuracy: 0.4173 - val_loss: 1.9263 - val_accuracy: 0.2697\n",
            "Epoch 71/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3878 - accuracy: 0.4205 - val_loss: 1.7414 - val_accuracy: 0.3110\n",
            "Epoch 72/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4087 - accuracy: 0.4057 - val_loss: 1.7064 - val_accuracy: 0.3150\n",
            "Epoch 73/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4114 - accuracy: 0.4080 - val_loss: 1.6687 - val_accuracy: 0.3150\n",
            "Epoch 74/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4050 - accuracy: 0.4279 - val_loss: 1.6855 - val_accuracy: 0.3228\n",
            "Epoch 75/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3952 - accuracy: 0.4349 - val_loss: 1.9034 - val_accuracy: 0.3169\n",
            "Epoch 76/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3898 - accuracy: 0.4113 - val_loss: 1.7451 - val_accuracy: 0.3228\n",
            "Epoch 77/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3819 - accuracy: 0.4450 - val_loss: 1.6977 - val_accuracy: 0.2933\n",
            "Epoch 78/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3640 - accuracy: 0.4394 - val_loss: 1.7506 - val_accuracy: 0.3012\n",
            "Epoch 79/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3584 - accuracy: 0.4320 - val_loss: 1.7314 - val_accuracy: 0.2953\n",
            "Epoch 80/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3922 - accuracy: 0.4160 - val_loss: 2.2817 - val_accuracy: 0.2461\n",
            "Epoch 81/100\n",
            "254/254 [==============================] - 1s 5ms/step - loss: 1.3754 - accuracy: 0.4350 - val_loss: 1.7634 - val_accuracy: 0.3189\n",
            "Epoch 82/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3369 - accuracy: 0.4348 - val_loss: 1.7100 - val_accuracy: 0.2992\n",
            "Epoch 83/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3839 - accuracy: 0.4309 - val_loss: 1.6847 - val_accuracy: 0.3051\n",
            "Epoch 84/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3806 - accuracy: 0.4251 - val_loss: 1.7482 - val_accuracy: 0.3209\n",
            "Epoch 85/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3696 - accuracy: 0.4668 - val_loss: 1.7659 - val_accuracy: 0.2835\n",
            "Epoch 86/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3418 - accuracy: 0.4550 - val_loss: 1.9723 - val_accuracy: 0.3366\n",
            "Epoch 87/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3632 - accuracy: 0.4474 - val_loss: 1.8101 - val_accuracy: 0.3051\n",
            "Epoch 88/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3716 - accuracy: 0.4394 - val_loss: 2.0711 - val_accuracy: 0.3307\n",
            "Epoch 89/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3545 - accuracy: 0.4395 - val_loss: 1.9747 - val_accuracy: 0.2618\n",
            "Epoch 90/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3459 - accuracy: 0.4515 - val_loss: 2.0748 - val_accuracy: 0.2913\n",
            "Epoch 91/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3402 - accuracy: 0.4509 - val_loss: 1.7741 - val_accuracy: 0.3110\n",
            "Epoch 92/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3441 - accuracy: 0.4469 - val_loss: 1.9906 - val_accuracy: 0.2953\n",
            "Epoch 93/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3700 - accuracy: 0.4486 - val_loss: 1.8075 - val_accuracy: 0.2913\n",
            "Epoch 94/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3291 - accuracy: 0.4547 - val_loss: 2.5336 - val_accuracy: 0.2264\n",
            "Epoch 95/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4171 - accuracy: 0.4274 - val_loss: 2.0920 - val_accuracy: 0.2677\n",
            "Epoch 96/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3493 - accuracy: 0.4813 - val_loss: 1.8876 - val_accuracy: 0.2894\n",
            "Epoch 97/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3942 - accuracy: 0.4375 - val_loss: 1.8223 - val_accuracy: 0.3209\n",
            "Epoch 98/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3672 - accuracy: 0.4510 - val_loss: 1.8547 - val_accuracy: 0.2795\n",
            "Epoch 99/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3660 - accuracy: 0.4385 - val_loss: 1.7573 - val_accuracy: 0.3051\n",
            "Epoch 100/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3624 - accuracy: 0.4322 - val_loss: 1.7590 - val_accuracy: 0.2756\n",
            "processing fold # 4\n",
            "Epoch 1/100\n",
            "254/254 [==============================] - 2s 5ms/step - loss: 2.0689 - accuracy: 0.2236 - val_loss: 1.6403 - val_accuracy: 0.3051\n",
            "Epoch 2/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6506 - accuracy: 0.2655 - val_loss: 1.6555 - val_accuracy: 0.2835\n",
            "Epoch 3/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6192 - accuracy: 0.2648 - val_loss: 1.6990 - val_accuracy: 0.2303\n",
            "Epoch 4/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6187 - accuracy: 0.2601 - val_loss: 1.5516 - val_accuracy: 0.2874\n",
            "Epoch 5/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6192 - accuracy: 0.2683 - val_loss: 1.5618 - val_accuracy: 0.2736\n",
            "Epoch 6/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.6152 - accuracy: 0.2843 - val_loss: 1.6515 - val_accuracy: 0.2421\n",
            "Epoch 7/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5818 - accuracy: 0.2967 - val_loss: 1.5463 - val_accuracy: 0.2717\n",
            "Epoch 8/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5801 - accuracy: 0.2962 - val_loss: 1.5486 - val_accuracy: 0.2835\n",
            "Epoch 9/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5702 - accuracy: 0.2709 - val_loss: 1.5549 - val_accuracy: 0.2598\n",
            "Epoch 10/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5932 - accuracy: 0.2747 - val_loss: 1.6324 - val_accuracy: 0.2756\n",
            "Epoch 11/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5826 - accuracy: 0.2967 - val_loss: 1.5496 - val_accuracy: 0.3110\n",
            "Epoch 12/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5283 - accuracy: 0.3083 - val_loss: 1.6994 - val_accuracy: 0.2323\n",
            "Epoch 13/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5549 - accuracy: 0.2918 - val_loss: 1.5437 - val_accuracy: 0.3425\n",
            "Epoch 14/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5152 - accuracy: 0.3037 - val_loss: 1.8490 - val_accuracy: 0.2736\n",
            "Epoch 15/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5382 - accuracy: 0.3047 - val_loss: 1.5734 - val_accuracy: 0.3031\n",
            "Epoch 16/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5568 - accuracy: 0.3150 - val_loss: 1.5543 - val_accuracy: 0.2815\n",
            "Epoch 17/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5039 - accuracy: 0.3088 - val_loss: 1.5735 - val_accuracy: 0.3091\n",
            "Epoch 18/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5143 - accuracy: 0.3049 - val_loss: 1.5616 - val_accuracy: 0.2854\n",
            "Epoch 19/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5277 - accuracy: 0.2975 - val_loss: 1.6060 - val_accuracy: 0.2835\n",
            "Epoch 20/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5141 - accuracy: 0.3190 - val_loss: 1.5806 - val_accuracy: 0.2874\n",
            "Epoch 21/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4658 - accuracy: 0.3585 - val_loss: 1.5845 - val_accuracy: 0.2736\n",
            "Epoch 22/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.5021 - accuracy: 0.3321 - val_loss: 1.5835 - val_accuracy: 0.3071\n",
            "Epoch 23/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4717 - accuracy: 0.3385 - val_loss: 1.5645 - val_accuracy: 0.2795\n",
            "Epoch 24/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4823 - accuracy: 0.3246 - val_loss: 1.5682 - val_accuracy: 0.2874\n",
            "Epoch 25/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4886 - accuracy: 0.3307 - val_loss: 1.5669 - val_accuracy: 0.2697\n",
            "Epoch 26/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4545 - accuracy: 0.3620 - val_loss: 1.5653 - val_accuracy: 0.2776\n",
            "Epoch 27/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4656 - accuracy: 0.3539 - val_loss: 1.6000 - val_accuracy: 0.2776\n",
            "Epoch 28/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4899 - accuracy: 0.3628 - val_loss: 1.5951 - val_accuracy: 0.2874\n",
            "Epoch 29/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4900 - accuracy: 0.3317 - val_loss: 1.6363 - val_accuracy: 0.2933\n",
            "Epoch 30/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4804 - accuracy: 0.3389 - val_loss: 1.6840 - val_accuracy: 0.3130\n",
            "Epoch 31/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4758 - accuracy: 0.3399 - val_loss: 1.6977 - val_accuracy: 0.2559\n",
            "Epoch 32/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4600 - accuracy: 0.3537 - val_loss: 1.6213 - val_accuracy: 0.2795\n",
            "Epoch 33/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4478 - accuracy: 0.3543 - val_loss: 1.6181 - val_accuracy: 0.2992\n",
            "Epoch 34/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4478 - accuracy: 0.3669 - val_loss: 1.6106 - val_accuracy: 0.3071\n",
            "Epoch 35/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4661 - accuracy: 0.3679 - val_loss: 1.6872 - val_accuracy: 0.3031\n",
            "Epoch 36/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4634 - accuracy: 0.3705 - val_loss: 1.6486 - val_accuracy: 0.2815\n",
            "Epoch 37/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4647 - accuracy: 0.3593 - val_loss: 1.6643 - val_accuracy: 0.2933\n",
            "Epoch 38/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4519 - accuracy: 0.3894 - val_loss: 1.6223 - val_accuracy: 0.2756\n",
            "Epoch 39/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4272 - accuracy: 0.3769 - val_loss: 1.6001 - val_accuracy: 0.2953\n",
            "Epoch 40/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4349 - accuracy: 0.3742 - val_loss: 1.9319 - val_accuracy: 0.2835\n",
            "Epoch 41/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4572 - accuracy: 0.3513 - val_loss: 1.6040 - val_accuracy: 0.2638\n",
            "Epoch 42/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4361 - accuracy: 0.3611 - val_loss: 1.6326 - val_accuracy: 0.3150\n",
            "Epoch 43/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4319 - accuracy: 0.3627 - val_loss: 1.7413 - val_accuracy: 0.2933\n",
            "Epoch 44/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4472 - accuracy: 0.3686 - val_loss: 1.7892 - val_accuracy: 0.2165\n",
            "Epoch 45/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4290 - accuracy: 0.3865 - val_loss: 1.9760 - val_accuracy: 0.2343\n",
            "Epoch 46/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4177 - accuracy: 0.3868 - val_loss: 1.6128 - val_accuracy: 0.2953\n",
            "Epoch 47/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4235 - accuracy: 0.4049 - val_loss: 1.7907 - val_accuracy: 0.2618\n",
            "Epoch 48/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4152 - accuracy: 0.3948 - val_loss: 1.6542 - val_accuracy: 0.3051\n",
            "Epoch 49/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4339 - accuracy: 0.4043 - val_loss: 2.1148 - val_accuracy: 0.2047\n",
            "Epoch 50/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3889 - accuracy: 0.4323 - val_loss: 1.6393 - val_accuracy: 0.3130\n",
            "Epoch 51/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4030 - accuracy: 0.3935 - val_loss: 1.6703 - val_accuracy: 0.3209\n",
            "Epoch 52/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3901 - accuracy: 0.4267 - val_loss: 1.7669 - val_accuracy: 0.2598\n",
            "Epoch 53/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4192 - accuracy: 0.4134 - val_loss: 1.8381 - val_accuracy: 0.3071\n",
            "Epoch 54/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4026 - accuracy: 0.4022 - val_loss: 1.7272 - val_accuracy: 0.3287\n",
            "Epoch 55/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4239 - accuracy: 0.3995 - val_loss: 1.7913 - val_accuracy: 0.3169\n",
            "Epoch 56/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3950 - accuracy: 0.4062 - val_loss: 1.7060 - val_accuracy: 0.3307\n",
            "Epoch 57/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3937 - accuracy: 0.4241 - val_loss: 1.6548 - val_accuracy: 0.3287\n",
            "Epoch 58/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3800 - accuracy: 0.4171 - val_loss: 1.6715 - val_accuracy: 0.3130\n",
            "Epoch 59/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3777 - accuracy: 0.4340 - val_loss: 1.7520 - val_accuracy: 0.2913\n",
            "Epoch 60/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3753 - accuracy: 0.4216 - val_loss: 1.6534 - val_accuracy: 0.3189\n",
            "Epoch 61/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3768 - accuracy: 0.4314 - val_loss: 1.8365 - val_accuracy: 0.3091\n",
            "Epoch 62/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4095 - accuracy: 0.4152 - val_loss: 2.0070 - val_accuracy: 0.2953\n",
            "Epoch 63/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4210 - accuracy: 0.3965 - val_loss: 1.7138 - val_accuracy: 0.3031\n",
            "Epoch 64/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3940 - accuracy: 0.4313 - val_loss: 1.7442 - val_accuracy: 0.3169\n",
            "Epoch 65/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3889 - accuracy: 0.4163 - val_loss: 2.5840 - val_accuracy: 0.2047\n",
            "Epoch 66/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4111 - accuracy: 0.4263 - val_loss: 1.7752 - val_accuracy: 0.3465\n",
            "Epoch 67/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3678 - accuracy: 0.4316 - val_loss: 1.6565 - val_accuracy: 0.2933\n",
            "Epoch 68/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3875 - accuracy: 0.4451 - val_loss: 1.7348 - val_accuracy: 0.3051\n",
            "Epoch 69/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3551 - accuracy: 0.4404 - val_loss: 1.8773 - val_accuracy: 0.3169\n",
            "Epoch 70/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3732 - accuracy: 0.4300 - val_loss: 1.7711 - val_accuracy: 0.3051\n",
            "Epoch 71/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3693 - accuracy: 0.4393 - val_loss: 2.0969 - val_accuracy: 0.2933\n",
            "Epoch 72/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3602 - accuracy: 0.4717 - val_loss: 2.0363 - val_accuracy: 0.3130\n",
            "Epoch 73/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3687 - accuracy: 0.4337 - val_loss: 1.8127 - val_accuracy: 0.2736\n",
            "Epoch 74/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3881 - accuracy: 0.4241 - val_loss: 1.7368 - val_accuracy: 0.3287\n",
            "Epoch 75/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3443 - accuracy: 0.4465 - val_loss: 1.6916 - val_accuracy: 0.3346\n",
            "Epoch 76/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3431 - accuracy: 0.4567 - val_loss: 1.7794 - val_accuracy: 0.2972\n",
            "Epoch 77/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3501 - accuracy: 0.4591 - val_loss: 1.7396 - val_accuracy: 0.3150\n",
            "Epoch 78/100\n",
            "254/254 [==============================] - 1s 5ms/step - loss: 1.3719 - accuracy: 0.4410 - val_loss: 1.8310 - val_accuracy: 0.2972\n",
            "Epoch 79/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3734 - accuracy: 0.4517 - val_loss: 1.7961 - val_accuracy: 0.3130\n",
            "Epoch 80/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3434 - accuracy: 0.4476 - val_loss: 1.7484 - val_accuracy: 0.3228\n",
            "Epoch 81/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3488 - accuracy: 0.4701 - val_loss: 1.8273 - val_accuracy: 0.2402\n",
            "Epoch 82/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3720 - accuracy: 0.4288 - val_loss: 1.7097 - val_accuracy: 0.3228\n",
            "Epoch 83/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3584 - accuracy: 0.4429 - val_loss: 2.0965 - val_accuracy: 0.3012\n",
            "Epoch 84/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3450 - accuracy: 0.4614 - val_loss: 1.9515 - val_accuracy: 0.3169\n",
            "Epoch 85/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3632 - accuracy: 0.4369 - val_loss: 2.0563 - val_accuracy: 0.2972\n",
            "Epoch 86/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3585 - accuracy: 0.4562 - val_loss: 2.2460 - val_accuracy: 0.2854\n",
            "Epoch 87/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3321 - accuracy: 0.4640 - val_loss: 1.7555 - val_accuracy: 0.2835\n",
            "Epoch 88/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.2956 - accuracy: 0.4791 - val_loss: 1.7904 - val_accuracy: 0.3386\n",
            "Epoch 89/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3746 - accuracy: 0.4579 - val_loss: 1.8708 - val_accuracy: 0.3110\n",
            "Epoch 90/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3325 - accuracy: 0.4612 - val_loss: 2.0510 - val_accuracy: 0.2264\n",
            "Epoch 91/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3776 - accuracy: 0.4530 - val_loss: 3.8271 - val_accuracy: 0.1850\n",
            "Epoch 92/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.4452 - accuracy: 0.4408 - val_loss: 2.1183 - val_accuracy: 0.2264\n",
            "Epoch 93/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3974 - accuracy: 0.4486 - val_loss: 1.8909 - val_accuracy: 0.3189\n",
            "Epoch 94/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3241 - accuracy: 0.4694 - val_loss: 1.7067 - val_accuracy: 0.2992\n",
            "Epoch 95/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3504 - accuracy: 0.4583 - val_loss: 1.9561 - val_accuracy: 0.3110\n",
            "Epoch 96/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3330 - accuracy: 0.4822 - val_loss: 1.8816 - val_accuracy: 0.3209\n",
            "Epoch 97/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3588 - accuracy: 0.4601 - val_loss: 1.8235 - val_accuracy: 0.2933\n",
            "Epoch 98/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3465 - accuracy: 0.4784 - val_loss: 2.0492 - val_accuracy: 0.2835\n",
            "Epoch 99/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3549 - accuracy: 0.4461 - val_loss: 1.7723 - val_accuracy: 0.2756\n",
            "Epoch 100/100\n",
            "254/254 [==============================] - 1s 4ms/step - loss: 1.3332 - accuracy: 0.4660 - val_loss: 1.9291 - val_accuracy: 0.3051\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.22826696932315826,\n",
              " 0.2804262340068817,\n",
              " 0.2922041416168213,\n",
              " 0.2697700560092926,\n",
              " 0.2849130630493164]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 221
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "UF0KOSbTRbtg",
        "outputId": "78205b71-716b-44f6-a4eb-a554c082b22d"
      },
      "source": [
        "plt.plot(range(1, len(ave_val_loss_hist)+1)[:], ave_val_loss_hist[:], \"bo\", label=\"Validation Loss\")\n",
        "plt.plot(range(1, len(ave_val_loss_hist)+1)[:], ave_loss_hist[:], \"b\", label=\"Training Loss\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 222,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 279
        },
        "id": "gboveCttRc_O",
        "outputId": "4f1cb0f3-ab11-45bc-a70a-a007f7231021"
      },
      "source": [
        "plt.plot(range(1, len(ave_val_acc_hist)+1)[:], ave_val_acc_hist[:], \"bo\", label=\"Validation Accuracy\")\n",
        "plt.plot(range(1, len(ave_val_acc_hist)+1)[:], ave_acc_hist[:], \"b\", label=\"Training Accuracy\")\n",
        "plt.xlabel(\"Epochs\")\n",
        "plt.ylabel(\"Accuracy\")\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "execution_count": 223,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "image/png": "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\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": [],
            "needs_background": "light"
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "godACOVI2oBI"
      },
      "source": [
        "**Best Fit Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7P7k-jWYrWln"
      },
      "source": [
        "model=models.Sequential()\n",
        "model.add(layers.Dense(64, activation=\"relu\", kernel_regularizer=regularizers.l2(l2=.002), input_shape=(pic_size*pic_size,)))\n",
        "model.add(layers.Dropout(.2))\n",
        "model.add(layers.Dense(5, activation=\"softmax\"))\n",
        "model.compile(optimizer=\"rmsprop\", loss = \"categorical_crossentropy\", metrics=[\"accuracy\"])"
      ],
      "execution_count": 224,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xXJ_IXyl2BMi"
      },
      "source": [
        "**Training the model with all samples**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-Xl78IydtPae",
        "outputId": "7673f323-c7b9-4f0d-f805-65d7a1fb32bb"
      },
      "source": [
        "model.fit(train_data,train_labels, epochs=10,verbose=1, batch_size=8)"
      ],
      "execution_count": 225,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/10\n",
            "318/318 [==============================] - 2s 3ms/step - loss: 1.9943 - accuracy: 0.2499\n",
            "Epoch 2/10\n",
            "318/318 [==============================] - 1s 3ms/step - loss: 1.6572 - accuracy: 0.2524\n",
            "Epoch 3/10\n",
            "318/318 [==============================] - 1s 3ms/step - loss: 1.5911 - accuracy: 0.2803\n",
            "Epoch 4/10\n",
            "318/318 [==============================] - 1s 3ms/step - loss: 1.5988 - accuracy: 0.2590\n",
            "Epoch 5/10\n",
            "318/318 [==============================] - 1s 4ms/step - loss: 1.5977 - accuracy: 0.2809\n",
            "Epoch 6/10\n",
            "318/318 [==============================] - 1s 3ms/step - loss: 1.5682 - accuracy: 0.2948\n",
            "Epoch 7/10\n",
            "318/318 [==============================] - 1s 3ms/step - loss: 1.5851 - accuracy: 0.2812\n",
            "Epoch 8/10\n",
            "318/318 [==============================] - 1s 3ms/step - loss: 1.5664 - accuracy: 0.2860\n",
            "Epoch 9/10\n",
            "318/318 [==============================] - 1s 3ms/step - loss: 1.5518 - accuracy: 0.3108\n",
            "Epoch 10/10\n",
            "318/318 [==============================] - 1s 3ms/step - loss: 1.5359 - accuracy: 0.3344\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7efd57498f90>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 225
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2FtImZvW12r5"
      },
      "source": [
        "**Evaluating the Model**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BrzyW7Virabl",
        "outputId": "2f48e5fc-4e04-4254-bedb-a6bea998be94"
      },
      "source": [
        "test_loss_score, test_acc_score=model.evaluate(test_data, test_labels)"
      ],
      "execution_count": 226,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "56/56 [==============================] - 0s 2ms/step - loss: 1.5499 - accuracy: 0.3051\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKAbqYNB0x83"
      },
      "source": [
        "**Predicting the values**"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SkJiTqCkrzx8"
      },
      "source": [
        "prediction=model.predict(test_data)"
      ],
      "execution_count": 227,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "frYGw5G9vje0",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "25b55e11-a65e-498a-b687-7ea965a386b3"
      },
      "source": [
        "prediction_result = []\n",
        "for i, v in enumerate(prediction):\n",
        "  prediction_result.append(np.argmax(prediction[i]))\n",
        "print(prediction_result[:35])\n",
        "\n",
        "test_labels_result = []\n",
        "for i, v in enumerate(test_labels):\n",
        "  test_labels_result.append(np.argmax(test_labels[i]))\n",
        "print(test_labels_result[:35])"
      ],
      "execution_count": 242,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[4, 4, 4, 4, 4, 4, 4, 3, 4, 1, 4, 4, 4, 4, 4, 1, 1, 1, 4, 1, 1, 4, 4, 1, 4, 4, 4, 4, 4, 4, 0, 1, 1, 4, 4]\n",
            "[1, 4, 4, 1, 3, 2, 4, 1, 4, 0, 4, 2, 0, 3, 2, 1, 0, 0, 3, 2, 2, 1, 3, 1, 2, 4, 4, 3, 2, 1, 0, 1, 2, 1, 4]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P7CFLYgfwydF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38cb887e-c43f-4a0c-9b1c-c7ebf7609acc"
      },
      "source": [
        "matched = []\n",
        "for i, v in enumerate(test_labels_result):\n",
        "  if  prediction_result[i] == test_labels_result[i]:\n",
        "    matched.append(\"Yes\")\n",
        "  else:\n",
        "    matched.append(\"No\")\n",
        "\n",
        "print(\"There are\",matched.count(\"Yes\"), \"matched flowers and there are\", matched.count(\"No\"), \"unmatched flowers \")"
      ],
      "execution_count": 229,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "There are 544 matched flowers and there are 1239 unmatched flowers \n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}